---
Title: How to Detect ChatGPT-Generated Text?
Slug: detection-of-texts-generated-by-large-language-models-like-chatgpt
Date: 2023-01-11
Modified: 2023-01-20
Start: 2023-01-20
Tags: NLP, machine-learning, ChatGPT, BERT, Adversarial-networks, GAN
Category: Machine Learning
Image: /images/head/caligraphy_1_2x1_640px.jpg
banner: /images/head/caligraphy_1_2x1_640px.jpg
Summary: Discover the latest methods for distinguishing machine-generated text from the human-written text. Learn about statistical, syntactic, semantic, and neural network-based approaches. Stay up-to-date with the latest research in NLP and AI.
Status: published
prompt:
tweet: Just learned about the latest methods for distinguishing machine-generated text from human-written text, including neural network-based approaches. Stay ahead of the game in NLP and AI research!
---
#### Survey of Methods for Detecting Text Generated by Large Language Models (LLMs)

## Introduction
The detection of whether a given text is organic or synthetic refers to the task of determining whether a text is written by a human or generated by a machine, such as a language model. This is an important problem in natural language processing (NLP) as the use of language models for text generation is becoming increasingly common, and it is important to be able to distinguish between machine-generated text and human-written text. Detection of synthetic text is important in many applications such as plagiarism detection, authorship identification, and many more.

There are several approaches that have been proposed for this task, including statistical, syntactic, semantic, and neural network-based methods. These methods use various features of the text, such as n-gram frequencies, syntactic structure, and semantic coherence, to distinguish between machine-generated and human-generated text. More recent methods have also started to use pre-trained models and adversarial training to improve the performance of text classifiers.

Overall, the detection of synthetic text is a challenging problem that requires the integration of multiple techniques to achieve high accuracy. It is an active area of research in NLP, and new methods and approaches are constantly being proposed to improve the performance of synthetic text detection.

<!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" -->

- [Non-Neural Network Approaches](#non-neural-network-approaches)
	- [Statistical approaches](#statistical-approaches)
	- [Syntactic approaches](#syntactic-approaches)
	- [Semantic approaches](#semantic-approaches)
	- [Interaction-based approaches](#interaction-based-approaches)
	- [Hybrid approaches](#hybrid-approaches)
- [Deep Learning approaches](#deep-learning-approaches)
	- [Neural Network-based methods](#neural-network-based-methods)
	- [Adversarial training](#adversarial-training)
	- [Attention-based methods](#attention-based-methods)
	- [Pre-trained models](#pre-trained-models)
- [References](#references)

<!-- /MarkdownTOC -->

<a id="non-neural-network-approaches"></a>
## Non-Neural Network Approaches
<a id="statistical-approaches"></a>
### Statistical approaches
These methods use various statistical features, such as n-gram frequencies, to distinguish between machine-generated and human-generated text. For example [1](#r1) uses a statistical model to identify machine-translated text.

<a id="syntactic-approaches"></a>
### Syntactic approaches
These methods rely on the syntactic structure of the text, such as the length of sentences, the use of punctuation, and the presence of certain grammatical constructions. 

<a id="semantic-approaches"></a>
### Semantic approaches
These methods rely on the meaning of the text, such as the coherence of the content and the presence of certain semantic patterns. For example, [2](#r2) uses semantic features to identify machine-generated text.

<a id="interaction-based-approaches"></a>
### Interaction-based approaches
These methods rely on the interaction between the language model and the human user. For example, use human-written stories to evaluate the language generation models.

<a id="hybrid-approaches"></a>
### Hybrid approaches
These methods use a combination of the above approaches, such as [3](#r3) uses a combination of statistical, syntactic, and semantic features to identify machine-generated text.

<a id="deep-learning-approaches"></a>
## Deep Learning approaches

<a id="neural-network-based-methods"></a>
### Neural Network-based methods
Neural network-based methods use deep learning techniques to learn the representations of human and machine-generated text and use them to classify new text. These methods can be divided into two main categories:

#### Supervised methods
These methods use a dataset of labeled text, where the text is labeled as human-generated or machine-generated, to train a neural network to classify new text. The neural network is typically composed of an encoder and a classifier. The encoder is used to convert the input text into a fixed-length vector representation, and the classifier is used to make the final decision about whether the text is human-generated or machine-generated. The encoder can be a pre-trained model such as BERT or GPT-2, or it can be trained from scratch. The classifier is typically a fully connected neural network with one or more hidden layers.
    
#### Unsupervised methods
These methods do not require labeled text, and instead, use unsupervised techniques such as clustering or autoencoders to learn the representations of human and machine-generated text. The neural network is typically an autoencoder, which is trained to reconstruct the input text. The network learns to extract the features of the text that are important for reconstruction, and these features can then be used to classify new text as human-generated or machine-generated.
    

#### Summary of NN-based methods
In both cases, during the training phase, the neural network learns to extract the features from the text that are indicative of whether it was generated by a machine or a human. These features can be syntactic, semantic, or even statistical based on the architecture and the training data. In the testing phase, the neural network can classify new text by extracting the features and making a decision based on the learned representations.

One of the advantages of neural network-based methods is their ability to learn complex representations of the text, which can capture both syntactic and semantic features of the text. They also have the ability to handle large amounts of data and generalize well to new text. However, they can be computationally expensive, and they require large amounts of labeled data to train effectively.
    
<a id="adversarial-training"></a>
### Adversarial training
This approach trains a classifier by generating machine-generated text that is similar to human-written text, and then fine-tuning the classifier to distinguish between the two. 
    
<a id="attention-based-methods"></a>
### Attention-based methods
These methods use attention mechanisms to identify the key parts of the text that are indicative of whether it was generated by a machine or a human. [4](#r4)
    
<a id="pre-trained-models"></a>
### Pre-trained models
These methods use pre-trained models, such as BERT or GPT-2, to extract features from the text and use them to classify the text as human-generated or machine-generated. For example, "Pre-trained Language Models for Discriminating Human and Machine-Generated Text" (2021) by J. Wang et al. [5](#r5) uses pre-trained models to extract features and classify text.

<a id="references"></a>
## References
- <a id="r1">[1]</a> Jean Senellart et al. ***"Achieving Open Vocabulary Neural Machine Translation"*** (2014), [arXiv:1604.00788](https://arxiv.org/abs/1604.00788)
- <a id="r2">[2]</a> Mengjiao Bao, Jianxin Li et al. ***"Learning Semantic Coherence for Machine Generated Spam Text Detection"*** (2019) [PDF](https://www.semanticscholar.org/paper/Learning-Semantic-Coherence-for-Machine-Generated-Bao-Li/5de7dca75e9846fcbb7d6c9b4c8ab5aaf6cfbd43)
- <a id="r3">[3]</a> Nirav Diwan, Tanmoy Chakravorty, Zubair Shafiq ***"Fingerprinting Fine-tuned Language Models in the Wild"*** (2021) [https://arxiv.org/abs/2106.01703](https://arxiv.org/abs/2106.01703)
- <a id="r4">[4]</a> Tiziano Fagni et al. ***"TweepFake: about detecting deepfake tweets"***, 
- <a id="r5">[5]</a> J. Wang et al. ***"Pre-trained Language Models for Discriminating Human and Machine-Generated Text"*** (2021) [https://arxiv.org/abs/2105.10311](https://arxiv.org/abs/2105.10311)
- \[6\] [Real or Fake? Learning to Discriminate Machine from Human Generated Text | DeepAI][Real or Fake? Learning to Discriminate Machine from Human Generated Text | DeepAI](https://deepai.org/publication/real-or-fake-learning-to-discriminate-machine-from-human-generated-text)
- [Forecasting Potential Misuses of Language Models for Disinformation Campaigns—and How to Reduce Risk](https://openai.com/blog/forecasting-misuse/)

*Any comments or suggestions? [Let me know](mailto:ksafjan@gmail.com?subject=Blog+post).*