---
Category: note
Date: 2024-07-03
Modified: 2024-08-01
Slug: best-small-models-for-fine-tuning
Status: published
Summary: Learn about the top small models for fine-tuning, including rankings and insights from a webinar on optimizing open-source large language models to rival GPT-4.
ai_summary: true
Title: Best Small Models for Fine Tuning
tags:
  - predibase
  - evaluation
  - llm
  - slm
  - small-language-models
  - fine-tuning
---
X:[[fine_tune_small_language_models_LLM_SLM__broken]]
X::[[RAG_evaluation]]

## Best models to fine-tune

![[Pasted image 20240708122914.png]]

**Ranking:**
1. llama-3-8b
2. phi-3-4k
3. zephyr-7b-beta
4. llama-3-8b-instruct
5. mistral-7b

Source: [The Fine-tuning Index - Predibase](https://predibase.com/fine-tuning-index)

## Related Webinar

```vid
https://www.youtube.com/watch?v=kFdfDJ1fQxY
```

[https://pbase.ai/3x4JB6G](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa1NGYXVWNTY1aEI0cV9Qb2F0aGVHdUZWRjVFUXxBQ3Jtc0tsZGJfTFl5SEV0SlZMOXBSRWNiZERxWlA0Q2Zvd0gzSVRSSXY1WnV0eGRFV2w0VGE0WnB2QkFhY3R2YVBMZlM0aVpHb0daMjd4R1dLVkxDeFhJZjMwcDkzWHczcVFleU05aGg2QUExaDhndzZtN1ZIWQ&q=https%3A%2F%2Fpbase.ai%2F3x4JB6G&v=kFdfDJ1fQxY)

In this on-demand webinar, Staff Software Engineer Justin Zhao, and ML Engineer Timothy Wang, lead an in-depth discussion on our findings:

• How we fine-tuned open-source LLMs that rival GPT-4
• How you can implement Parameter-Efficient Fine-Tuning (PEFT) methods like Low Rank Adaptation (LoRA)
• Which tasks are best suited for fine-tuning based on our benchmarks
• Which popular LLMs - namely Phi, Gemma, and Mistral - perform best and worst across tasks
• How we implemented an evaluation harness for fine-tuning at scale
