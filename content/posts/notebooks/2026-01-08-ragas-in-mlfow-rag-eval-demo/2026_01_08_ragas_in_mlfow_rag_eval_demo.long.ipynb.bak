{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation with RAGAS and MLflow - long version\n",
    "\n",
    "This tutorial demonstrates how to evaluate Retrieval-Augmented Generation (RAG) systems using **RAGAS** (Retrieval Augmented Generation Assessment) metrics through **MLflow** integration.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Build a minimal RAG pipeline** using LangChain and FAISS\n",
    "2. **Create a golden evaluation dataset** with expected answers\n",
    "3. **Evaluate RAG quality** using RAGAS metrics (Faithfulness, Context Precision, Context Recall, Factual Correctness)\n",
    "4. **Track results in MLflow** for systematic comparison\n",
    "5. **Support multiple LLM providers**: OpenAI, Azure OpenAI, and Ollama\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- API key for your chosen LLM provider\n",
    "- Basic understanding of RAG concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Provider Configuration\n",
    "\n",
    "This tutorial supports three LLM providers. Choose your provider and configure the appropriate environment variables:\n",
    "\n",
    "| Provider | Required Environment Variables |\n",
    "|----------|-------------------------------|\n",
    "| OpenAI | `OPENAI_API_KEY` |\n",
    "| Azure OpenAI | `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_DEPLOYMENT_NAME` |\n",
    "| Ollama | None (runs locally on `http://localhost:11434`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: azure_openai\n",
      "Chat model: gpt-4o-mini\n",
      "Embedding model: text-embedding-ada-002-v2\n"
     ]
    }
   ],
   "source": [
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    AZURE_OPENAI = \"azure_openai\"\n",
    "    OLLAMA = \"ollama\"\n",
    "\n",
    "\n",
    "# === CONFIGURE YOUR PROVIDER HERE ===\n",
    "PROVIDER = LLMProvider.AZURE_OPENAI\n",
    "\n",
    "# Model names per provider\n",
    "MODEL_CONFIG = {\n",
    "    LLMProvider.OPENAI: {\n",
    "        \"chat_model\": \"gpt-4o-mini\",\n",
    "        \"embedding_model\": \"text-embedding-3-small\",\n",
    "    },\n",
    "    LLMProvider.AZURE_OPENAI: {\n",
    "        \"chat_model\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4o-mini\"),\n",
    "        \"embedding_model\": os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-small\"),\n",
    "    },\n",
    "    LLMProvider.OLLAMA: {\n",
    "        \"chat_model\": \"llama3.2:3b\",\n",
    "        \"embedding_model\": \"nomic-embed-text\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Using provider: {PROVIDER.value}\")\n",
    "print(f\"Chat model: {MODEL_CONFIG[PROVIDER]['chat_model']}\")\n",
    "print(f\"Embedding model: {MODEL_CONFIG[PROVIDER]['embedding_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using azure_openai - Ollama model listing skipped\n"
     ]
    }
   ],
   "source": [
    "# List available Ollama models (only runs if Ollama provider is selected)\n",
    "if PROVIDER == LLMProvider.OLLAMA:\n",
    "    import subprocess\n",
    "    import shutil\n",
    "    \n",
    "    if shutil.which(\"ollama\"):\n",
    "        print(\"Available Ollama models:\")\n",
    "        print(\"-\" * 50)\n",
    "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout)\n",
    "            # Check if required models are available\n",
    "            available = result.stdout.lower()\n",
    "            chat_model = MODEL_CONFIG[PROVIDER][\"chat_model\"]\n",
    "            embed_model = MODEL_CONFIG[PROVIDER][\"embedding_model\"]\n",
    "            \n",
    "            missing = []\n",
    "            if chat_model.split(\":\")[0] not in available:\n",
    "                missing.append(chat_model)\n",
    "            if embed_model.split(\":\")[0] not in available:\n",
    "                missing.append(embed_model)\n",
    "            \n",
    "            if missing:\n",
    "                print(f\"\\n‚ö†Ô∏è  Missing required models: {missing}\")\n",
    "                print(f\"Run: ollama pull {' && ollama pull '.join(missing)}\")\n",
    "            else:\n",
    "                print(\"‚úì All required models are available\")\n",
    "        else:\n",
    "            print(f\"Error listing models: {result.stderr}\")\n",
    "            print(\"Make sure Ollama is running: ollama serve\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Ollama CLI not found. Please install Ollama: https://ollama.ai\")\n",
    "else:\n",
    "    print(f\"Using {PROVIDER.value} - Ollama model listing skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure litellm env vars configured\n",
      "Environment validated for azure_openai\n"
     ]
    }
   ],
   "source": [
    "def validate_environment(provider: LLMProvider) -> None:\n",
    "    \"\"\"Validate required environment variables for the selected provider.\"\"\"\n",
    "    required_vars = {\n",
    "        LLMProvider.OPENAI: [\"OPENAI_API_KEY\"],\n",
    "        LLMProvider.AZURE_OPENAI: [\n",
    "            \"AZURE_OPENAI_ENDPOINT\",\n",
    "            \"AZURE_OPENAI_API_KEY\",\n",
    "        ],\n",
    "        LLMProvider.OLLAMA: [],\n",
    "    }\n",
    "\n",
    "    missing = [var for var in required_vars[provider] if not os.getenv(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(\n",
    "            f\"Missing environment variables for {provider.value}: {missing}\\n\"\n",
    "            f\"Please set them before continuing.\"\n",
    "        )\n",
    "    \n",
    "    # Set provider-specific env vars for litellm (used by RAGAS scorers)\n",
    "    if provider == LLMProvider.OLLAMA:\n",
    "        os.environ.setdefault(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
    "        print(f\"OLLAMA_API_BASE set to: {os.environ['OLLAMA_API_BASE']}\")\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        # Set litellm Azure env vars from Azure OpenAI vars\n",
    "        if os.getenv(\"AZURE_OPENAI_API_KEY\") and not os.getenv(\"AZURE_API_KEY\"):\n",
    "            os.environ[\"AZURE_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "        if os.getenv(\"AZURE_OPENAI_ENDPOINT\") and not os.getenv(\"AZURE_API_BASE\"):\n",
    "            os.environ[\"AZURE_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "        os.environ.setdefault(\"AZURE_API_VERSION\", os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"))\n",
    "        print(f\"Azure litellm env vars configured\")\n",
    "    \n",
    "    print(f\"Environment validated for {provider.value}\")\n",
    "\n",
    "\n",
    "validate_environment(PROVIDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: AzureChatOpenAI\n",
      "Embeddings initialized: AzureOpenAIEmbeddings\n",
      "MLflow model URI: azure:/gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings, AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "def get_llm(provider: LLMProvider):\n",
    "    \"\"\"Factory function to create LLM instance based on provider.\"\"\"\n",
    "    config = MODEL_CONFIG[provider]\n",
    "\n",
    "    if provider == LLMProvider.OPENAI:\n",
    "        return ChatOpenAI(\n",
    "            model=config[\"chat_model\"],\n",
    "            temperature=0,\n",
    "        )\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        return AzureChatOpenAI(\n",
    "            azure_deployment=config[\"chat_model\"],\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "            temperature=0,\n",
    "        )\n",
    "    elif provider == LLMProvider.OLLAMA:\n",
    "        return ChatOllama(\n",
    "            model=config[\"chat_model\"],\n",
    "            temperature=0,\n",
    "            base_url=\"http://localhost:11434\",\n",
    "        )\n",
    "\n",
    "\n",
    "def get_embeddings(provider: LLMProvider):\n",
    "    \"\"\"Factory function to create embeddings instance based on provider.\"\"\"\n",
    "    config = MODEL_CONFIG[provider]\n",
    "\n",
    "    if provider == LLMProvider.OPENAI:\n",
    "        return OpenAIEmbeddings(model=config[\"embedding_model\"])\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        return AzureOpenAIEmbeddings(\n",
    "            azure_deployment=config[\"embedding_model\"],\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "        )\n",
    "    elif provider == LLMProvider.OLLAMA:\n",
    "        return OllamaEmbeddings(\n",
    "            model=config[\"embedding_model\"],\n",
    "            base_url=\"http://localhost:11434\",\n",
    "        )\n",
    "\n",
    "\n",
    "def get_mlflow_model_uri(provider: LLMProvider) -> str:\n",
    "    \"\"\"Get MLflow model URI for RAGAS scorers (uses litellm format).\"\"\"\n",
    "    config = MODEL_CONFIG[provider]\n",
    "\n",
    "    if provider == LLMProvider.OPENAI:\n",
    "        return f\"openai:/{config['chat_model']}\"\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        # Azure format: azure/<deployment_name>\n",
    "        return f\"azure:/{config['chat_model']}\"\n",
    "    elif provider == LLMProvider.OLLAMA:\n",
    "        # Ollama format for litellm: ollama/<model_name>\n",
    "        # Note: ollama_chat format has issues with litellm, use ollama/ prefix\n",
    "        return f\"ollama:/{config['chat_model']}\"\n",
    "\n",
    "\n",
    "llm = get_llm(PROVIDER)\n",
    "embeddings = get_embeddings(PROVIDER)\n",
    "mlflow_model_uri = get_mlflow_model_uri(PROVIDER)\n",
    "\n",
    "print(f\"LLM initialized: {type(llm).__name__}\")\n",
    "print(f\"Embeddings initialized: {type(embeddings).__name__}\")\n",
    "print(f\"MLflow model URI: {mlflow_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Knowledge Base\n",
    "\n",
    "We'll create a small knowledge base about **MLflow** - fitting for a tutorial that uses MLflow for evaluation! This dataset contains key concepts that our RAG system will retrieve from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base contains 8 documents\n",
      "  1. MLflow Tracking is an API and UI for logging parameters, code versions, metrics,...\n",
      "  2. The MLflow Model Registry is a centralized model store that provides model linea...\n",
      "  3. MLflow GenAI provides specialized tools for developing and evaluating generative...\n",
      "  4. RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework    ...\n",
      "  5. MLflow Projects package code in a reusable, reproducible form. A project is simp...\n",
      "  6. MLflow's autolog feature automatically logs metrics, parameters, and models duri...\n",
      "  7. The MLflow Model format is a standard format for packaging machine learning mode...\n",
      "  8. Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML...\n"
     ]
    }
   ],
   "source": [
    "KNOWLEDGE_BASE = [\n",
    "    \"\"\"\n",
    "    MLflow Tracking is an API and UI for logging parameters, code versions, metrics, \n",
    "    and artifacts when running your machine learning code. It allows you to log and \n",
    "    query experiments using Python, REST, R API, and Java API. The MLflow Tracking \n",
    "    component lets you log source code, models, and visualizations. Each run records:\n",
    "    code version, start and end time, source, parameters, metrics, and artifacts.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The MLflow Model Registry is a centralized model store that provides model lineage,\n",
    "    model versioning, stage transitions (Staging, Production, Archived), and annotations.\n",
    "    It enables teams to share models, collaborate on moving models from experimentation \n",
    "    to production, and integrate with CI/CD pipelines. Models can be registered from \n",
    "    any MLflow run or loaded directly from the registry.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MLflow GenAI provides specialized tools for developing and evaluating generative AI \n",
    "    applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM \n",
    "    outputs using configurable scorers. The framework supports both built-in judges and \n",
    "    custom evaluation metrics. Tracing capabilities allow you to capture detailed \n",
    "    execution traces including inputs, outputs, and intermediate steps.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework \n",
    "    integrated with MLflow for assessing RAG pipelines. Key metrics include:\n",
    "    - Faithfulness: measures if the answer is grounded in the retrieved context\n",
    "    - Context Precision: evaluates if relevant documents are ranked higher\n",
    "    - Context Recall: checks if the context contains all needed information\n",
    "    - Factual Correctness: compares output against expected ground truth answers\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MLflow Projects package code in a reusable, reproducible form. A project is simply \n",
    "    a directory or Git repository containing code and a MLproject file that specifies \n",
    "    dependencies and entry points. Projects can be run locally or remotely on \n",
    "    Databricks, Kubernetes, or any compute backend. The MLproject file uses YAML \n",
    "    format to define the project name, environment, and entry points with parameters.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MLflow's autolog feature automatically logs metrics, parameters, and models during \n",
    "    training runs for popular ML libraries. Supported frameworks include scikit-learn, \n",
    "    TensorFlow, Keras, PyTorch, XGBoost, LightGBM, Spark, and LangChain. For LangChain, \n",
    "    mlflow.langchain.autolog() captures chain invocations, inputs, outputs, and traces \n",
    "    without requiring manual instrumentation code.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The MLflow Model format is a standard format for packaging machine learning models \n",
    "    that can be used in a variety of downstream tools. Each model is saved as a directory \n",
    "    containing a MLmodel file (YAML) that lists the flavors the model can be used with. \n",
    "    Common flavors include python_function (generic Python), sklearn, tensorflow, pytorch, \n",
    "    langchain, and openai. Models can be served via REST API using mlflow models serve.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML models \n",
    "    or mlflow.genai.evaluate() for generative AI applications. For GenAI, evaluation uses \n",
    "    Scorer objects that can be LLM-based judges (like Faithfulness, AnswerRelevance) or \n",
    "    deterministic metrics (like ExactMatch, BLEU, ROUGE). Results are logged to MLflow \n",
    "    and can be viewed in the UI with per-sample breakdowns and aggregate statistics.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base contains {len(KNOWLEDGE_BASE)} documents\")\n",
    "for i, doc in enumerate(KNOWLEDGE_BASE, 1):\n",
    "    preview = doc.strip()[:80].replace(\"\\n\", \" \")\n",
    "    print(f\"  {i}. {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 8 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=doc.strip()) for doc in KNOWLEDGE_BASE]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 8 vectors\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test query: 'What metrics does RAGAS provide?'\n",
      "Retrieved 3 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework \n",
      "    integrated with MLflow for assessing RAG pipelines. Key metrics include:\n",
      "    - Faithfulness: measures if the answer is...\n",
      "\n",
      "--- Document 2 ---\n",
      "MLflow GenAI provides specialized tools for developing and evaluating generative AI \n",
      "    applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM \n",
      "    outputs using configura...\n",
      "\n",
      "--- Document 3 ---\n",
      "Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML models \n",
      "    or mlflow.genai.evaluate() for generative AI applications. For GenAI, evaluation uses \n",
      "    Scorer objects t...\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What metrics does RAGAS provide?\"\n",
    "test_results = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Test query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(test_results)} documents:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Minimal RAG Pipeline\n",
    "\n",
    "We'll build a simple RAG chain using LangChain's LCEL (LangChain Expression Language) that:\n",
    "1. Retrieves relevant context from our FAISS vector store\n",
    "2. Formats a prompt with the context and question\n",
    "3. Generates an answer using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant answering questions about MLflow.\n",
    "Use ONLY the following context to answer the question.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is MLflow Tracking?\n",
      "\n",
      "Answer: MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code. It allows you to log and query experiments using Python, REST, R API, and Java API. The MLflow Tracking component lets you log source code, models, and visualizations. Each run records code version, start and end time, source, parameters, metrics, and artifacts.\n"
     ]
    }
   ],
   "source": [
    "test_answer = rag_chain.invoke(\"What is MLflow Tracking?\")\n",
    "print(\"Test Question: What is MLflow Tracking?\")\n",
    "print(f\"\\nAnswer: {test_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable MLflow Tracing\n",
    "\n",
    "MLflow's LangChain integration can automatically capture traces of our RAG pipeline invocations. This is essential for evaluation - RAGAS scorers analyze these traces to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment: RAG-Evaluation-Tutorial\n",
      "LangChain autologging enabled with tracing\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"RAG-Evaluation-Tutorial\")\n",
    "\n",
    "mlflow.langchain.autolog(log_traces=True)\n",
    "\n",
    "print(f\"MLflow experiment: {mlflow.get_experiment_by_name('RAG-Evaluation-Tutorial').name}\")\n",
    "print(\"LangChain autologging enabled with tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Golden Dataset Creation\n",
    "\n",
    "A **golden dataset** (also called ground truth or evaluation dataset) contains:\n",
    "- **Questions**: User queries we want to evaluate\n",
    "- **Expected Answers**: The correct/ideal responses\n",
    "- **Expected Contexts** (optional): Which documents should be retrieved\n",
    "\n",
    "This dataset allows us to systematically measure our RAG system's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden dataset contains 10 evaluation samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow Tracking used for?</td>\n",
       "      <td>MLflow Tracking is used for logging parameters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features does the MLflow Model Registry p...</td>\n",
       "      <td>The MLflow Model Registry provides model linea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What metrics does RAGAS provide for RAG evalua...</td>\n",
       "      <td>RAGAS provides four key metrics: Faithfulness ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does MLflow autolog work with LangChain?</td>\n",
       "      <td>MLflow's mlflow.langchain.autolog() automatica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the MLflow Model format?</td>\n",
       "      <td>The MLflow Model format is a standard format f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                  What is MLflow Tracking used for?   \n",
       "1  What features does the MLflow Model Registry p...   \n",
       "2  What metrics does RAGAS provide for RAG evalua...   \n",
       "3       How does MLflow autolog work with LangChain?   \n",
       "4                   What is the MLflow Model format?   \n",
       "\n",
       "                                     expected_answer  \n",
       "0  MLflow Tracking is used for logging parameters...  \n",
       "1  The MLflow Model Registry provides model linea...  \n",
       "2  RAGAS provides four key metrics: Faithfulness ...  \n",
       "3  MLflow's mlflow.langchain.autolog() automatica...  \n",
       "4  The MLflow Model format is a standard format f...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOLDEN_DATASET = [\n",
    "    {\n",
    "        \"question\": \"What is MLflow Tracking used for?\",\n",
    "        \"expected_answer\": \"MLflow Tracking is used for logging parameters, code versions, metrics, and artifacts when running machine learning code. It allows you to log and query experiments using Python, REST, R API, and Java API.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What features does the MLflow Model Registry provide?\",\n",
    "        \"expected_answer\": \"The MLflow Model Registry provides model lineage, model versioning, stage transitions (Staging, Production, Archived), and annotations. It enables teams to share models and collaborate on moving models from experimentation to production.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What metrics does RAGAS provide for RAG evaluation?\",\n",
    "        \"expected_answer\": \"RAGAS provides four key metrics: Faithfulness (measures if the answer is grounded in context), Context Precision (evaluates if relevant documents are ranked higher), Context Recall (checks if context contains all needed information), and Factual Correctness (compares output against expected answers).\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does MLflow autolog work with LangChain?\",\n",
    "        \"expected_answer\": \"MLflow's mlflow.langchain.autolog() automatically captures chain invocations, inputs, outputs, and traces without requiring manual instrumentation code. It logs metrics, parameters, and models during training runs.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the MLflow Model format?\",\n",
    "        \"expected_answer\": \"The MLflow Model format is a standard format for packaging machine learning models. Each model is saved as a directory containing a MLmodel file (YAML) that lists the flavors the model can be used with, such as python_function, sklearn, tensorflow, pytorch, langchain, and openai.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is MLflow GenAI used for?\",\n",
    "        \"expected_answer\": \"MLflow GenAI provides specialized tools for developing and evaluating generative AI applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM outputs using configurable scorers, and tracing capabilities to capture detailed execution traces.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can you run MLflow Projects?\",\n",
    "        \"expected_answer\": \"MLflow Projects can be run locally or remotely on Databricks, Kubernetes, or any compute backend. A project is a directory or Git repository containing code and a MLproject file that specifies dependencies and entry points.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What types of scorers are available for GenAI evaluation in MLflow?\",\n",
    "        \"expected_answer\": \"MLflow GenAI evaluation uses Scorer objects that can be LLM-based judges (like Faithfulness, AnswerRelevance) or deterministic metrics (like ExactMatch, BLEU, ROUGE). Results are logged to MLflow with per-sample breakdowns and aggregate statistics.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Faithfulness in RAGAS?\",\n",
    "        \"expected_answer\": \"Faithfulness is a RAGAS metric that measures if the generated answer is grounded in the retrieved context. It evaluates whether the output is factually consistent with the information provided in the context.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What frameworks support MLflow autolog?\",\n",
    "        \"expected_answer\": \"MLflow autolog supports scikit-learn, TensorFlow, Keras, PyTorch, XGBoost, LightGBM, Spark, and LangChain. It automatically logs metrics, parameters, and models during training runs for these frameworks.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(GOLDEN_DATASET)\n",
    "print(f\"Golden dataset contains {len(eval_df)} evaluation samples\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate RAG Responses for Evaluation\n",
    "\n",
    "We'll run our RAG pipeline on each question and collect the responses along with the retrieved contexts. This data will be used by RAGAS scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing traced RAG function...\n",
      "Response: MLflow is an open-source platform designed for managing the machine learning lifecycle, which includ...\n",
      "Retrieved 3 context chunks\n"
     ]
    }
   ],
   "source": [
    "# Define traced RAG function for evaluation\n",
    "# IMPORTANT: Function parameter names must match keys in data['inputs']\n",
    "# Since inputs={'question': ...}, the function must accept 'question' parameter\n",
    "\n",
    "@mlflow.trace(span_type=\"CHAIN\")\n",
    "def traced_rag_predict(question: str) -> dict:\n",
    "    \"\"\"Traced RAG prediction function for mlflow.genai.evaluate().\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer (matches inputs['question'] key)\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'response' and 'retrieved_contexts' for RAGAS scorers\n",
    "    \"\"\"\n",
    "    # Retrieval step - creates RETRIEVER span\n",
    "    with mlflow.start_span(name=\"retriever\", span_type=\"RETRIEVER\") as span:\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "        contexts = [doc.page_content for doc in retrieved_docs]\n",
    "        span.set_inputs({\"question\": question})\n",
    "        span.set_outputs({\"retrieved_contexts\": contexts})\n",
    "    \n",
    "    # Generation step - creates LLM span\n",
    "    with mlflow.start_span(name=\"generator\", span_type=\"LLM\") as span:\n",
    "        answer = rag_chain.invoke(question)\n",
    "        span.set_inputs({\"question\": question, \"contexts\": contexts})\n",
    "        span.set_outputs({\"response\": answer})\n",
    "    \n",
    "    return {\n",
    "        \"response\": answer,\n",
    "        \"retrieved_contexts\": contexts,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the traced function\n",
    "print(\"Testing traced RAG function...\")\n",
    "test_result = traced_rag_predict(question=\"What is MLflow?\")\n",
    "print(f\"Response: {test_result['response'][:100]}...\")\n",
    "print(f\"Retrieved {len(test_result['retrieved_contexts'])} context chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample evaluation record #3:\n",
      "\n",
      "Question: What metrics does RAGAS provide for RAG evaluation?\n",
      "\n",
      "Expected Answer: RAGAS provides four key metrics: Faithfulness (measures if the answer is grounded in context), Context Precision (evaluates if relevant documents are ranked higher), Context Recall (checks if context contains all needed information), and Factual Correctness (compares output against expected answers).\n",
      "\n",
      "--- Testing RAG response for this question ---\n",
      "\n",
      "RAG Answer: RAGAS provides the following metrics for RAG evaluation:\n",
      "- Faithfulness: measures if the answer is grounded in the retrieved context\n",
      "- Context Precision: evaluates if relevant documents are ranked higher\n",
      "- Context Recall: checks if the context contains all needed information\n",
      "- Factual Correctness: compares output against expected ground truth answers\n",
      "\n",
      "Retrieved Contexts (3):\n",
      "\n",
      "  Context 1: RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework \n",
      "    integrated with ML...\n",
      "\n",
      "  Context 2: Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML models \n",
      "    or mlfl...\n",
      "\n",
      "  Context 3: MLflow GenAI provides specialized tools for developing and evaluating generative AI \n",
      "    application...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview a sample from the golden dataset\n",
    "# Note: With predict_fn approach, answers are generated during evaluation\n",
    "sample_idx = 2\n",
    "print(f\"Sample evaluation record #{sample_idx + 1}:\")\n",
    "print(f\"\\nQuestion: {eval_df.iloc[sample_idx]['question']}\")\n",
    "print(f\"\\nExpected Answer: {eval_df.iloc[sample_idx]['expected_answer']}\")\n",
    "\n",
    "# Show what the traced function would produce for this question\n",
    "print(f\"\\n--- Testing RAG response for this question ---\")\n",
    "test_output = traced_rag_predict(question=eval_df.iloc[sample_idx]['question'])\n",
    "print(f\"\\nRAG Answer: {test_output['response']}\")\n",
    "print(f\"\\nRetrieved Contexts ({len(test_output['retrieved_contexts'])}):\\n\")\n",
    "for j, ctx in enumerate(test_output['retrieved_contexts'], 1):\n",
    "    print(f\"  Context {j}: {ctx[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAGAS Evaluation with MLflow\n",
    "\n",
    "Now we'll use MLflow's RAGAS integration to evaluate our RAG pipeline. The key metrics we'll compute:\n",
    "\n",
    "| Metric | What it measures | Requires |\n",
    "|--------|-----------------|----------|\n",
    "| **Faithfulness** | Is the answer grounded in the retrieved context? | answer, contexts |\n",
    "| **Context Precision** | Are relevant docs ranked higher than irrelevant ones? | question, contexts, expected_answer |\n",
    "| **Context Recall** | Does the context contain all info needed to answer? | contexts, expected_answer |\n",
    "| **Factual Correctness** | Does the answer match the expected ground truth? | answer, expected_answer |\n",
    "\n",
    "> **Note on LLM Judge**: RAGAS metrics use an LLM as a judge. For best results, use **OpenAI** (gpt-4o-mini) as the judge model even if you're using Ollama for RAG generation. Ollama/local models may have issues with litellm's structured output parsing. Set `JUDGE_PROVIDER = LLMProvider.OPENAI` below if you encounter scoring errors with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing judge model: azure:/gpt-4o-mini\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úó litellm connection failed: azure:/gpt-4o-mini\n",
      "  Error: BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call.\n",
      "\n",
      "‚ö†Ô∏è  Consider using OpenAI as judge model for reliable scoring.\n",
      "   Set JUDGE_PROVIDER = LLMProvider.OPENAI in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Test litellm connectivity (optional - helps debug scoring issues)\n",
    "import litellm\n",
    "\n",
    "def test_litellm_connection(model_uri: str) -> bool:\n",
    "    \"\"\"Test if litellm can connect to the model.\"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=model_uri,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'test' and nothing else.\"}],\n",
    "            max_tokens=10,\n",
    "        )\n",
    "        print(f\"‚úì litellm connection successful: {model_uri}\")\n",
    "        print(f\"  Response: {response.choices[0].message.content[:50]}...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó litellm connection failed: {model_uri}\")\n",
    "        print(f\"  Error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "# Test the judge model connection\n",
    "judge_model_uri = get_mlflow_model_uri(PROVIDER)\n",
    "print(f\"Testing judge model: {judge_model_uri}\\n\")\n",
    "litellm_ok = test_litellm_connection(judge_model_uri)\n",
    "\n",
    "if not litellm_ok:\n",
    "    print(\"\\n‚ö†Ô∏è  Consider using OpenAI as judge model for reliable scoring.\")\n",
    "    print(\"   Set JUDGE_PROVIDER = LLMProvider.OPENAI in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge model: azure:/gpt-4o-mini\n",
      "(Change JUDGE_PROVIDER to LLMProvider.OPENAI if scoring fails with Ollama)\n",
      "\n",
      "Configured 2 RAGAS scorers:\n",
      "  - Faithfulness\n",
      "  - FactualCorrectness\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai.scorers.ragas import (\n",
    "    Faithfulness,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    FactualCorrectness,\n",
    ")\n",
    "\n",
    "# Configure the judge model for RAGAS evaluation\n",
    "# For reliable scoring, use OpenAI even when using Ollama for RAG generation\n",
    "JUDGE_PROVIDER = PROVIDER  # Change to LLMProvider.OPENAI for better results\n",
    "judge_model_uri = get_mlflow_model_uri(JUDGE_PROVIDER)\n",
    "\n",
    "print(f\"Judge model: {judge_model_uri}\")\n",
    "print(f\"(Change JUDGE_PROVIDER to LLMProvider.OPENAI if scoring fails with Ollama)\\n\")\n",
    "\n",
    "# Note: ContextPrecision and ContextRecall require traces with RETRIEVER spans\n",
    "# For evaluation without traces, use Faithfulness and FactualCorrectness\n",
    "scorers = [\n",
    "    Faithfulness(model=judge_model_uri),\n",
    "    FactualCorrectness(model=judge_model_uri),\n",
    "    # These require traces with retriever spans - may show errors without proper tracing:\n",
    "    # ContextPrecision(model=judge_model_uri),\n",
    "    # ContextRecall(model=judge_model_uri),\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(scorers)} RAGAS scorers:\")\n",
    "for scorer in scorers:\n",
    "    print(f\"  - {type(scorer).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Evaluation Data\n",
    "\n",
    "MLflow's `genai.evaluate()` expects data in a specific format. We need to map our data to the expected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 10 samples for evaluation\n",
      "\n",
      "Sample format:\n",
      "  inputs: ['question']\n",
      "  expectations: ['expected_answer']\n",
      "\n",
      "Note: outputs will be generated by traced_rag_predict() during evaluation\n",
      "      The traced function creates RETRIEVER spans for Faithfulness metric\n"
     ]
    }
   ],
   "source": [
    "# Prepare evaluation data for predict_fn approach\n",
    "# With predict_fn, we pass inputs and expectations - outputs come from the traced function\n",
    "eval_data = []\n",
    "for _, row in eval_df.iterrows():\n",
    "    eval_data.append({\n",
    "        \"inputs\": {\"question\": row[\"question\"]},\n",
    "        \"expectations\": {\n",
    "            \"expected_answer\": row[\"expected_answer\"],\n",
    "        },\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(eval_data)} samples for evaluation\")\n",
    "print(f\"\\nSample format:\")\n",
    "print(f\"  inputs: {list(eval_data[0]['inputs'].keys())}\")\n",
    "print(f\"  expectations: {list(eval_data[0]['expectations'].keys())}\")\n",
    "print(f\"\\nNote: outputs will be generated by traced_rag_predict() during evaluation\")\n",
    "print(f\"      The traced function creates RETRIEVER spans for Faithfulness metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:43:23 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2026/01/08 11:43:23 WARNING mlflow.tracing.fluent: Failed to start span VectorStoreRetriever: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAGAS evaluation with traced predict_fn...\n",
      "This generates traces with RETRIEVER spans for Faithfulness metric.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:43:23 WARNING mlflow.tracing.fluent: Failed to start span RunnableSequence: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e557ff65744247caa9953adc458b8999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mragas-evaluation-traced\u001b[0m\n",
      "  Run ID: \u001b[94md0e7244200984c048bed0838cb5f4de8\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "\n",
      "Evaluation complete! Run ID: d0e7244200984c048bed0838cb5f4de8\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RAGAS evaluation with traced predict_fn...\")\n",
    "print(\"This generates traces with RETRIEVER spans for Faithfulness metric.\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"ragas-evaluation-traced\") as run:\n",
    "    mlflow.log_param(\"provider\", PROVIDER.value)\n",
    "    mlflow.log_param(\"model\", MODEL_CONFIG[PROVIDER][\"chat_model\"])\n",
    "    mlflow.log_param(\"num_samples\", len(eval_data))\n",
    "    mlflow.log_param(\"retriever_k\", 3)\n",
    "    mlflow.log_param(\"evaluation_mode\", \"predict_fn\")\n",
    "\n",
    "    # Use predict_fn to generate traces with RETRIEVER spans\n",
    "    # This allows Faithfulness scorer to access retrieved_contexts\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        predict_fn=traced_rag_predict,\n",
    "        data=eval_data,\n",
    "        scorers=scorers,\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "print(f\"\\nEvaluation complete! Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MLflow Results Analysis\n",
    "\n",
    "Let's examine the evaluation results both programmatically and understand how to view them in the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìä RAGAS Metrics:\n",
      "----------------------------------------\n",
      "  ‚úì Faithfulness/value: 1.000 (¬±0.000) [10/10 samples]\n",
      "  ‚úì FactualCorrectness/value: 0.488 (¬±0.276) [10/10 samples]\n",
      "\n",
      "üìù Summary: 2 metrics succeeded, 0 metrics failed\n",
      "   Total samples: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = eval_results.tables[\"eval_results\"]\n",
    "\n",
    "# Find RAGAS scorer columns (Faithfulness, FactualCorrectness, Context*)\n",
    "import pandas as pd\n",
    "ragas_metrics = ['Faithfulness', 'FactualCorrectness', 'ContextPrecision', 'ContextRecall']\n",
    "value_columns = [col for col in results_df.columns \n",
    "                 if col.endswith('/value') and any(m in col for m in ragas_metrics)]\n",
    "error_columns = [col for col in results_df.columns \n",
    "                 if col.endswith('/error') and any(m in col for m in ragas_metrics)]\n",
    "\n",
    "print(\"\\nüìä RAGAS Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "successful_metrics = 0\n",
    "failed_metrics = 0\n",
    "\n",
    "for col in value_columns:\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    numeric_col = pd.to_numeric(results_df[col], errors='coerce')\n",
    "    non_null = numeric_col.dropna()\n",
    "    total = len(results_df)\n",
    "    success_count = len(non_null)\n",
    "    \n",
    "    if success_count > 0:\n",
    "        mean_val = non_null.mean()\n",
    "        std_val = non_null.std() if len(non_null) > 1 else 0\n",
    "        print(f\"  ‚úì {col}: {mean_val:.3f} (¬±{std_val:.3f}) [{success_count}/{total} samples]\")\n",
    "        successful_metrics += 1\n",
    "    else:\n",
    "        print(f\"  ‚úó {col}: NO SCORES (0/{total} samples succeeded)\")\n",
    "        failed_metrics += 1\n",
    "\n",
    "print(f\"\\nüìù Summary: {successful_metrics} metrics succeeded, {failed_metrics} metrics failed\")\n",
    "print(f\"   Total samples: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All metrics computed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Debug: Show error messages if scoring failed\n",
    "if failed_metrics > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç DIAGNOSTIC: Error Details for Failed Metrics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col in error_columns:\n",
    "        metric_name = col.replace('/error', '')\n",
    "        errors = results_df[col].dropna()\n",
    "        \n",
    "        if len(errors) > 0:\n",
    "            print(f\"\\n‚ùå {metric_name}:\")\n",
    "            # Get first unique error message\n",
    "            unique_errors = errors.unique()\n",
    "            for err in unique_errors[:2]:  # Show max 2 unique errors\n",
    "                # Truncate long error messages\n",
    "                err_str = str(err)[:300]\n",
    "                if len(str(err)) > 300:\n",
    "                    err_str += \"...\"\n",
    "                print(f\"   {err_str}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üí° Common fixes:\")\n",
    "    print(\"   1. Use OpenAI as judge: JUDGE_PROVIDER = LLMProvider.OPENAI\")\n",
    "    print(\"   2. For Ollama: ensure model is running and OLLAMA_API_BASE is set\")\n",
    "    print(\"   3. ContextPrecision/ContextRecall require traces with RETRIEVER spans\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All metrics computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Faithfulness/value</th>\n",
       "      <th>FactualCorrectness/value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Faithfulness/value  FactualCorrectness/value\n",
       "0                 1.0                      0.31\n",
       "1                 1.0                      0.77\n",
       "2                 1.0                      0.44\n",
       "3                 1.0                      0.30\n",
       "4                 1.0                      0.80\n",
       "5                 1.0                      0.40\n",
       "6                 1.0                      0.43\n",
       "7                 1.0                      1.00\n",
       "8                 1.0                      0.12\n",
       "9                 1.0                      0.31"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_cols = [\"inputs.question\"] + value_columns\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "\n",
    "results_summary = results_df[available_cols].copy()\n",
    "if \"inputs.question\" in results_summary.columns:\n",
    "    results_summary[\"inputs.question\"] = results_summary[\"inputs.question\"].str[:50] + \"...\"\n",
    "\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Identifying Low-Scoring Samples:\n",
      "----------------------------------------\n",
      "\n",
      "‚ö†Ô∏è  FactualCorrectness/value < 0.5: 7 samples\n",
      "    - [0.31] N/A...\n",
      "    - [0.44] N/A...\n",
      "    - [0.30] N/A...\n",
      "    - [0.40] N/A...\n",
      "    - [0.43] N/A...\n",
      "    - [0.12] N/A...\n",
      "    - [0.31] N/A...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Identifying Low-Scoring Samples:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for col in value_columns:\n",
    "    if col in results_df.columns:\n",
    "        numeric_col = pd.to_numeric(results_df[col], errors='coerce')\n",
    "        low_mask = numeric_col < 0.5\n",
    "        low_scores = results_df[low_mask]\n",
    "        if len(low_scores) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  {col} < 0.5: {len(low_scores)} samples\")\n",
    "            for idx, row in low_scores.iterrows():\n",
    "                question = str(row.get(\"inputs.question\", \"N/A\"))[:60]\n",
    "                score = numeric_col.loc[idx]\n",
    "                if pd.notna(score):\n",
    "                    print(f\"    - [{score:.2f}] {question}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà VIEW RESULTS IN MLFLOW UI\n",
      "============================================================\n",
      "\n",
      "To view detailed results in the MLflow UI:\n",
      "\n",
      "1. Start MLflow UI (if not running):\n",
      "   $ mlflow ui --port 5000\n",
      "\n",
      "2. Open http://localhost:5000 in your browser\n",
      "\n",
      "3. Navigate to:\n",
      "   - Experiment: 'RAG-Evaluation-Tutorial'\n",
      "   - Run: 'ragas-evaluation'\n",
      "\n",
      "4. In the run details, you'll find:\n",
      "   - Parameters: model configuration\n",
      "   - Metrics: aggregate RAGAS scores\n",
      "   - Artifacts: detailed evaluation tables\n",
      "   - Traces: individual RAG invocations\n",
      "\n",
      "Run ID: d0e7244200984c048bed0838cb5f4de8\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìà VIEW RESULTS IN MLFLOW UI\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "To view detailed results in the MLflow UI:\n",
    "\n",
    "1. Start MLflow UI (if not running):\n",
    "   $ mlflow ui --port 5000\n",
    "\n",
    "2. Open http://localhost:5000 in your browser\n",
    "\n",
    "3. Navigate to:\n",
    "   - Experiment: 'RAG-Evaluation-Tutorial'\n",
    "   - Run: 'ragas-evaluation'\n",
    "\n",
    "4. In the run details, you'll find:\n",
    "   - Parameters: model configuration\n",
    "   - Metrics: aggregate RAGAS scores\n",
    "   - Artifacts: detailed evaluation tables\n",
    "   - Traces: individual RAG invocations\n",
    "\"\"\")\n",
    "print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Provider Comparison (Optional)\n",
    "\n",
    "One of the strengths of this setup is the ability to easily compare RAG performance across different LLM providers. Below is a template for running comparative evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider comparison function defined.\n",
      "\n",
      "To run comparison across providers:\n",
      "\n",
      "comparison = run_provider_comparison(\n",
      "    providers=[LLMProvider.OPENAI, LLMProvider.AZURE_OPENAI, LLMProvider.OLLAMA],\n",
      "    eval_questions=[q['question'] for q in GOLDEN_DATASET[:3]]\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_provider_comparison(providers: list[LLMProvider], eval_questions: list[str]):\n",
    "    \"\"\"\n",
    "    Run evaluation across multiple providers and compare results.\n",
    "\n",
    "    Note: This requires appropriate API keys/setup for each provider.\n",
    "    \"\"\"\n",
    "    comparison_results = {}\n",
    "\n",
    "    for provider in providers:\n",
    "        try:\n",
    "            validate_environment(provider)\n",
    "\n",
    "            provider_llm = get_llm(provider)\n",
    "            provider_embeddings = get_embeddings(provider)\n",
    "\n",
    "            provider_vectorstore = FAISS.from_documents(splits, provider_embeddings)\n",
    "            provider_retriever = provider_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "            provider_chain = (\n",
    "                {\n",
    "                    \"context\": provider_retriever | format_docs,\n",
    "                    \"question\": RunnablePassthrough(),\n",
    "                }\n",
    "                | RAG_PROMPT\n",
    "                | provider_llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            answers = [provider_chain.invoke(q) for q in eval_questions]\n",
    "            comparison_results[provider.value] = answers\n",
    "            print(f\"‚úì {provider.value}: Generated {len(answers)} responses\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó {provider.value}: Skipped ({e})\")\n",
    "\n",
    "    return comparison_results\n",
    "\n",
    "\n",
    "print(\"Provider comparison function defined.\")\n",
    "print(\"\\nTo run comparison across providers:\")\n",
    "print(\"\"\"\n",
    "comparison = run_provider_comparison(\n",
    "    providers=[LLMProvider.OPENAI, LLMProvider.AZURE_OPENAI, LLMProvider.OLLAMA],\n",
    "    eval_questions=[q['question'] for q in GOLDEN_DATASET[:3]]\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Multi-Provider Setup**: Configured LLM providers (OpenAI, Azure OpenAI, Ollama) with a clean factory pattern\n",
    "\n",
    "2. **RAG Pipeline**: Built a minimal but functional RAG system using:\n",
    "   - FAISS vector store for document retrieval\n",
    "   - LangChain for chain orchestration\n",
    "   - MLflow for experiment tracking and tracing\n",
    "\n",
    "3. **Golden Dataset**: Created an evaluation dataset with questions and expected answers\n",
    "\n",
    "4. **RAGAS Evaluation**: Assessed RAG quality using four key metrics:\n",
    "   - **Faithfulness**: Grounding in retrieved context\n",
    "   - **Context Precision**: Relevance ranking of retrieved documents\n",
    "   - **Context Recall**: Completeness of retrieved information\n",
    "   - **Factual Correctness**: Alignment with ground truth\n",
    "\n",
    "5. **MLflow Integration**: Logged all results for systematic tracking and comparison\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Expand the knowledge base with your domain-specific documents\n",
    "- Create a larger golden dataset (50-100+ samples) for statistically significant evaluation\n",
    "- Experiment with different retrieval strategies (k values, reranking)\n",
    "- Compare chunking strategies and their impact on metrics\n",
    "- Set up automated evaluation in CI/CD pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ Tutorial Complete!\n",
      "============================================================\n",
      "\n",
      "Summary:\n",
      "  - Provider: azure_openai\n",
      "  - Model: gpt-4o-mini\n",
      "  - Samples evaluated: 10\n",
      "  - MLflow Run ID: d0e7244200984c048bed0838cb5f4de8\n",
      "\n",
      "View results: mlflow ui --port 5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Tutorial Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Summary:\n",
    "  - Provider: {PROVIDER.value}\n",
    "  - Model: {MODEL_CONFIG[PROVIDER]['chat_model']}\n",
    "  - Samples evaluated: {len(eval_data)}\n",
    "  - MLflow Run ID: {run_id}\n",
    "\n",
    "View results: mlflow ui --port 5000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Tutorials\n",
    "\n",
    "- Agent as judge: Evaluating RAG with Agent and MLflow\n",
    "  - https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/agentic-overview/index.html\n",
    "\n",
    "- Optimize prompts for RAG evaluation with MLflow\n",
    "  - https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extras: Comparing RAG Variants with MLflow\n",
    "\n",
    "One of MLflow's key strengths is enabling systematic A/B comparisons between different RAG configurations. Here's how to structure experiments comparing variants like chunk sizes, models, or retrieval strategies.\n",
    "\n",
    "### Example: Comparing Chunk Sizes\n",
    "\n",
    "```python\n",
    "# Define variants to compare\n",
    "CHUNK_SIZES = [256, 512, 1024]\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    # Create vector store with this chunk size\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size // 10\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # Update the RAG chain with new retriever\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Run evaluation with descriptive run name\n",
    "    with mlflow.start_run(run_name=f\"chunk-size-{chunk_size}\"):\n",
    "        mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "        mlflow.log_param(\"chunk_overlap\", chunk_size // 10)\n",
    "        mlflow.log_param(\"num_chunks\", len(chunks))\n",
    "        \n",
    "        eval_results = mlflow.genai.evaluate(\n",
    "            predict_fn=traced_rag_predict,\n",
    "            data=eval_data,\n",
    "            scorers=scorers,\n",
    "        )\n",
    "```\n",
    "\n",
    "### Example: Comparing Different LLMs\n",
    "\n",
    "```python\n",
    "# Define model variants\n",
    "MODELS = [\n",
    "    {\"name\": \"gpt-4o-mini\", \"provider\": LLMProvider.OPENAI},\n",
    "    {\"name\": \"gpt-4o\", \"provider\": LLMProvider.OPENAI},\n",
    "    {\"name\": \"llama3.2:3b\", \"provider\": LLMProvider.OLLAMA},\n",
    "]\n",
    "\n",
    "for model_config in MODELS:\n",
    "    # Update MODEL_CONFIG and recreate LLM\n",
    "    MODEL_CONFIG[model_config[\"provider\"]][\"chat_model\"] = model_config[\"name\"]\n",
    "    llm = get_llm(model_config[\"provider\"])\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"model-{model_config['name']}\"):\n",
    "        mlflow.log_param(\"model\", model_config[\"name\"])\n",
    "        mlflow.log_param(\"provider\", model_config[\"provider\"].value)\n",
    "        \n",
    "        eval_results = mlflow.genai.evaluate(\n",
    "            predict_fn=traced_rag_predict,\n",
    "            data=eval_data,\n",
    "            scorers=scorers,\n",
    "        )\n",
    "```\n",
    "\n",
    "### Example: Comparing Retrieval Strategies\n",
    "\n",
    "```python\n",
    "# Compare different k values for retrieval\n",
    "K_VALUES = [1, 3, 5, 10]\n",
    "\n",
    "for k in K_VALUES:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"retriever-k-{k}\"):\n",
    "        mlflow.log_param(\"retriever_k\", k)\n",
    "        \n",
    "        eval_results = mlflow.genai.evaluate(\n",
    "            predict_fn=traced_rag_predict,\n",
    "            data=eval_data,\n",
    "            scorers=scorers,\n",
    "        )\n",
    "```\n",
    "\n",
    "### Comparing Results in MLflow UI\n",
    "\n",
    "After running multiple variants:\n",
    "\n",
    "1. Open MLflow UI: `mlflow ui --port 5000`\n",
    "2. Navigate to your experiment\n",
    "3. Select runs to compare using checkboxes\n",
    "4. Click **Compare** to see side-by-side metrics\n",
    "5. Use **Chart** view to visualize metric differences\n",
    "\n",
    "You can also compare programmatically:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Get all runs from experiment\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[\"RAG-Evaluation-Tutorial\"],\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.`FactualCorrectness/value/mean` DESC\"]\n",
    ")\n",
    "\n",
    "# Display comparison table\n",
    "comparison_cols = [\n",
    "    \"run_name\", \n",
    "    \"params.chunk_size\",\n",
    "    \"params.model\",\n",
    "    \"metrics.Faithfulness/value/mean\",\n",
    "    \"metrics.FactualCorrectness/value/mean\"\n",
    "]\n",
    "print(runs[[c for c in comparison_cols if c in runs.columns]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extras: MLflow MCP Server Integration\n",
    "\n",
    "MLflow provides a **Model Context Protocol (MCP)** server that enables AI assistants (Claude, VS Code, Cursor) to interact with your MLflow traces programmatically. This is powerful for:\n",
    "\n",
    "- Debugging production issues via natural language queries\n",
    "- Performance analysis of your RAG pipelines\n",
    "- Logging feedback and assessments through AI assistants\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install 'mlflow[mcp]>=3.5.1'\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Add to your editor's MCP configuration:\n",
    "\n",
    "**VS Code** (`.vscode/mcp.json`):\n",
    "```json\n",
    "{\n",
    "  \"servers\": {\n",
    "    \"mlflow-mcp\": {\n",
    "      \"command\": \"uv\",\n",
    "      \"args\": [\"run\", \"--with\", \"mlflow[mcp]>=3.5.1\", \"mlflow\", \"mcp\", \"run\"],\n",
    "      \"env\": {\n",
    "        \"MLFLOW_TRACKING_URI\": \"http://localhost:5000\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Cursor** (`.cursor/mcp.json`):\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"mlflow-mcp\": {\n",
    "      \"command\": \"uv\",\n",
    "      \"args\": [\"run\", \"--with\", \"mlflow[mcp]>=3.5.1\", \"mlflow\", \"mcp\", \"run\"],\n",
    "      \"env\": {\n",
    "        \"MLFLOW_TRACKING_URI\": \"http://localhost:5000\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Available MCP Tools\n",
    "\n",
    "| Tool | Description |\n",
    "|------|-------------|\n",
    "| `search_traces` | Search and filter traces in experiments |\n",
    "| `get_trace` | Get detailed trace information |\n",
    "| `log_feedback` | Log evaluation scores or judgments |\n",
    "| `log_expectation` | Log ground truth labels |\n",
    "| `set_trace_tag` | Add custom tags to traces |\n",
    "| `delete_traces` | Delete traces by ID or timestamp |\n",
    "\n",
    "### Example Queries to Your AI Assistant\n",
    "\n",
    "Once configured, you can ask your AI assistant:\n",
    "\n",
    "- *\"Find all failed traces in my RAG-Evaluation-Tutorial experiment\"*\n",
    "- *\"Show me traces with execution time over 5 seconds\"*\n",
    "- *\"Log a relevance score of 0.9 for trace tr-abc123\"*\n",
    "- *\"What's the average faithfulness score across my recent traces?\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
