[
  "MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code. It allows you to log and query experiments using Python, REST, R API, and Java API. The MLflow Tracking component lets you log source code, models, and visualizations. Each run records: code version, start and end time, source, parameters, metrics, and artifacts.",
  "The MLflow Model Registry is a centralized model store that provides model lineage, model versioning, stage transitions (Staging, Production, Archived), and annotations. It enables teams to share models, collaborate on moving models from experimentation to production, and integrate with CI/CD pipelines. Models can be registered from any MLflow run or loaded directly from the registry.",
  "MLflow GenAI provides specialized tools for developing and evaluating generative AI applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM outputs using configurable scorers. The framework supports both built-in judges and custom evaluation metrics. Tracing capabilities allow you to capture detailed execution traces including inputs, outputs, and intermediate steps.",
  "RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework integrated with MLflow for assessing RAG pipelines. Key metrics include: Faithfulness (measures if the answer is grounded in the retrieved context), Context Precision (evaluates if relevant documents are ranked higher), Context Recall (checks if the context contains all needed information), Factual Correctness (compares output against expected ground truth answers).",
  "MLflow Projects package code in a reusable, reproducible form. A project is simply a directory or Git repository containing code and a MLproject file that specifies dependencies and entry points. Projects can be run locally or remotely on Databricks, Kubernetes, or any compute backend. The MLproject file uses YAML format to define the project name, environment, and entry points with parameters.",
  "MLflow's autolog feature automatically logs metrics, parameters, and models during training runs for popular ML libraries. Supported frameworks include scikit-learn, TensorFlow, Keras, PyTorch, XGBoost, LightGBM, Spark, and LangChain. For LangChain, mlflow.langchain.autolog() captures chain invocations, inputs, outputs, and traces without requiring manual instrumentation code.",
  "The MLflow Model format is a standard format for packaging machine learning models that can be used in a variety of downstream tools. Each model is saved as a directory containing a MLmodel file (YAML) that lists the flavors the model can be used with. Common flavors include python_function (generic Python), sklearn, tensorflow, pytorch, langchain, and openai. Models can be served via REST API using mlflow models serve.",
  "Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML models or mlflow.genai.evaluate() for generative AI applications. For GenAI, evaluation uses Scorer objects that can be LLM-based judges (like Faithfulness, AnswerRelevance) or deterministic metrics (like ExactMatch, BLEU, ROUGE). Results are logged to MLflow and can be viewed in the UI with per-sample breakdowns and aggregate statistics.",
  "MLflow Model Serving enables deploying models as REST API endpoints. You can serve models locally using mlflow models serve -m <model_uri> -p <port> or deploy to cloud platforms. The serving layer automatically handles model loading, input parsing, and prediction formatting. Supported deployment targets include AWS SageMaker, Azure ML, Databricks, and Kubernetes.",
  "MLflow Recipes (formerly MLflow Pipelines) provide predefined templates for common ML workflows. The regression and classification recipes include steps for data ingestion, splitting, transformation, training, evaluation, and registration. Recipes use a recipe.yaml configuration file and modular step definitions to create reproducible, production-ready ML pipelines.",
  "The MLflow CLI provides commands for running projects, serving models, and managing experiments. Key commands include: mlflow run (execute a project), mlflow models serve (deploy a model), mlflow experiments (manage experiments), mlflow artifacts (download artifacts), and mlflow db upgrade (upgrade the tracking database schema).",
  "MLflow's REST API allows programmatic access to the tracking server. Endpoints include /api/2.0/mlflow/experiments for experiment management, /api/2.0/mlflow/runs for run operations, and /api/2.0/mlflow/artifacts for artifact access. The API supports creating experiments, logging metrics, searching runs, and downloading artifacts.",
  "MLflow experiments organize runs into logical groups. Each experiment has a unique name and ID. You can create experiments using mlflow.create_experiment() or mlflow experiments create CLI command. Runs within an experiment can be compared, filtered, and sorted using the MLflow UI or search_runs() API with filter expressions.",
  "MLflow provides run comparison capabilities through the UI and API. The Compare Runs view shows metrics, parameters, and artifacts side-by-side. You can use mlflow.search_runs() with filter_string parameter to query runs programmatically. Supported operators include =, !=, >, <, LIKE, and IN for filtering by metrics, parameters, and tags.",
  "MLflow artifacts are files associated with runs, such as models, data files, and plots. Artifacts are stored in a configured artifact location (local filesystem, S3, Azure Blob, GCS, or HDFS). Use mlflow.log_artifact() for single files, mlflow.log_artifacts() for directories, and mlflow.get_artifact_uri() to retrieve artifact locations.",
  "Model signatures in MLflow define the expected input and output schema for models. Signatures include column names, types, and optional shapes. You can infer signatures using mlflow.models.infer_signature() or define them manually. Input examples provide sample data for testing and documentation, logged alongside the model.",
  "MLflow on Databricks provides managed MLflow tracking, model registry, and model serving. Databricks automatically logs experiments to a workspace MLflow server. Unity Catalog integration enables fine-grained access control for models. Databricks Model Serving offers serverless endpoints with automatic scaling and A/B testing capabilities.",
  "MLflow supports multiple environment managers for reproducibility. Projects can specify conda environments (conda.yaml), virtualenv (python_env.yaml), or Docker containers. The environment is automatically created when running projects. Use mlflow.pyfunc.get_model_dependencies() to retrieve a model's environment specification.",
  "MLflow Prompt Engineering tools help develop and version prompts for LLM applications. The mlflow.llm module provides utilities for prompt templating, versioning, and A/B testing. Prompts can be logged as artifacts, tracked with parameters, and associated with evaluation metrics to optimize prompt performance.",
  "MLflow integrates with LangChain through mlflow.langchain module. The integration supports logging LangChain chains, agents, and retrievers as MLflow models. Autologging captures chain structure, prompts, and execution traces. Models can be loaded and served using standard MLflow APIs, enabling deployment of complex LangChain applications."
]
