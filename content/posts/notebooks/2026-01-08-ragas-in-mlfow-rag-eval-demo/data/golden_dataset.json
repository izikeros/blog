[
  {
    "question": "What is MLflow Tracking used for?",
    "ground_truth": "MLflow Tracking is used for logging parameters, code versions, metrics, and artifacts when running machine learning code. It allows you to log and query experiments using Python, REST, R API, and Java API.",
    "contexts": [
      "MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code. It allows you to log and query experiments using Python, REST, R API, and Java API."
    ]
  },
  {
    "question": "What features does the MLflow Model Registry provide?",
    "ground_truth": "The MLflow Model Registry provides model lineage, model versioning, stage transitions (Staging, Production, Archived), and annotations. It enables teams to share models and collaborate on moving models from experimentation to production.",
    "contexts": [
      "The MLflow Model Registry is a centralized model store that provides model lineage, model versioning, stage transitions (Staging, Production, Archived), and annotations. It enables teams to share models, collaborate on moving models from experimentation to production, and integrate with CI/CD pipelines."
    ]
  },
  {
    "question": "What metrics does RAGAS provide for RAG evaluation?",
    "ground_truth": "RAGAS provides four key metrics: Faithfulness (measures if the answer is grounded in context), Context Precision (evaluates if relevant documents are ranked higher), Context Recall (checks if context contains all needed information), and Factual Correctness (compares output against expected answers).",
    "contexts": [
      "Key metrics include: Faithfulness (measures if the answer is grounded in the retrieved context), Context Precision (evaluates if relevant documents are ranked higher), Context Recall (checks if the context contains all needed information), Factual Correctness (compares output against expected ground truth answers)."
    ]
  },
  {
    "question": "How does MLflow autolog work with LangChain?",
    "ground_truth": "MLflow's mlflow.langchain.autolog() automatically captures chain invocations, inputs, outputs, and traces without requiring manual instrumentation code. It logs metrics, parameters, and models during training runs.",
    "contexts": [
      "For LangChain, mlflow.langchain.autolog() captures chain invocations, inputs, outputs, and traces without requiring manual instrumentation code."
    ]
  },
  {
    "question": "What is the MLflow Model format?",
    "ground_truth": "The MLflow Model format is a standard format for packaging machine learning models. Each model is saved as a directory containing a MLmodel file (YAML) that lists the flavors the model can be used with, such as python_function, sklearn, tensorflow, pytorch, langchain, and openai.",
    "contexts": [
      "The MLflow Model format is a standard format for packaging machine learning models that can be used in a variety of downstream tools. Each model is saved as a directory containing a MLmodel file (YAML) that lists the flavors the model can be used with."
    ]
  },
  {
    "question": "What is MLflow GenAI used for?",
    "ground_truth": "MLflow GenAI provides specialized tools for developing and evaluating generative AI applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM outputs using configurable scorers, and tracing capabilities to capture detailed execution traces.",
    "contexts": [
      "MLflow GenAI provides specialized tools for developing and evaluating generative AI applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM outputs using configurable scorers."
    ]
  },
  {
    "question": "How can you run MLflow Projects?",
    "ground_truth": "MLflow Projects can be run locally or remotely on Databricks, Kubernetes, or any compute backend. A project is a directory or Git repository containing code and a MLproject file that specifies dependencies and entry points.",
    "contexts": [
      "Projects can be run locally or remotely on Databricks, Kubernetes, or any compute backend. The MLproject file uses YAML format to define the project name, environment, and entry points with parameters."
    ]
  },
  {
    "question": "What types of scorers are available for GenAI evaluation in MLflow?",
    "ground_truth": "MLflow GenAI evaluation uses Scorer objects that can be LLM-based judges (like Faithfulness, AnswerRelevance) or deterministic metrics (like ExactMatch, BLEU, ROUGE). Results are logged to MLflow with per-sample breakdowns and aggregate statistics.",
    "contexts": [
      "For GenAI, evaluation uses Scorer objects that can be LLM-based judges (like Faithfulness, AnswerRelevance) or deterministic metrics (like ExactMatch, BLEU, ROUGE). Results are logged to MLflow and can be viewed in the UI with per-sample breakdowns and aggregate statistics."
    ]
  },
  {
    "question": "What is Faithfulness in RAGAS?",
    "ground_truth": "Faithfulness is a RAGAS metric that measures if the generated answer is grounded in the retrieved context. It evaluates whether the output is factually consistent with the information provided in the context.",
    "contexts": [
      "Faithfulness (measures if the answer is grounded in the retrieved context)"
    ]
  },
  {
    "question": "What frameworks support MLflow autolog?",
    "ground_truth": "MLflow autolog supports scikit-learn, TensorFlow, Keras, PyTorch, XGBoost, LightGBM, Spark, and LangChain. It automatically logs metrics, parameters, and models during training runs for these frameworks.",
    "contexts": [
      "Supported frameworks include scikit-learn, TensorFlow, Keras, PyTorch, XGBoost, LightGBM, Spark, and LangChain."
    ]
  },
  {
    "question": "How can you deploy MLflow models as REST endpoints?",
    "ground_truth": "You can serve models locally using mlflow models serve -m <model_uri> -p <port> or deploy to cloud platforms like AWS SageMaker, Azure ML, Databricks, and Kubernetes. The serving layer automatically handles model loading, input parsing, and prediction formatting.",
    "contexts": [
      "MLflow Model Serving enables deploying models as REST API endpoints. You can serve models locally using mlflow models serve -m <model_uri> -p <port> or deploy to cloud platforms.",
      "Supported deployment targets include AWS SageMaker, Azure ML, Databricks, and Kubernetes."
    ]
  },
  {
    "question": "What are MLflow Recipes and what steps do they include?",
    "ground_truth": "MLflow Recipes (formerly MLflow Pipelines) provide predefined templates for common ML workflows. The regression and classification recipes include steps for data ingestion, splitting, transformation, training, evaluation, and registration.",
    "contexts": [
      "MLflow Recipes (formerly MLflow Pipelines) provide predefined templates for common ML workflows. The regression and classification recipes include steps for data ingestion, splitting, transformation, training, evaluation, and registration."
    ]
  },
  {
    "question": "What are the key MLflow CLI commands?",
    "ground_truth": "Key MLflow CLI commands include: mlflow run (execute a project), mlflow models serve (deploy a model), mlflow experiments (manage experiments), mlflow artifacts (download artifacts), and mlflow db upgrade (upgrade the tracking database schema).",
    "contexts": [
      "Key commands include: mlflow run (execute a project), mlflow models serve (deploy a model), mlflow experiments (manage experiments), mlflow artifacts (download artifacts), and mlflow db upgrade (upgrade the tracking database schema)."
    ]
  },
  {
    "question": "How can you access MLflow programmatically via REST API?",
    "ground_truth": "MLflow's REST API allows programmatic access to the tracking server. Endpoints include /api/2.0/mlflow/experiments for experiment management, /api/2.0/mlflow/runs for run operations, and /api/2.0/mlflow/artifacts for artifact access.",
    "contexts": [
      "MLflow's REST API allows programmatic access to the tracking server. Endpoints include /api/2.0/mlflow/experiments for experiment management, /api/2.0/mlflow/runs for run operations, and /api/2.0/mlflow/artifacts for artifact access."
    ]
  },
  {
    "question": "How do you create and organize MLflow experiments?",
    "ground_truth": "MLflow experiments organize runs into logical groups. Each experiment has a unique name and ID. You can create experiments using mlflow.create_experiment() or mlflow experiments create CLI command.",
    "contexts": [
      "MLflow experiments organize runs into logical groups. Each experiment has a unique name and ID. You can create experiments using mlflow.create_experiment() or mlflow experiments create CLI command."
    ]
  },
  {
    "question": "How can you compare runs in MLflow?",
    "ground_truth": "MLflow provides run comparison through the UI Compare Runs view and programmatically via mlflow.search_runs() with filter_string parameter. Supported operators include =, !=, >, <, LIKE, and IN for filtering by metrics, parameters, and tags.",
    "contexts": [
      "MLflow provides run comparison capabilities through the UI and API. The Compare Runs view shows metrics, parameters, and artifacts side-by-side. You can use mlflow.search_runs() with filter_string parameter to query runs programmatically."
    ]
  },
  {
    "question": "How are artifacts stored and managed in MLflow?",
    "ground_truth": "MLflow artifacts are files associated with runs stored in configured locations (local filesystem, S3, Azure Blob, GCS, or HDFS). Use mlflow.log_artifact() for single files, mlflow.log_artifacts() for directories, and mlflow.get_artifact_uri() to retrieve artifact locations.",
    "contexts": [
      "MLflow artifacts are files associated with runs, such as models, data files, and plots. Artifacts are stored in a configured artifact location (local filesystem, S3, Azure Blob, GCS, or HDFS). Use mlflow.log_artifact() for single files, mlflow.log_artifacts() for directories, and mlflow.get_artifact_uri() to retrieve artifact locations."
    ]
  },
  {
    "question": "What are model signatures and input examples in MLflow?",
    "ground_truth": "Model signatures define the expected input and output schema for models, including column names, types, and optional shapes. You can infer signatures using mlflow.models.infer_signature() or define them manually. Input examples provide sample data for testing and documentation.",
    "contexts": [
      "Model signatures in MLflow define the expected input and output schema for models. Signatures include column names, types, and optional shapes. You can infer signatures using mlflow.models.infer_signature() or define them manually. Input examples provide sample data for testing and documentation, logged alongside the model."
    ]
  },
  {
    "question": "What features does MLflow on Databricks provide?",
    "ground_truth": "MLflow on Databricks provides managed MLflow tracking, model registry, and model serving. Databricks automatically logs experiments to a workspace MLflow server. Unity Catalog integration enables fine-grained access control, and Databricks Model Serving offers serverless endpoints with automatic scaling.",
    "contexts": [
      "MLflow on Databricks provides managed MLflow tracking, model registry, and model serving. Databricks automatically logs experiments to a workspace MLflow server. Unity Catalog integration enables fine-grained access control for models. Databricks Model Serving offers serverless endpoints with automatic scaling and A/B testing capabilities."
    ]
  },
  {
    "question": "How does MLflow integrate with LangChain?",
    "ground_truth": "MLflow integrates with LangChain through the mlflow.langchain module, supporting logging of chains, agents, and retrievers as MLflow models. Autologging captures chain structure, prompts, and execution traces. Models can be loaded and served using standard MLflow APIs.",
    "contexts": [
      "MLflow integrates with LangChain through mlflow.langchain module. The integration supports logging LangChain chains, agents, and retrievers as MLflow models. Autologging captures chain structure, prompts, and execution traces. Models can be loaded and served using standard MLflow APIs, enabling deployment of complex LangChain applications."
    ]
  }
]
