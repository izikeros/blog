{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation with RAGAS and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [What You'll Learn](#toc1_)    \n",
    "- [Prerequisites](#toc2_)    \n",
    "- [Setup and Configuration](#toc3_)    \n",
    "  - [LLM Provider Configuration](#toc3_1_)    \n",
    "- [Sample Knowledge Base](#toc4_)    \n",
    "- [Minimal RAG Pipeline](#toc5_)    \n",
    "  - [Enable MLflow Tracing](#toc5_1_)    \n",
    "- [Load Golden Dataset](#toc6_)    \n",
    "  - [Generate RAG Responses for Evaluation](#toc6_1_)    \n",
    "- [RAGAS Evaluation with MLflow](#toc7_)    \n",
    "  - [Prepare Evaluation Data](#toc7_1_)    \n",
    "- [MLflow Results Analysis](#toc8_)    \n",
    "- [Extras: Comparing RAG Variants with MLflow](#toc9_)    \n",
    "  - [Example: Comparing Chunk Sizes](#toc9_1_)    \n",
    "  - [Comparing Results in MLflow UI](#toc9_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to evaluate Retrieval-Augmented Generation (RAG) systems using **RAGAS** (Retrieval Augmented Generation Assessment) metrics through **MLflow** integration.\n",
    "\n",
    "## <a id='toc1_'></a>[What You'll Learn](#toc0_)\n",
    "\n",
    "1. **Build a minimal RAG pipeline** using LangChain and FAISS\n",
    "2. **Create a golden evaluation dataset** with expected answers\n",
    "3. **Evaluate RAG quality** using RAGAS metrics (Faithfulness, Context Precision, Context Recall, Factual Correctness)\n",
    "4. **Track results in MLflow** for systematic comparison\n",
    "5. **Support multiple LLM providers**: OpenAI, Azure OpenAI, and Ollama\n",
    "\n",
    "## <a id='toc2_'></a>[Prerequisites](#toc0_)\n",
    "\n",
    "- Python 3.10+\n",
    "- API key for your chosen LLM provider\n",
    "- Basic understanding of RAG concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Setup and Configuration](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[LLM Provider Configuration](#toc0_)\n",
    "\n",
    "This tutorial supports three LLM providers. Choose your provider and configure the appropriate environment variables:\n",
    "\n",
    "| Provider | Required Environment Variables |\n",
    "|----------|-------------------------------|\n",
    "| OpenAI | `OPENAI_API_KEY` |\n",
    "| Azure OpenAI | `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_DEPLOYMENT_NAME` |\n",
    "| Ollama | None (runs locally on `http://localhost:11434`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: azure_openai\n",
      "Chat model: gpt-4o-mini\n",
      "Embedding model: text-embedding-ada-002-v2\n"
     ]
    }
   ],
   "source": [
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    AZURE_OPENAI = \"azure_openai\"\n",
    "    OLLAMA = \"ollama\"\n",
    "\n",
    "\n",
    "# === CONFIGURE YOUR PROVIDER HERE ===\n",
    "PROVIDER = LLMProvider.AZURE_OPENAI\n",
    "\n",
    "# Model names per provider\n",
    "MODEL_CONFIG = {\n",
    "    LLMProvider.OPENAI: {\n",
    "        \"chat_model\": \"gpt-4o-mini\",\n",
    "        \"embedding_model\": \"text-embedding-3-small\",\n",
    "    },\n",
    "    LLMProvider.AZURE_OPENAI: {\n",
    "        \"chat_model\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4o-mini\"),\n",
    "        \"embedding_model\": os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-small\"),\n",
    "    },\n",
    "    LLMProvider.OLLAMA: {\n",
    "        \"chat_model\": \"llama3.2:3b\",\n",
    "        \"embedding_model\": \"nomic-embed-text\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Using provider: {PROVIDER.value}\")\n",
    "print(f\"Chat model: {MODEL_CONFIG[PROVIDER]['chat_model']}\")\n",
    "print(f\"Embedding model: {MODEL_CONFIG[PROVIDER]['embedding_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure litellm env vars configured\n",
      "Environment validated for azure_openai\n"
     ]
    }
   ],
   "source": [
    "def validate_environment(provider: LLMProvider) -> None:\n",
    "    \"\"\"Validate required environment variables for the selected provider.\"\"\"\n",
    "    required_vars = {\n",
    "        LLMProvider.OPENAI: [\"OPENAI_API_KEY\"],\n",
    "        LLMProvider.AZURE_OPENAI: [\n",
    "            \"AZURE_OPENAI_ENDPOINT\",\n",
    "            \"AZURE_OPENAI_API_KEY\",\n",
    "        ],\n",
    "        LLMProvider.OLLAMA: [],\n",
    "    }\n",
    "\n",
    "    missing = [var for var in required_vars[provider] if not os.getenv(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(\n",
    "            f\"Missing environment variables for {provider.value}: {missing}\\n\"\n",
    "            f\"Please set them before continuing.\"\n",
    "        )\n",
    "    \n",
    "    # Set provider-specific env vars for litellm (used by RAGAS scorers)\n",
    "    if provider == LLMProvider.OLLAMA:\n",
    "        os.environ.setdefault(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
    "        print(f\"OLLAMA_API_BASE set to: {os.environ['OLLAMA_API_BASE']}\")\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        # Set litellm Azure env vars from Azure OpenAI vars\n",
    "        if os.getenv(\"AZURE_OPENAI_API_KEY\") and not os.getenv(\"AZURE_API_KEY\"):\n",
    "            os.environ[\"AZURE_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "        if os.getenv(\"AZURE_OPENAI_ENDPOINT\") and not os.getenv(\"AZURE_API_BASE\"):\n",
    "            os.environ[\"AZURE_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "        os.environ.setdefault(\"AZURE_API_VERSION\", os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"))\n",
    "        print(f\"Azure litellm env vars configured\")\n",
    "    \n",
    "    print(f\"Environment validated for {provider.value}\")\n",
    "\n",
    "\n",
    "validate_environment(PROVIDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: AzureChatOpenAI\n",
      "Embeddings initialized: AzureOpenAIEmbeddings\n",
      "MLflow model URI: azure:/gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings, AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "def get_llm(provider: LLMProvider):\n",
    "    \"\"\"Factory function to create LLM instance based on provider.\"\"\"\n",
    "    config = MODEL_CONFIG[provider]\n",
    "\n",
    "    if provider == LLMProvider.OPENAI:\n",
    "        return ChatOpenAI(\n",
    "            model=config[\"chat_model\"],\n",
    "            temperature=0,\n",
    "        )\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        return AzureChatOpenAI(\n",
    "            azure_deployment=config[\"chat_model\"],\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "            temperature=0,\n",
    "        )\n",
    "    elif provider == LLMProvider.OLLAMA:\n",
    "        return ChatOllama(\n",
    "            model=config[\"chat_model\"],\n",
    "            temperature=0,\n",
    "            base_url=\"http://localhost:11434\",\n",
    "        )\n",
    "\n",
    "\n",
    "def get_embeddings(provider: LLMProvider):\n",
    "    \"\"\"Factory function to create embeddings instance based on provider.\"\"\"\n",
    "    config = MODEL_CONFIG[provider]\n",
    "\n",
    "    if provider == LLMProvider.OPENAI:\n",
    "        return OpenAIEmbeddings(model=config[\"embedding_model\"])\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        return AzureOpenAIEmbeddings(\n",
    "            azure_deployment=config[\"embedding_model\"],\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "        )\n",
    "    elif provider == LLMProvider.OLLAMA:\n",
    "        return OllamaEmbeddings(\n",
    "            model=config[\"embedding_model\"],\n",
    "            base_url=\"http://localhost:11434\",\n",
    "        )\n",
    "\n",
    "\n",
    "def get_mlflow_model_uri(provider: LLMProvider) -> str:\n",
    "    \"\"\"Get MLflow model URI for RAGAS scorers (uses litellm format).\"\"\"\n",
    "    config = MODEL_CONFIG[provider]\n",
    "\n",
    "    if provider == LLMProvider.OPENAI:\n",
    "        return f\"openai:/{config['chat_model']}\"\n",
    "    elif provider == LLMProvider.AZURE_OPENAI:\n",
    "        # Azure format: azure/<deployment_name>\n",
    "        return f\"azure:/{config['chat_model']}\"\n",
    "    elif provider == LLMProvider.OLLAMA:\n",
    "        # Ollama format for litellm: ollama/<model_name>\n",
    "        # Note: ollama_chat format has issues with litellm, use ollama/ prefix\n",
    "        return f\"ollama:/{config['chat_model']}\"\n",
    "\n",
    "\n",
    "llm = get_llm(PROVIDER)\n",
    "embeddings = get_embeddings(PROVIDER)\n",
    "mlflow_model_uri = get_mlflow_model_uri(PROVIDER)\n",
    "\n",
    "print(f\"LLM initialized: {type(llm).__name__}\")\n",
    "print(f\"Embeddings initialized: {type(embeddings).__name__}\")\n",
    "print(f\"MLflow model URI: {mlflow_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Sample Knowledge Base](#toc0_)\n",
    "\n",
    "We'll create a small knowledge base about **MLflow** - fitting for a tutorial that uses MLflow for evaluation! This dataset contains key concepts that our RAG system will retrieve from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base contains 20 documents\n",
      "  1. MLflow Tracking is an API and UI for logging parameters, code versions, metrics,...\n",
      "  2. The MLflow Model Registry is a centralized model store that provides model linea...\n",
      "  3. MLflow GenAI provides specialized tools for developing and evaluating generative...\n",
      "  4. RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework int...\n",
      "  5. MLflow Projects package code in a reusable, reproducible form. A project is simp...\n",
      "  6. MLflow's autolog feature automatically logs metrics, parameters, and models duri...\n",
      "  7. The MLflow Model format is a standard format for packaging machine learning mode...\n",
      "  8. Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML...\n",
      "  9. MLflow Model Serving enables deploying models as REST API endpoints. You can ser...\n",
      "  10. MLflow Recipes (formerly MLflow Pipelines) provide predefined templates for comm...\n",
      "  11. The MLflow CLI provides commands for running projects, serving models, and manag...\n",
      "  12. MLflow's REST API allows programmatic access to the tracking server. Endpoints i...\n",
      "  13. MLflow experiments organize runs into logical groups. Each experiment has a uniq...\n",
      "  14. MLflow provides run comparison capabilities through the UI and API. The Compare ...\n",
      "  15. MLflow artifacts are files associated with runs, such as models, data files, and...\n",
      "  16. Model signatures in MLflow define the expected input and output schema for model...\n",
      "  17. MLflow on Databricks provides managed MLflow tracking, model registry, and model...\n",
      "  18. MLflow supports multiple environment managers for reproducibility. Projects can ...\n",
      "  19. MLflow Prompt Engineering tools help develop and version prompts for LLM applica...\n",
      "  20. MLflow integrates with LangChain through mlflow.langchain module. The integratio...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load knowledge base from external file\n",
    "with open(\"data/knowledge_base.json\") as f:\n",
    "    KNOWLEDGE_BASE = json.load(f)\n",
    "\n",
    "print(f\"Knowledge base contains {len(KNOWLEDGE_BASE)} documents\")\n",
    "for i, doc in enumerate(KNOWLEDGE_BASE, 1):\n",
    "    preview = doc[:80].replace('\\n', ' ')\n",
    "    print(f\"  {i}. {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 20 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=doc.strip()) for doc in KNOWLEDGE_BASE]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 20 vectors\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test query: 'What metrics does RAGAS provide?'\n",
      "Retrieved 3 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework integrated with MLflow for assessing RAG pipelines. Key metrics include: Faithfulness (measures if the answer is grounded i...\n",
      "\n",
      "--- Document 2 ---\n",
      "MLflow GenAI provides specialized tools for developing and evaluating generative AI applications. It includes mlflow.genai.evaluate() for systematic assessment of LLM outputs using configurable scorer...\n",
      "\n",
      "--- Document 3 ---\n",
      "Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML models or mlflow.genai.evaluate() for generative AI applications. For GenAI, evaluation uses Scorer objects that can be...\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What metrics does RAGAS provide?\"\n",
    "test_results = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Test query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(test_results)} documents:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Minimal RAG Pipeline](#toc0_)\n",
    "\n",
    "We'll build a simple RAG chain using LangChain's LCEL (LangChain Expression Language) that:\n",
    "1. Retrieves relevant context from our FAISS vector store\n",
    "2. Formats a prompt with the context and question\n",
    "3. Generates an answer using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant answering questions about MLflow.\n",
    "Use ONLY the following context to answer the question.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is MLflow Tracking?\n",
      "\n",
      "Answer: MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code. It allows you to log and query experiments using Python, REST, R API, and Java API. The MLflow Tracking component lets you log source code, models, and visualizations. Each run records: code version, start and end time, source, parameters, metrics, and artifacts.\n"
     ]
    }
   ],
   "source": [
    "test_answer = rag_chain.invoke(\"What is MLflow Tracking?\")\n",
    "print(\"Test Question: What is MLflow Tracking?\")\n",
    "print(f\"\\nAnswer: {test_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Enable MLflow Tracing](#toc0_)\n",
    "\n",
    "MLflow's LangChain integration can automatically capture traces of our RAG pipeline invocations. This is essential for evaluation - RAGAS scorers analyze these traces to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 16:18:16 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/08 16:18:16 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/08 16:18:16 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/08 16:18:16 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/08 16:18:16 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/08 16:18:16 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment: RAG-Evaluation-Tutorial\n",
      "LangChain autologging enabled with tracing\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"RAG-Evaluation-Tutorial\")\n",
    "\n",
    "mlflow.langchain.autolog(log_traces=True)\n",
    "\n",
    "print(f\"MLflow experiment: {mlflow.get_experiment_by_name('RAG-Evaluation-Tutorial').name}\")\n",
    "print(\"LangChain autologging enabled with tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Load Golden Dataset](#toc0_)\n",
    "\n",
    "A **golden dataset** (also called ground truth or evaluation dataset) contains:\n",
    "- **Questions**: User queries we want to evaluate\n",
    "- **Expected Answers**: The correct/ideal responses\n",
    "- **Expected Contexts** (optional): Which documents should be retrieved\n",
    "\n",
    "This dataset allows us to systematically measure our RAG system's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden dataset contains 20 evaluation samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow Tracking used for?</td>\n",
       "      <td>MLflow Tracking is used for logging parameters...</td>\n",
       "      <td>[MLflow Tracking is an API and UI for logging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What features does the MLflow Model Registry p...</td>\n",
       "      <td>The MLflow Model Registry provides model linea...</td>\n",
       "      <td>[The MLflow Model Registry is a centralized mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                  What is MLflow Tracking used for?   \n",
       "1  What features does the MLflow Model Registry p...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  MLflow Tracking is used for logging parameters...   \n",
       "1  The MLflow Model Registry provides model linea...   \n",
       "\n",
       "                                            contexts  \n",
       "0  [MLflow Tracking is an API and UI for logging ...  \n",
       "1  [The MLflow Model Registry is a centralized mo...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load golden dataset from external file\n",
    "with open(\"data/golden_dataset.json\") as f:\n",
    "    GOLDEN_DATASET = json.load(f)\n",
    "\n",
    "eval_df = pd.DataFrame(GOLDEN_DATASET)\n",
    "print(f\"Golden dataset contains {len(GOLDEN_DATASET)} evaluation samples\")\n",
    "eval_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Generate RAG Responses for Evaluation](#toc0_)\n",
    "\n",
    "We'll run our RAG pipeline on each question and collect the responses along with the retrieved contexts. This data will be used by RAGAS scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define traced RAG function for evaluation\n",
    "# IMPORTANT: Function parameter names must match keys in data['inputs']\n",
    "# Since inputs={'question': ...}, the function must accept 'question' parameter\n",
    "\n",
    "@mlflow.trace(span_type=\"CHAIN\")\n",
    "def traced_rag_predict(question: str) -> dict:\n",
    "    \"\"\"Traced RAG prediction function for mlflow.genai.evaluate().\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer (matches inputs['question'] key)\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'response' and 'retrieved_contexts' for RAGAS scorers\n",
    "    \"\"\"\n",
    "    # Retrieval step - creates RETRIEVER span\n",
    "    with mlflow.start_span(name=\"retriever\", span_type=\"RETRIEVER\") as span:\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "        contexts = [doc.page_content for doc in retrieved_docs]\n",
    "        span.set_inputs({\"question\": question})\n",
    "        span.set_outputs({\"retrieved_contexts\": contexts})\n",
    "    \n",
    "    # Generation step - creates LLM span\n",
    "    with mlflow.start_span(name=\"generator\", span_type=\"LLM\") as span:\n",
    "        answer = rag_chain.invoke(question)\n",
    "        span.set_inputs({\"question\": question, \"contexts\": contexts})\n",
    "        span.set_outputs({\"response\": answer})\n",
    "    \n",
    "    return {\n",
    "        \"response\": answer,\n",
    "        \"retrieved_contexts\": contexts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample evaluation record #3:\n",
      "\n",
      "Question: What metrics does RAGAS provide for RAG evaluation?\n",
      "\n",
      "Expected Answer: RAGAS provides four key metrics: Faithfulness (measures if the answer is grounded in context), Context Precision (evaluates if relevant documents are ranked higher), Context Recall (checks if context contains all needed information), and Factual Correctness (compares output against expected answers).\n",
      "\n",
      "--- Testing RAG response for this question ---\n",
      "\n",
      "RAG Answer: RAGAS provides the following key metrics for RAG evaluation: Faithfulness, Context Precision, Context Recall, and Factual Correctness.\n",
      "\n",
      "Retrieved Contexts (3):\n",
      "\n",
      "  Context 1: RAGAS (Retrieval Augmented Generation Assessment) is an evaluation framework integrated with MLflow ...\n",
      "\n",
      "  Context 2: Evaluation in MLflow can be performed using mlflow.evaluate() for traditional ML models or mlflow.ge...\n",
      "\n",
      "  Context 3: MLflow GenAI provides specialized tools for developing and evaluating generative AI applications. It...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview a sample from the golden dataset\n",
    "# Note: With predict_fn approach, answers are generated during evaluation\n",
    "sample_idx = 2\n",
    "print(f\"Sample evaluation record #{sample_idx + 1}:\")\n",
    "print(f\"\\nQuestion: {eval_df.iloc[sample_idx]['question']}\")\n",
    "print(f\"\\nExpected Answer: {eval_df.iloc[sample_idx]['ground_truth']}\")\n",
    "\n",
    "# Show what the traced function would produce for this question\n",
    "print(f\"\\n--- Testing RAG response for this question ---\")\n",
    "test_output = traced_rag_predict(question=eval_df.iloc[sample_idx]['question'])\n",
    "print(f\"\\nRAG Answer: {test_output['response']}\")\n",
    "print(f\"\\nRetrieved Contexts ({len(test_output['retrieved_contexts'])}):\\n\")\n",
    "for j, ctx in enumerate(test_output['retrieved_contexts'], 1):\n",
    "    print(f\"  Context {j}: {ctx[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>[RAGAS Evaluation with MLflow](#toc0_)\n",
    "\n",
    "Now we'll use MLflow's RAGAS integration to evaluate our RAG pipeline. The key metrics we'll compute:\n",
    "\n",
    "| Metric | What it measures | Requires |\n",
    "|--------|-----------------|----------|\n",
    "| **Faithfulness** | Is the answer grounded in the retrieved context? | answer, contexts |\n",
    "| **Context Precision** | Are relevant docs ranked higher than irrelevant ones? | question, contexts, expected_answer |\n",
    "| **Context Recall** | Does the context contain all info needed to answer? | contexts, expected_answer |\n",
    "| **Factual Correctness** | Does the answer match the expected ground truth? | answer, expected_answer |\n",
    "\n",
    "\n",
    "\n",
    "> **Note on LLM Judge**: RAGAS metrics use an LLM as a judge. For best results, use **OpenAI** (gpt-4o-mini) as the judge model even if you're using Ollama for RAG generation. Ollama/local models may have issues with litellm's structured output parsing. Set `JUDGE_PROVIDER = LLMProvider.OPENAI` below if you encounter scoring errors with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing judge model: azure:/gpt-4o-mini\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úó litellm connection failed: azure:/gpt-4o-mini\n",
      "  Error: BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call.\n",
      "\n",
      "‚ö†Ô∏è  Consider using OpenAI as judge model for reliable scoring.\n",
      "   Set JUDGE_PROVIDER = LLMProvider.OPENAI in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Test litellm connectivity (optional - helps debug scoring issues)\n",
    "import litellm\n",
    "\n",
    "def test_litellm_connection(model_uri: str) -> bool:\n",
    "    \"\"\"Test if litellm can connect to the model.\"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=model_uri,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'test' and nothing else.\"}],\n",
    "            max_tokens=10,\n",
    "        )\n",
    "        print(f\"‚úì litellm connection successful: {model_uri}\")\n",
    "        print(f\"  Response: {response.choices[0].message.content[:50]}...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó litellm connection failed: {model_uri}\")\n",
    "        print(f\"  Error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "# Test the judge model connection\n",
    "judge_model_uri = get_mlflow_model_uri(PROVIDER)\n",
    "print(f\"Testing judge model: {judge_model_uri}\\n\")\n",
    "litellm_ok = test_litellm_connection(judge_model_uri)\n",
    "\n",
    "if not litellm_ok:\n",
    "    print(\"\\n‚ö†Ô∏è  Consider using OpenAI as judge model for reliable scoring.\")\n",
    "    print(\"   Set JUDGE_PROVIDER = LLMProvider.OPENAI in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge model: azure:/gpt-4o-mini\n",
      "(Change JUDGE_PROVIDER to LLMProvider.OPENAI if scoring fails with Ollama)\n",
      "\n",
      "Configured 4 RAGAS scorers:\n",
      "  - Faithfulness\n",
      "  - FactualCorrectness\n",
      "  - ContextPrecision\n",
      "  - ContextRecall\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai.scorers.ragas import (\n",
    "    Faithfulness,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    FactualCorrectness,\n",
    ")\n",
    "\n",
    "# Configure the judge model for RAGAS evaluation\n",
    "# For reliable scoring, use OpenAI even when using Ollama for RAG generation\n",
    "JUDGE_PROVIDER = PROVIDER  # Change to LLMProvider.OPENAI for better results\n",
    "judge_model_uri = get_mlflow_model_uri(JUDGE_PROVIDER)\n",
    "\n",
    "print(f\"Judge model: {judge_model_uri}\")\n",
    "print(f\"(Change JUDGE_PROVIDER to LLMProvider.OPENAI if scoring fails with Ollama)\\n\")\n",
    "\n",
    "# Note: ContextPrecision and ContextRecall require traces with RETRIEVER spans\n",
    "# For evaluation without traces, use Faithfulness and FactualCorrectness\n",
    "scorers = [\n",
    "    Faithfulness(model=judge_model_uri),\n",
    "    FactualCorrectness(model=judge_model_uri),\n",
    "    # These require traces with retriever spans - may show errors without proper tracing:\n",
    "    ContextPrecision(model=judge_model_uri),\n",
    "    ContextRecall(model=judge_model_uri),\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(scorers)} RAGAS scorers:\")\n",
    "for scorer in scorers:\n",
    "    print(f\"  - {type(scorer).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_1_'></a>[Prepare Evaluation Data](#toc0_)\n",
    "\n",
    "MLflow's `genai.evaluate()` expects data in a specific format. We need to map our data to the expected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 20 samples for evaluation\n",
      "\n",
      "Sample format:\n",
      "  inputs: ['question']\n",
      "  expectations: ['ground_truth', 'contexts']\n",
      "  ground_truth contexts: 205 items\n",
      "\n",
      "Note: outputs will be generated by traced_rag_predict() during evaluation\n",
      "      ground_truth enables ContextPrecision and ContextRecall metrics\n"
     ]
    }
   ],
   "source": [
    "# Prepare evaluation data for predict_fn approach\n",
    "# With predict_fn, we pass inputs and expectations - outputs come from the traced function\n",
    "eval_data = []\n",
    "for _, row in eval_df.iterrows():\n",
    "    eval_data.append({\n",
    "        \"inputs\": {\"question\": row[\"question\"]},\n",
    "        \"expectations\": {\n",
    "            \"ground_truth\": row[\"ground_truth\"],\n",
    "            \"contexts\": row.get(\"contexts\", []),  # For ContextRecall\n",
    "        },\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(eval_data)} samples for evaluation\")\n",
    "print(f\"\\nSample format:\")\n",
    "print(f\"  inputs: {list(eval_data[0]['inputs'].keys())}\")\n",
    "print(f\"  expectations: {list(eval_data[0]['expectations'].keys())}\")\n",
    "if eval_data[0]['expectations'].get('ground_truth'):\n",
    "    print(f\"  ground_truth contexts: {len(eval_data[0]['expectations']['ground_truth'])} items\")\n",
    "print(f\"\\nNote: outputs will be generated by traced_rag_predict() during evaluation\")\n",
    "print(f\"      ground_truth enables ContextPrecision and ContextRecall metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 16:18:22 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2026/01/08 16:18:22 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2026/01/08 16:18:22 WARNING mlflow.tracing.fluent: Failed to start span VectorStoreRetriever: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAGAS evaluation with traced predict_fn...\n",
      "This generates traces with RETRIEVER spans for Faithfulness metric.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 16:18:23 WARNING mlflow.tracing.fluent: Failed to start span RunnableSequence: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0351735af7c4a979ac4ce83bc4a33d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mragas-evaluation-traced\u001b[0m\n",
      "  Run ID: \u001b[94mc18463db51174ea8ba36935fe2adcfe5\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "\n",
      "Evaluation complete! Run ID: c18463db51174ea8ba36935fe2adcfe5\n"
     ]
    }
   ],
   "source": [
    "print(\"Running RAGAS evaluation with traced predict_fn...\")\n",
    "print(\"This generates traces with RETRIEVER spans for Faithfulness metric.\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"ragas-evaluation-traced\") as run:\n",
    "    mlflow.log_param(\"provider\", PROVIDER.value)\n",
    "    mlflow.log_param(\"model\", MODEL_CONFIG[PROVIDER][\"chat_model\"])\n",
    "    mlflow.log_param(\"num_samples\", len(eval_data))\n",
    "    mlflow.log_param(\"retriever_k\", 3)\n",
    "    mlflow.log_param(\"evaluation_mode\", \"predict_fn\")\n",
    "\n",
    "    # Use predict_fn to generate traces with RETRIEVER spans\n",
    "    # This allows Faithfulness scorer to access retrieved_contexts\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        predict_fn=traced_rag_predict,\n",
    "        data=eval_data,\n",
    "        scorers=scorers,\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "print(f\"\\nEvaluation complete! Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_'></a>[MLflow Results Analysis](#toc0_)\n",
    "\n",
    "Let's examine the evaluation results both programmatically and understand how to view them in the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "RAGAS Metrics:\n",
      "----------------------------------------\n",
      "  ‚úì ContextRecall/value: 0.830 (¬±0.201) [20/20 samples]\n",
      "  ‚úì ContextPrecision/value: 0.992 (¬±0.037) [20/20 samples]\n",
      "  ‚úì Faithfulness/value: 0.994 (¬±0.020) [20/20 samples]\n",
      "  ‚úì FactualCorrectness/value: 0.550 (¬±0.218) [20/20 samples]\n",
      "\n",
      "Summary: 4 metrics succeeded, 0 metrics failed\n",
      "   Total samples: 20\n",
      "\n",
      "‚úÖ All metrics computed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = eval_results.tables[\"eval_results\"]\n",
    "\n",
    "# Find RAGAS scorer columns (Faithfulness, FactualCorrectness, Context*)\n",
    "import pandas as pd\n",
    "ragas_metrics = ['Faithfulness', 'FactualCorrectness', 'ContextPrecision', 'ContextRecall']\n",
    "value_columns = [col for col in results_df.columns \n",
    "                 if col.endswith('/value') and any(m in col for m in ragas_metrics)]\n",
    "error_columns = [col for col in results_df.columns \n",
    "                 if col.endswith('/error') and any(m in col for m in ragas_metrics)]\n",
    "\n",
    "print(\"\\nRAGAS Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "successful_metrics = 0\n",
    "failed_metrics = 0\n",
    "\n",
    "for col in value_columns:\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    numeric_col = pd.to_numeric(results_df[col], errors='coerce')\n",
    "    non_null = numeric_col.dropna()\n",
    "    total = len(results_df)\n",
    "    success_count = len(non_null)\n",
    "\n",
    "    if success_count > 0:\n",
    "        mean_val = non_null.mean()\n",
    "        std_val = non_null.std() if len(non_null) > 1 else 0\n",
    "        print(f\"  ‚úì {col}: {mean_val:.3f} (¬±{std_val:.3f}) [{success_count}/{total} samples]\")\n",
    "        successful_metrics += 1\n",
    "    else:\n",
    "        print(f\"  ‚úó {col}: NO SCORES (0/{total} samples succeeded)\")\n",
    "        failed_metrics += 1\n",
    "\n",
    "print(f\"\\nSummary: {successful_metrics} metrics succeeded, {failed_metrics} metrics failed\")\n",
    "print(f\"   Total samples: {len(results_df)}\")\n",
    "\n",
    "# Error diagnostics (if any metrics failed)\n",
    "if failed_metrics > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç DIAGNOSTIC: Error Details for Failed Metrics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col in error_columns:\n",
    "        metric_name = col.replace('/error', '')\n",
    "        errors = results_df[col].dropna()\n",
    "        \n",
    "        if len(errors) > 0:\n",
    "            print(f\"\\n‚ùå {metric_name}:\")\n",
    "            # Get first unique error message\n",
    "            unique_errors = errors.unique()\n",
    "            for err in unique_errors[:2]:  # Show max 2 unique errors\n",
    "                # Truncate long error messages\n",
    "                err_str = str(err)[:300]\n",
    "                if len(str(err)) > 300:\n",
    "                    err_str += \"...\"\n",
    "                print(f\"   {err_str}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"üí° Common fixes:\")\n",
    "    print(\"   1. Use OpenAI as judge: JUDGE_PROVIDER = LLMProvider.OPENAI\")\n",
    "    print(\"   2. For Ollama: ensure model is running and OLLAMA_API_BASE is set\")\n",
    "    print(\"   3. ContextPrecision/ContextRecall require traces with RETRIEVER spans\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All metrics computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying Low-Scoring Samples:\n",
      "----------------------------------------\n",
      "\n",
      "‚ö†Ô∏è  ContextRecall/value < 0.5: 1 samples\n",
      "    - [0.33] What is Faithfulness in RAGAS?...\n",
      "\n",
      "‚ö†Ô∏è  FactualCorrectness/value < 0.5: 7 samples\n",
      "    - [0.43] What is MLflow GenAI used for?...\n",
      "    - [0.40] How can you run MLflow Projects?...\n",
      "    - [0.12] What is Faithfulness in RAGAS?...\n",
      "    - [0.40] What frameworks support MLflow autolog?...\n",
      "    - [0.35] What are MLflow Recipes and what steps do they include?...\n",
      "    - [0.24] How can you access MLflow programmatically via REST API?...\n",
      "    - [0.40] What are model signatures and input examples in MLflow?...\n"
     ]
    }
   ],
   "source": [
    "# Helper function to extract question from request column\n",
    "def extract_question(request_data):\n",
    "    \"\"\"Extract question from MLflow request column.\"\"\"\n",
    "    if isinstance(request_data, dict):\n",
    "        return str(request_data.get(\"question\", \"N/A\"))[:60]\n",
    "    elif isinstance(request_data, str):\n",
    "        return request_data[:60]\n",
    "    return \"N/A\"\n",
    "\n",
    "# Display results summary with metric columns\n",
    "available_cols = [col for col in value_columns if col in results_df.columns]\n",
    "results_summary = results_df[available_cols].copy()\n",
    "\n",
    "# Add question column from request data\n",
    "if \"request\" in results_df.columns:\n",
    "    results_summary.insert(0, \"question\", results_df[\"request\"].apply(extract_question))\n",
    "\n",
    "results_summary\n",
    "\n",
    "\n",
    "print(\"\\nIdentifying Low-Scoring Samples:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for col in value_columns:\n",
    "    if col in results_df.columns:\n",
    "        numeric_col = pd.to_numeric(results_df[col], errors='coerce')\n",
    "        low_mask = numeric_col < 0.5\n",
    "        low_scores = results_df[low_mask]\n",
    "        if len(low_scores) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  {col} < 0.5: {len(low_scores)} samples\")\n",
    "            for idx, row in low_scores.iterrows():\n",
    "                question = extract_question(row.get(\"request\", {}))\n",
    "                score = numeric_col.loc[idx]\n",
    "                if pd.notna(score):\n",
    "                    print(f\"    - [{score:.2f}] {question}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view detailed results in the MLflow UI:\n",
    "\n",
    "1. Start MLflow UI (if not running):\n",
    "   `$ mlflow ui --port 5000`\n",
    "\n",
    "2. Open http://localhost:5000 in your browser\n",
    "\n",
    "3. Navigate to:\n",
    "   - Experiment: 'RAG-Evaluation-Tutorial'\n",
    "   - Run: 'ragas-evaluation'\n",
    "\n",
    "4. In the run details, you'll find:\n",
    "   - Parameters: model configuration\n",
    "   - Metrics: aggregate RAGAS scores\n",
    "   - Artifacts: detailed evaluation tables\n",
    "   - Traces: individual RAG invocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: c18463db51174ea8ba36935fe2adcfe5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ Tutorial Complete!\n",
      "============================================================\n",
      "\n",
      "Summary:\n",
      "  - Provider: azure_openai\n",
      "  - Model: gpt-4o-mini\n",
      "  - Samples evaluated: 20\n",
      "  - MLflow Run ID: c18463db51174ea8ba36935fe2adcfe5\n",
      "\n",
      "View results: mlflow ui --port 5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Tutorial Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Summary:\n",
    "  - Provider: {PROVIDER.value}\n",
    "  - Model: {MODEL_CONFIG[PROVIDER]['chat_model']}\n",
    "  - Samples evaluated: {len(eval_data)}\n",
    "  - MLflow Run ID: {run_id}\n",
    "\n",
    "View results: mlflow ui --port 5000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id='toc9_'></a>[Extras: Comparing RAG Variants with MLflow](#toc0_)\n",
    "\n",
    "One of MLflow's key strengths is enabling systematic A/B comparisons between different RAG configurations. Here's how to structure experiments comparing variants like chunk sizes, models, or retrieval strategies.\n",
    "\n",
    "### <a id='toc9_1_'></a>[Example: Comparing Chunk Sizes](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running chunk size comparison experiments...\n",
      "============================================================\n",
      "\n",
      "Testing chunk_size=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 16:23:07 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2026/01/08 16:23:07 WARNING mlflow.tracing.fluent: Failed to start span VectorStoreRetriever: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2026/01/08 16:23:08 WARNING mlflow.tracing.fluent: Failed to start span RunnableSequence: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 175 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9b523e04f34d3aaeaadd7b0c896e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mchunk-size-50\u001b[0m\n",
      "  Run ID: \u001b[94m5843dd1f28b94c3a9bc0aace400ab8dd\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "   ‚úì Run ID: 5843dd1f28b94c3a9bc0aace400ab8dd\n",
      "\n",
      "Testing chunk_size=150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 16:23:40 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2026/01/08 16:23:40 WARNING mlflow.tracing.fluent: Failed to start span VectorStoreRetriever: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2026/01/08 16:23:41 WARNING mlflow.tracing.fluent: Failed to start span RunnableSequence: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 62 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364e2da242954337b5efa15aa0749a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mchunk-size-150\u001b[0m\n",
      "  Run ID: \u001b[94m07e5bad6efbc4c79b017e6b8828ba7c5\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "   ‚úì Run ID: 07e5bad6efbc4c79b017e6b8828ba7c5\n",
      "\n",
      "============================================================\n",
      "Completed 2 experiments. Run IDs saved for comparison.\n"
     ]
    }
   ],
   "source": [
    "# Comparing RAG Variants: Different Chunk Sizes\n",
    "# This demonstrates how to evaluate the same RAG pipeline with different configurations\n",
    "\n",
    "print(\"Running chunk size comparison experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "CHUNK_SIZES = [50, 150]\n",
    "experiment_run_ids = []\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    print(f\"\\nTesting chunk_size={chunk_size}\")\n",
    "    \n",
    "    # Rebuild the vector store with new chunk size\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size // 10\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Update the global vectorstore and retriever used by traced_rag_predict\n",
    "    global vectorstore, retriever\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    print(f\"   Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    with mlflow.start_run(run_name=f\"chunk-size-{chunk_size}\") as run:\n",
    "        mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "        mlflow.log_param(\"chunk_overlap\", chunk_size // 10)\n",
    "        mlflow.log_param(\"num_chunks\", len(chunks))\n",
    "        \n",
    "        eval_results = mlflow.genai.evaluate(\n",
    "            predict_fn=traced_rag_predict,\n",
    "            data=eval_data,\n",
    "            scorers=scorers,\n",
    "        )\n",
    "        experiment_run_ids.append(run.info.run_id)\n",
    "        print(f\"   ‚úì Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Completed {len(CHUNK_SIZES)} experiments. Run IDs saved for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_2_'></a>[Comparing Results in MLflow UI](#toc0_)\n",
    "\n",
    "After running multiple variants:\n",
    "\n",
    "1. Open MLflow UI: `mlflow ui --port 5000`\n",
    "2. Navigate to your experiment\n",
    "3. Select runs to compare using checkboxes\n",
    "4. Click **Compare** to see side-by-side metrics\n",
    "5. Use **Chart** view to visualize metric differences\n",
    "\n",
    "You can also compare programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available metric columns (4 total):\n",
      "  - metrics.FactualCorrectness/mean\n",
      "  - metrics.Faithfulness/mean\n",
      "  - metrics.ContextRecall/mean\n",
      "  - metrics.ContextPrecision/mean\n",
      "\n",
      "Chunk Size Comparison Results\n",
      "================================================================================\n",
      "      Run Name Chunk Size Num Chunks Faithfulness FactualCorrectness ContextPrecision ContextRecall\n",
      "chunk-size-150        150         62        0.915              0.781            0.983         0.834\n",
      " chunk-size-50         50        175        0.944              0.758            0.983         0.844\n",
      "\n",
      "‚ú® Best configuration: chunk-size-150\n"
     ]
    }
   ],
   "source": [
    "# Compare Results Programmatically\n",
    "# Query MLflow for runs and display a formatted comparison table\n",
    "\n",
    "experiment_name = \"RAG-Evaluation-Tutorial\"\n",
    "\n",
    "# Get runs with chunk_size parameter (our comparison experiments)\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name],\n",
    "    filter_string=\"params.chunk_size != ''\",\n",
    "    order_by=[\"params.chunk_size ASC\"]\n",
    ")\n",
    "\n",
    "if len(runs_df) == 0:\n",
    "    print(\"No chunk size comparison runs found. Run the comparison cell above first.\")\n",
    "else:\n",
    "    # Debug: show available metric columns\n",
    "    metric_cols = [c for c in runs_df.columns if c.startswith(\"metrics.\")]\n",
    "    print(f\"Available metric columns ({len(metric_cols)} total):\")\n",
    "    for col in metric_cols[:8]:  # Show first 8\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Define metrics we want (will search for partial matches)\n",
    "    metric_names = [\"Faithfulness\", \"FactualCorrectness\", \"ContextPrecision\", \"ContextRecall\"]\n",
    "    \n",
    "    # Find actual column names (may have backticks or different format)\n",
    "    def find_metric_col(df, metric_name):\n",
    "        \"\"\"Find column containing metric_name in its name.\"\"\"\n",
    "        for col in df.columns:\n",
    "            if metric_name in col and \"mean\" in col:\n",
    "                return col\n",
    "        return None\n",
    "    \n",
    "    comparison_data = []\n",
    "    for _, run in runs_df.iterrows():\n",
    "        row = {\n",
    "            \"Run Name\": run.get(\"tags.mlflow.runName\", \"N/A\"),\n",
    "            \"Chunk Size\": run.get(\"params.chunk_size\", \"N/A\"),\n",
    "            \"Num Chunks\": run.get(\"params.num_chunks\", \"N/A\"),\n",
    "        }\n",
    "        for metric_name in metric_names:\n",
    "            col = find_metric_col(runs_df, metric_name)\n",
    "            if col:\n",
    "                value = run.get(col)\n",
    "                row[metric_name] = f\"{value:.3f}\" if pd.notna(value) else \"N/A\"\n",
    "            else:\n",
    "                row[metric_name] = \"N/A\"\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nChunk Size Comparison Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Find best configuration\n",
    "    if \"FactualCorrectness\" in comparison_df.columns:\n",
    "        best_idx = comparison_df[\"FactualCorrectness\"].apply(\n",
    "            lambda x: float(x) if x != \"N/A\" else 0\n",
    "        ).idxmax()\n",
    "        print(f\"\\n‚ú® Best configuration: {comparison_df.iloc[best_idx]['Run Name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
