---
Title: K-D Trees explained in simple terms
Slug: ktrees_in_simple_terms
Date: 2023-03-02
Modified: 2023-03-02
Start: 2023-03-02
Tags: k-d-tree, binary-search-tree, multi-dimensional-space, computer-graphics, image-processing, machine-learning, data-structure, search-algorithm, nearest-neighbor-search, range-search, ball-trees, cover-trees, curse-of-dimensionality, efficient-data-organization
Category: Machine Learning
Image: /images/head/abstract_1.jpg
Summary: Discover the power of k-d trees for multi-dimensional data organization and search. Learn how to efficiently find nearest neighbors and points within a given range.
Status: draft
prompt: Give me long blog-post text with explanation of k-d tree in simple terms.
---

## Introduction

K-d trees are a popular data structure used for efficiently storing and searching for data in multi-dimensional space. K-d trees are commonly used in computer graphics, image processing, and machine learning. In this blog post, we will discuss what k-d trees are, how they work, and how they can be used.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Tree_0001.svg/740px-Tree_0001.svg.png)

## K-D Tree
### What is a k-d tree?

A k-d tree is a binary search tree that is used to organize points in k-dimensional space. Each node in the tree represents a k-dimensional point, and each subtree represents a subset of the k-dimensional space. The k-d tree is constructed by recursively partitioning the space into smaller and smaller regions.

The name k-d tree comes from the fact that the tree is constructed by repeatedly splitting the space along one of the k dimensions. In other words, the first level of the tree will split the space into two regions along the first dimension, the second level will split each of these regions into two regions along the second dimension, and so on.

### How does a k-d tree work?

The construction of a k-d tree involves selecting a splitting dimension and a splitting value for each level of the tree. The splitting dimension is chosen by selecting the dimension that has the highest variance among the points in the current subtree. The splitting value is chosen by selecting the median value of the points in the splitting dimension.

Once the splitting dimension and value have been chosen, the space is partitioned into two regions based on whether the points lie on the "left" or "right" side of the splitting value. The points on the left side are inserted into the left subtree, and the points on the right side are inserted into the right subtree. This process is repeated recursively until each node in the tree represents a single point.

When searching for a point in a k-d tree, the tree is traversed recursively starting at the root node. At each level of the tree, the search algorithm determines which side of the splitting plane the query point lies on, and recursively searches the subtree on that side of the plane. If the query point lies exactly on the splitting plane, the algorithm will search both subtrees.

## Advantages of k-d trees

K-d trees have several advantages over other data structures for organizing points in multi-dimensional space. One of the main advantages is that k-d trees can be constructed efficiently, with a time complexity of O(n log n) in the average case. This makes k-d trees a good choice for applications that require fast construction of the data structure.

Another advantage of k-d trees is that they can be used to perform range searches efficiently. Range searches involve finding all points within a given distance of a query point. K-d trees can perform range searches in O(sqrt(n) + k) time, where k is the number of points found within the range.

K-d trees are also useful for nearest neighbor searches. Nearest neighbor searches involve finding the point in the tree that is closest to a given query point. K-d trees can perform nearest neighbor searches in O(log n) time on average.

### Disadvantages of k-d trees

While k-d trees have several advantages, they also have some disadvantages. One of the main disadvantages is that the performance of k-d trees can degrade rapidly as the dimensionality of the data increases. This is known as the "curse of dimensionality", and it can make k-d trees impractical for high-dimensional data.

Another disadvantage of k-d trees is that they can be sensitive to the order in which the points are inserted into the tree. If the points are inserted in a way that causes the tree to be unbalanced, the performance of the tree can suffer.

## Example
Let's demonstrate the k-d tree algorithm using a simple example. Consider the following set of 2-dimensional points:

```
[(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]
```
To construct a k-d tree for these points, we start by selecting a splitting dimension and splitting value for the root node. In this case, we will choose the x-axis as the splitting dimension and the median x-value (7) as the splitting value.

```
     (7,2)
```

Next, we split the points into two subsets based on whether they lie to the left or right of the splitting value:

```
     (7,2)
     /    \
(2,3)  (5,4)  (9,6)  (4,7)  (8,1)
```

We now have two subtrees, one containing the points to the left of the splitting value and one containing the points to the right. We can repeat the splitting process for each subtree recursively.

For the left subtree, we choose the y-axis as the splitting dimension and the median y-value (3.5) as the splitting value:

```
        (7,2)
        /    \
 (5,4)       (2,3)
           /      \
       (4,7)    (8,1)  (9,6)

```
For the right subtree, we choose the y-axis as the splitting dimension and the median y-value (5) as the splitting value:

```
        (7,2)
        /    \
 (5,4)        (9,6)
           /       \
      (8,1)     (2,3)   (4,7)

```

We have now constructed a k-d tree for the set of points. To search for a nearest neighbor of a query point, we start at the root node and recursively traverse the tree, choosing the subtree on the same side of the splitting plane as the query point at each level.

For example, if we want to find the nearest neighbor of the point (3,5), we start at the root node and compare the distance between (3,5) and (7,2) to the distance between (3,5) and the splitting plane along the x-axis. Since (3,5) lies to the left of the splitting plane, we traverse the left subtree.

At the next level, we compare the distance between (3,5) and the point (5,4) to the distance between (3,5) and the splitting plane along the y-axis. Since (3,5) lies above the splitting plane, we traverse the right subtree.

At the next level, we compare the distance between (3,5) and the point (2,3) to the distance between (3,5) and the splitting plane along the x-axis. Since (3,5) lies to the right of the splitting plane, we traverse the right subtree.

At the final level, we compare the distance between (3,5) and the point (4,7) to the distance between (3,5) and the point (8,1), which are the only two points in the subtree. We find that the nearest neighbor of (3,5) is (4,7).

In Python, we can construct a k-d tree using the `scipy` library:

```python
from scipy.spatial import KDTree

points = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]
tree = KDTree(points)
```

Once we have constructed the k-d tree, we can perform various types of queries such as nearest neighbor search and range search. For example, to find the nearest neighbor of a query point, we can use the `query` method:

```python
query_point = (3,5)
distance, nearest_point = tree.query(query_point)
```

This will return the distance between the query point and its nearest neighbor in the k-d tree, as well as the coordinates of the nearest neighbor.

We can also perform range searches to find all points within a certain distance from a query point:

```python
query_point = (3,5)
radius = 2
indices = tree.query_ball_point(query_point, radius)
```

This will return a list of indices of all points within a radius of 2 units from the query point.

## Conclusion

K-d trees are a powerful data structure for organizing points in multi-dimensional space. They offer efficient construction and fast searches for nearest neighbors and points within a given range. However, k-d trees can suffer from the curse of dimensionality and can be sensitive to the order in which points are inserted.

In applications where the dimensionality of the data is high, other data structures such as ball trees or cover trees may be more suitable. Overall, k-d trees are a valuable tool for efficient multi-dimensional data organization and searching.

## Related variations

-   [Quadtree](https://en.wikipedia.org/wiki/Quadtree "Quadtree"), a space-partitioning structure that splits in two dimensions simultaneously, so that each node has 4 children
-   [Octree](https://en.wikipedia.org/wiki/Octree "Octree"), a space-partitioning structure that splits in three dimensions simultaneously, so that each node has 8 children
-   [Ball tree](https://en.wikipedia.org/wiki/Ball_tree "Ball tree"), a multi-dimensional space partitioning useful for nearest neighbor search
-   [R-tree](https://en.wikipedia.org/wiki/R-tree "R-tree") and [bounding interval hierarchy](https://en.wikipedia.org/wiki/Bounding_interval_hierarchy "Bounding interval hierarchy"), structure for partitioning objects rather than points, with overlapping regions
-   [Vantage-point tree](https://en.wikipedia.org/wiki/Vantage-point_tree "Vantage-point tree"), a variant of a _k_-d tree that uses hyperspheres instead of hyperplanes to partition the data

## References
- [k-d tree - Wikipedia](https://en.wikipedia.org/wiki/K-d_tree)
- [KD Tree Example — astroML 0.4 documentation](https://www.astroml.org/book_figures/chapter2/fig_kdtree_example.html)
- [sklearn.neighbors.KDTree — scikit-learn 1.2.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html)
- 

