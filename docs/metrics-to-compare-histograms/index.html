
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="http://127.0.0.1:8000/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="http://127.0.0.1:8000/styles/custom.css">








    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="Learn about metrics used to compare histograms with examples of how to calculate them in python. From Chi-Squared distance to Kullback-Leibler divergence and Earth Mover&#39;s distance. A comprehensive guide."/>
    <meta name="keywords" content="histogram, statistics, machine-learning, statistical-tests, metrics, distance-metrics">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Metrics Used to Compare Histograms"/>
  <meta property="og:description" content="Learn about metrics used to compare histograms with examples of how to calculate them in python. From Chi-Squared distance to Kullback-Leibler divergence and Earth Mover&#39;s distance. A comprehensive guide."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="http://127.0.0.1:8000/metrics-to-compare-histograms/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2020-01-19 00:00:00+01:00"/>
  <meta property="article:modified_time" content="2023-07-12 00:00:00+02:00"/>
  <meta property="article:author" content="http://127.0.0.1:8000/author/krystian-safjan/"/>
  <meta property="article:section" content="Machine Learning"/>
  <meta property="article:tag" content="histogram"/>
  <meta property="article:tag" content="statistics"/>
  <meta property="article:tag" content="machine-learning"/>
  <meta property="article:tag" content="statistical-tests"/>
  <meta property="article:tag" content="metrics"/>
  <meta property="article:tag" content="distance-metrics"/>
  <meta property="og:image" content="http://127.0.0.1:8000//images/head/compare_histograms.jpg"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image" content="http://127.0.0.1:8000//images/head/compare_histograms.jpg"/>
    <meta name="twitter:image:alt" content="Metrics Used to Compare Histograms"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="Metrics Used to Compare Histograms"/>
    <meta name="twitter:description" content="Learn about metrics used to compare histograms with examples of how to calculate them in python. From Chi-Squared distance to Kullback-Leibler divergence and Earth Mover's distance. A..."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://127.0.0.1:8000/metrics-to-compare-histograms/"
  },
  "headline": "Metrics Used to Compare Histograms",
  "datePublished": "2020-01-19T00:00:00+01:00",
  "dateModified": "2023-07-12T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "http://127.0.0.1:8000/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "http://127.0.0.1:8000//images/head/compare_histograms.jpg",  "url": "http://127.0.0.1:8000/metrics-to-compare-histograms/",
  "description": "Learn about metrics used to compare histograms with examples of how to calculate them in python. From Chi-Squared distance to Kullback-Leibler divergence..."
}
</script>

    <title>    Metrics Used to Compare Histograms
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="http://127.0.0.1:8000/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="http://127.0.0.1:8000/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist | Researcher | Team Leader<br><br> working at Ernst &amp; Young and writing about <a href="/category/machine-learning.html">Data Science and Visualization</a>, on <a href="/category/machine-learning.html">Machine Learning, Deep Learning</a> and <a href="/tag/nlp/">NLP</a>. There are also some <a href="/category/howto.html">howto</a> posts on tools and workflows.</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A â€¢ PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav>
            <a href="http://127.0.0.1:8000/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>



            <div id="search" class="nav-search"></div>
            <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2020-01-19 


                <br/>
            </p>
            <h1 id="metrics-to-compare-histograms">Metrics Used to Compare Histograms</h1>
            <div class="header-underline"></div>
                <p class="summary"><p>Learn about metrics used to compare histograms with examples of how to calculate them in python. From Chi-Squared distance to Kullback-Leibler divergence and Earth Mover's distance. A comprehensive guide.</p></p>

                <div style="width: 100%; padding-bottom: 30px; position: relative;">
                    <a href="http://127.0.0.1:8000/metrics-to-compare-histograms/">
                        <img style="width: 100%; "
                             src="/images/head/compare_histograms.jpg" alt="Metrics Used to Compare Histograms">
                    </a>
                </div>


        </header>



        <details class="toc-details" id="toc-container">
            <summary>Table of Contents</summary>
            <nav class="toc" aria-label="Table of Contents">
                <ul class="toc-list"></ul>
            </nav>
        </details>

        <div class="article-content">
            <h2 id="introduction">Introduction</h2>
<p>Comparing histograms is a crucial step in data analysis, as it allows us to gain insights into the underlying distributions of our data. There are several metrics that can be used to compare histograms, each with its own strengths and weaknesses. In this article, we will discuss some of the most commonly used metrics for comparing histograms and provide examples of how to calculate them in Python.</p>
<!-- MarkdownTOC autolink="true" autoanchor="true" -->

<ul>
<li><a href="#most-common-methods">Most common methods</a></li>
<li><a href="#1-chi-squared-distance">1. Chi-Squared Distance</a></li>
<li><a href="#2-earth-movers-distance">2. Earth Mover's Distance</a></li>
<li><a href="#3-kullback-leibler-divergence">3. Kullback-Leibler Divergence</a></li>
<li><a href="#other-methods-for-histogram-comparison">Other methods for histogram comparison</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<!-- /MarkdownTOC -->

<p><a id="most-common-methods"></a></p>
<h2 id="most-common-methods">Most common methods</h2>
<p><a id="1-chi-squared-distance"></a></p>
<h3 id="1-chi-squared-distance">1. Chi-Squared Distance</h3>
<p>The Chi-Squared distance, also known as the Chi-Squared test, measures the difference between two histograms by comparing the observed frequencies to the expected frequencies. The Chi-Squared distance is defined as:</p>
<p>$$ \chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i} $$</p>
<p>Where $O_i$ is the observed frequency in bin $i$, $E_i$ is the expected frequency in bin $i$, and $n$ is the number of bins. The Chi-Squared distance is sensitive to large differences between the observed and expected frequencies, and is commonly used in hypothesis testing to determine if two histograms come from the same distribution.</p>
<p>To calculate the Chi-Squared distance in Python, we can use the <code>scipy.stats.chisquare</code> function from the SciPy library. Here is an example of how to use this function to calculate the Chi-Squared distance between two histograms:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chisquare</span>

<span class="c1"># observed frequencies</span>
<span class="n">obs1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>
<span class="n">obs2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">]</span>

<span class="c1"># calculate the Chi-Squared distance</span>
<span class="n">chi2</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">chisquare</span><span class="p">(</span><span class="n">obs1</span><span class="p">,</span> <span class="n">obs2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">chi2</span><span class="p">)</span>
</code></pre></div>

<p><a id="2-earth-movers-distance"></a></p>
<h3 id="2-earth-movers-distance">2. Earth Mover's Distance</h3>
<p>The Earth Mover's distance (EMD) is a more sophisticated metric that takes into account the shape of the histograms as well as the differences in frequency. The EMD is defined as the minimum amount of "work" required to transform one histogram into the other, where "work" is defined as the product of the difference in frequency and the distance between the bins. The EMD is commonly used in image processing and computer vision, but can also be used to compare histograms.</p>
<p>To calculate the EMD in Python, we can use the <code>emd</code> function from the <code>pyemd</code> library. Here is an example of how to use this function to calculate the EMD between two histograms:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pyemd</span> <span class="kn">import</span> <span class="n">emd</span>

<span class="c1"># histogram bin centers</span>
<span class="n">bins1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">bins2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">]</span>

<span class="c1"># histogram frequencies</span>
<span class="n">freq1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>
<span class="n">freq2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">]</span>

<span class="c1"># calculate the EMD</span>
<span class="n">emd_val</span> <span class="o">=</span> <span class="n">emd</span><span class="p">(</span><span class="n">bins1</span><span class="p">,</span> <span class="n">bins2</span><span class="p">,</span> <span class="n">freq1</span><span class="p">,</span> <span class="n">freq2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">emd_val</span><span class="p">)</span>
</code></pre></div>

<p><a id="3-kullback-leibler-divergence"></a></p>
<h3 id="3-kullback-leibler-divergence">3. Kullback-Leibler Divergence</h3>
<p>The Kullback-Leibler divergence (KLD), also known as the relative entropy, measures the difference between two probability distributions. The KLD is defined as:</p>
<p>$$ D_{KL}(P||Q) = \sum_{i=1}^{n} P(i) \log\frac{P(i)}{Q(i)} $$</p>
<p>Where $P$ is the probability distribution of the first histogram, $Q$ is the probability distribution of the second histogram, and $n$ is the number of bins. The KLD is a measure of the information lost when approximating one histogram with the other. It is commonly used in information theory and machine learning.</p>
<p>To calculate the KLD in Python, we can use the <code>scipy.stats.entropy</code> function from the SciPy library. Here is an example of how to use this function to calculate the KLD between two histograms:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">entropy</span>

<span class="c1"># histogram frequencies</span>
<span class="n">freq1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>
<span class="n">freq2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">]</span>

<span class="c1"># normalize the frequencies to get probability distributions</span>
<span class="n">prob1</span> <span class="o">=</span> <span class="n">freq1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">freq1</span><span class="p">)</span>
<span class="n">prob2</span> <span class="o">=</span> <span class="n">freq2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">freq2</span><span class="p">)</span>

<span class="c1"># calculate the KLD</span>
<span class="n">kld</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">prob1</span><span class="p">,</span> <span class="n">prob2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">kld</span><span class="p">)</span>
</code></pre></div>

<p><a id="other-methods-for-histogram-comparison"></a></p>
<h2 id="other-methods-for-histogram-comparison">Other methods for histogram comparison</h2>
<ul>
<li>
<p><strong>Intersection</strong>: it is a simple but widely used measure, which counts the number of bins where the histograms overlap. This metric gives a value between 0 and the minimum number of samples in the two histograms, with 0 indicating no overlap and the maximum value indicating perfect overlap.</p>
</li>
<li>
<p><strong>Bhattacharyya Distance</strong>: The Bhattacharyya distance is a measure of similarity between two histograms. It is based on the Bhattacharyya coefficient, which is a measure of the similarity of two probability distributions.</p>
</li>
<li>
<p><strong>Wasserstein Distance</strong>: Also known as the "Earth Mover's Distance" (EMD), it is a distance measure between probability distributions. It is widely used in image processing and computer vision, and has been applied to the comparison of histograms.</p>
</li>
</ul>
<p>There are other metrics such as <a href="https://en.wikipedia.org/wiki/Hellinger_distance">Hellinger distance</a>, <a href="https://encyclopediaofmath.org/wiki/Jeffreys_distance">Jeffreys divergence</a>, <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence">Jensen-Shannon divergence</a>, etc. that can be used to compare histograms as well.</p>
<p><a id="conclusion"></a></p>
<h2 id="conclusion">Conclusion</h2>
<p>There are several metrics that can be used to compare histograms, each with its own strengths and weaknesses.</p>
<ul>
<li>The <strong>Chi-Squared</strong> distance is <strong>sensitive to large differences in frequency</strong>,</li>
<li>the <strong>Earth Mover's Distance</strong> takes into account the <strong>shape of the histograms</strong>, and</li>
<li>the <strong>Kullback-Leibler Divergence</strong> measures the <strong>information lost</strong> when approximating one histogram with the other.</li>
</ul>
<p>By understanding these metrics and how to calculate them in Python, data scientists can choose the most appropriate metric for their analysis and gain deeper insights into the underlying distributions of their data.</p>
<p>It is always recommended to try out different metrics and choose the best one that suits the problem and data.</p>
<p>To learn more about these metrics and other techniques for comparing histograms, visit the following resources:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Squared Test</a></li>
<li><a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">Earth Mover's Distance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler Divergence</a></li>
<li><a href="https://pypi.org/project/pyemd/">pyemd library</a></li>
<li><a href="https://scipy.org/">SciPy library</a></li>
</ul>
        </div>




            <div class="bibtex-note">
                <p><b>To cite this article:</b></p>
                <pre>@article{Saf2020Metrics,
    author  = {Krystian Safjan},
    title   = {Metrics Used to Compare Histograms},
    journal = {Krystian's Safjan Blog},
    year    = {2020},
}</pre>
            </div>
        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="http://127.0.0.1:8000/tag/histogram/" class="article-tag">histogram</a>
                <a href="http://127.0.0.1:8000/tag/statistics/" class="article-tag">statistics</a>
                <a href="http://127.0.0.1:8000/tag/machine-learning/" class="article-tag">machine-learning</a>
                <a href="http://127.0.0.1:8000/tag/statistical-tests/" class="article-tag">statistical-tests</a>
                <a href="http://127.0.0.1:8000/tag/metrics/" class="article-tag">metrics</a>
                <a href="http://127.0.0.1:8000/tag/distance-metrics/" class="article-tag">distance-metrics</a>
        </div>









    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "http://127.0.0.1:8000",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="http://127.0.0.1:8000/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false,
            basePath: "http://127.0.0.1:8000/pagefind/"
        });
    });
</script>

<script src="http://127.0.0.1:8000/theme/js/theme-switcher.js"></script>
</body>
</html>