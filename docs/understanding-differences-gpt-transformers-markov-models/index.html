
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="https://www.safjan.com/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="https://www.safjan.com/styles/custom.css">

        <link href="https://www.safjan.com/feeds/all.atom.xml?utm_source=rss&utm_medium=feed&utm_campaign=rss-feed" type="application/atom+xml" rel="alternate"
              title="Krystian Safjan's Blog Atom">

        <link href="https://www.safjan.com/feeds/all.rss.xml?utm_source=rss&utm_medium=feed&utm_campaign=rss-feed" type="application/rss+xml" rel="alternate"
              title="Krystian Safjan's Blog RSS">


    <link rel="apple-touch-icon" sizes="180x180" href="https://www.safjan.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://www.safjan.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://www.safjan.com/favicon-16x16.png">
    <link rel="shortcut icon" href="https://www.safjan.com/favicon.ico">
    <link rel="manifest" href="https://www.safjan.com/site.webmanifest">
    <meta name="theme-color" content="#333333">
    <meta name="apple-mobile-web-app-title" content="Krystian Safjan's Blog">

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RM2PKDCCYM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-RM2PKDCCYM');
</script>
<!-- End of Google tag (gtag.js) -->



    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a sequence. It explores the fundamental differences between these models, with a particular emphasis on how the self-attention mechanism in Transformer models makes a difference compared to the fixed context length in Markov models."/>
    <meta name="keywords" content="machine-learning, transformers, markov-models, attention, self-attention, natural-language-processing, nlp, AI, deep-learning, language-models, GPT">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Understanding the Differences in Language Models - Transformers vs. Markov Models"/>
  <meta property="og:description" content="This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a sequence. It explores the fundamental differences between these models, with a particular emphasis on how the self-attention mechanism in Transformer models makes a difference compared to the fixed context length in Markov models."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://www.safjan.com/understanding-differences-gpt-transformers-markov-models/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2023-10-07 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2023-10-07 00:00:00+02:00"/>
  <meta property="article:author" content="https://www.safjan.com/author/krystian-safjan/"/>
  <meta property="article:section" content="Generative AI"/>
  <meta property="article:tag" content="machine-learning"/>
  <meta property="article:tag" content="transformers"/>
  <meta property="article:tag" content="markov-models"/>
  <meta property="article:tag" content="attention"/>
  <meta property="article:tag" content="self-attention"/>
  <meta property="article:tag" content="natural-language-processing"/>
  <meta property="article:tag" content="nlp"/>
  <meta property="article:tag" content="AI"/>
  <meta property="article:tag" content="deep-learning"/>
  <meta property="article:tag" content="language-models"/>
  <meta property="article:tag" content="GPT"/>
  <meta property="og:image" content="https://www.safjan.com//images/head/markov_vs_transformers.jpg"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image" content="https://www.safjan.com//images/head/markov_vs_transformers.jpg"/>
    <meta name="twitter:image:alt" content="Understanding the Differences in Language Models - Transformers vs. Markov Models"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="Understanding the Differences in Language Models - Transformers vs. Markov Models"/>
    <meta name="twitter:description" content="This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a sequence. It explores the fundamental..."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.safjan.com/understanding-differences-gpt-transformers-markov-models/"
  },
  "headline": "Understanding the Differences in Language Models - Transformers vs. Markov Models",
  "datePublished": "2023-10-07T00:00:00+02:00",
  "dateModified": "2023-10-07T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "https://www.safjan.com/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "https://www.safjan.com//images/head/markov_vs_transformers.jpg",  "url": "https://www.safjan.com/understanding-differences-gpt-transformers-markov-models/",
  "description": "This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a...",
  "articleSection": "Generative AI",
  "inLanguage": "en",
  "keywords": "machine-learning, transformers, markov-models, attention, self-attention, natural-language-processing, nlp, AI, deep-learning, language-models, GPT",
  "wordCount": 1118,
  "speakable": {
    "@type": "SpeakableSpecification",
    "cssSelector": ["article h1", ".summary", ".tldr", "article \u003e p:first-of-type"]
  }}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://www.safjan.com/"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "Generative AI",
      "item": "https://www.safjan.com/category/generative-ai.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Understanding the Differences in Language..."
    }
  ]
}
</script>



<meta name="ai:summary" content="This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a sequence. It explores the fundamental differences between these models, with a particular emphasis on how the self-attention mechanism in...">

<meta name="ai:topics" content="machine-learning, transformers, markov-models, attention, self-attention, natural-language-processing, nlp, AI, deep-learning, language-models, GPT">



<meta name="ai:word-count" content="1118">
<meta name="ai:reading-time" content="5 min">

<meta name="ai:section" content="Generative AI">



<meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">

    <title>    Understanding the Differences in Language Models - Transformers vs. Markov Models
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="https://www.safjan.com/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="https://www.safjan.com/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist and Team Leader writing about Machine Learning, MLOps, and Python</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A • PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav aria-label="Main navigation">
            <a href="https://www.safjan.com/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>

                <a href="https://www.safjan.com/feeds/all.atom.xml">Atom</a>

                <a href="https://www.safjan.com/feeds/all.rss.xml">RSS</a>

            <div id="search" class="nav-search"></div>
            <button type="button" id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2023-10-07 


                <br/>
            </p>
            <h1>Understanding the Differences in Language Models - Transformers vs. Markov Models</h1>
            <div class="header-underline"></div>
                <div class="summary"><p>This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a sequence. It explores the fundamental differences between these models, with a particular emphasis on how the self-attention mechanism in Transformer models makes a difference compared to the fixed context length in Markov models.</p></div>

                <div style="width: 100%; padding-bottom: 30px; position: relative;">
                    <a href="https://www.safjan.com/understanding-differences-gpt-transformers-markov-models/">
                        <img style="width: 100%; "
                             src="/images/head/markov_vs_transformers.jpg" alt="Understanding the Differences in Language Models - Transformers vs. Markov Models">
                    </a>
                </div>


        </header>




        <div class="article-content">
            <p>In Machine Learning (ML) and natural language processing (NLP), different models have been developed to understand and generate human language. Two such models that have gained significant attention are the Markov Models and the Transformer-based models like GPT (<a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer">Generative Pretrained Transformer</a>). While both types of models can predict the next character in a sequence, they differ significantly in their underlying mechanisms and capabilities. This article aims to look into the details of these models, with a particular focus on how the self-attention mechanism in Transformer models makes a difference compared to the fixed context length in Markov models.</p>
<h2 id="markov-models-a-brief-overview">Markov Models: A Brief Overview</h2>
<p><a href="https://en.wikipedia.org/wiki/Markov_model">Markov Models</a>, named after the Russian mathematician <a href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>, are a class of models that predict future states based solely on the current state, disregarding all past states. This property is known as the Markov Property, and it is the fundamental assumption that underlies all Markov models.</p>
<p>In the context of language modeling, a Markov Model might predict the next word or character in a sentence based on the current word or character. For instance, given the word "The", a Markov Model might predict that the next word is "cat" based on the probability distribution of words that follow "The" in its training data.</p>
<p>The main limitation of Markov Models is their lack of memory. Since they only consider the current state, they are unable to capture long-term dependencies in a sequence. For example, in the sentence "I grew up in France... I speak fluent ...", a Markov Model might struggle to fill in the blank correctly because the relevant context ("France") is several words back.</p>
<p><img alt="Markov Chain text generation" src="/images/transformers_vs_markov/markov_model_text_generation.png"></p>
<p><strong>Figure 1.</strong> <em>Markov Model might predict  the next word based on the probability distribution of words in its training data. Image Source: <a href="https://jaroslawwiosna.github.io/markov-chain-text/">markov-chain-text | Modern C++ Markov chain text generator</a> by Jarosław Wiosna</em></p>
<h2 id="transformer-models-an-introduction">Transformer Models: An Introduction</h2>
<p>Transformer models, on the other hand, were introduced in the seminal paper <a href="https://arxiv.org/abs/1706.03762">"Attention is All You Need"</a> by Vaswani et al. (2017). They represent a significant departure from previous sequence-to-sequence models, eschewing recurrent and convolutional layers in favor of self-attention mechanisms.</p>
<p>GPT, developed by OpenAI, is a prominent example of a Transformer model. It is a generative model that can generate human-like text by predicting the next word in a sequence. Unlike Markov Models, GPT considers the entire context of a sequence when making predictions, allowing it to capture long-term dependencies.</p>
<h2 id="the-power-of-self-attention">The Power of Self-Attention</h2>
<p>The key innovation of Transformer models is the self-attention mechanism. This mechanism allows the model to weigh the importance of different words in the context when predicting the next word. For instance, in the sentence "The cat, which was black and white, jumped over the ___", the model might assign more importance to "cat" and "jumped" when predicting the next word.</p>
<p>Self-attention is calculated using the dot product of the query and key vectors, which are learned representations of the input. The resulting attention scores are then used to weight the value vectors, which are also learned representations of the input. This weighted sum forms the output of the self-attention layer.</p>
<p>The self-attention mechanism allows Transformer models to consider the entire context of a sequence, rather than just the current state. This is a significant advantage over Markov Models, which are limited by their fixed context length.</p>
<p><img alt="Transformer model - Context and Attention" src="/images/transformers_vs_markov/transformers_context_and_atention.png"></p>
<p><strong>Figure 2.</strong> <em>The self-attention mechanism allows Transformer models to consider the entire context of a sequence, rather than just the current state. Image Source:  <a href="https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the">A Deep Dive Into the Transformer Architecture – The Development of Transformer Models</a> by Kevin Hooke</em></p>
<h2 id="fixed-context-length-vs-variable-context-length">Fixed Context Length vs. Variable Context Length</h2>
<p>Markov Models, due to their inherent design, have a fixed context length. They only consider the current state when making predictions, which limits their ability to capture long-term dependencies. This can lead to less accurate predictions, especially in complex sequences where the relevant context might be several states back.</p>
<p>Transformer models, on the other hand, have a variable context length. Thanks to the self-attention mechanism, they can consider the entire context of a sequence when making predictions. This allows them to capture long-term dependencies and make more accurate predictions.</p>
<p>Moreover, the self-attention mechanism allows Transformer models to dynamically adjust the context length based on the input. For instance, in a sentence with many irrelevant words, the model might focus on a few key words, effectively reducing the context length. This dynamic context length is another advantage of Transformer models over Markov Models.</p>
<h2 id="conclusion">Conclusion</h2>
<p>While both Markov Models and Transformer models like GPT can predict the next character in a sequence, they differ significantly in their underlying mechanisms and capabilities. Markov Models, with their fixed context length, are limited in their ability to capture long-term dependencies. Transformer models, with their self-attention mechanism, can consider the entire context of a sequence, allowing them to capture long-term dependencies and make more accurate predictions.</p>
<p><em>Any comments or suggestions? <a href="mailto:ksafjan@gmail.com?subject=Blog+post">Let me know</a>.</em></p>
<h2 id="references">References</h2>
<ol>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>. In Advances in neural information processing systems (pp. 5998-6008).</li>
<li>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>. OpenAI Blog.</li>
<li>Bishop, C. M. (2006). <a href="https://www.springer.com/gp/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer.</li>
<li>Ruder, S. (2019). <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>. Jay Alammar's Blog.</li>
<li>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... &amp; Amodei, D. (2020). <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a>. In Advances in Neural Information Processing Systems.</li>
<li>Chollet, F. (2018). <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a>. Manning Publications Co.</li>
<li>Jurafsky, D., &amp; Martin, J. H. (2019). <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>. Stanford University.</li>
<li>Al-Rfou, R., Choe, D., Constant, N., Guo, M., &amp; Jones, L. (2019). <a href="https://arxiv.org/abs/1808.04444">Character-Level Language Modeling with Deeper Self-Attention</a>. In Proceedings of the AAAI Conference on Artificial Intelligence.</li>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <a href="http://www.deeplearningbook.org/">Deep Learning</a>. MIT press.</li>
<li>Manning, C. D., &amp; Schütze, H. (1999). <a href="https://mitpress.mit.edu/books/foundations-statistical-natural-language-processing">Foundations of Statistical Natural Language Processing</a>. MIT Press.</li>
<li>Jurafsky, D., &amp; Martin, J. H. (2009). <a href="https://www.pearson.com/us/higher-education/program/Jurafsky-Speech-and-Language-Processing-An-Introduction-to-Natural-Language-Processing-Computational-Linguistics-and-Speech-Recognition-2nd-Edition/PGM319216.html">Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</a>. Prentice Hall.</li>
<li>Jelinek, F. (1997). <a href="https://mitpress.mit.edu/books/statistical-methods-speech-recognition">Statistical Methods for Speech Recognition</a>. MIT Press.</li>
<li>Russell, S., &amp; Norvig, P. (2016). <a href="http://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach</a>. Pearson.</li>
<li>Charniak, E. (1993). <a href="https://mitpress.mit.edu/books/statistical-language-learning">Statistical Language Learning</a>. MIT Press.</li>
<li>Lin, T. (2015). <a href="https://towardsdatascience.com/markov-chains-and-text-generation-162fd4ec8f26">Markov Chains and Text Generation</a>. Towards Data Science Blog.</li>
<li>Goodman, J. (2001). <a href="https://www.microsoft.com/en-us/research/publication/a-bit-of-progress-in-language-modeling/">A bit of progress in language modeling</a>. Microsoft Research.</li>
<li>Rosenfeld, R. (2000). <a href="https://www.cs.cmu.edu/~roni/papers/SLM-hlt01.pdf">Two Decades of Statistical Language Modeling: Where Do We Go From Here?</a>. Proceedings of the IEEE.</li>
<li>Nazarko, K. (2021). <a href="https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e">Word-level text generation using GPT-2, LSTM and Markov Chain</a>. Towards Data Science Blog.</li>
<li>Adyatama, A. (2020). <a href="https://algotech.netlify.app/blog/text-generating-with-markov-chains/">Text Generation with Markov Chains</a> Algoritma Technical Blog.</li>
</ol>
<p>X::[[transformers_vs_markov_models_illustrations]]</p>
        </div>




            <div class="bibtex-note">
                <p><b>To cite this article:</b></p>
                <pre>@article{Saf2023Understanding,
    author  = {Krystian Safjan},
    title   = {Understanding the Differences in Language Models - Transformers vs. Markov Models},
    journal = {Krystian's Safjan Blog},
    year    = {2023},
}</pre>
            </div>
        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="https://www.safjan.com/tag/machine-learning/" class="article-tag">machine-learning</a>
                <a href="https://www.safjan.com/tag/transformers/" class="article-tag">transformers</a>
                <a href="https://www.safjan.com/tag/markov-models/" class="article-tag">markov-models</a>
                <a href="https://www.safjan.com/tag/attention/" class="article-tag">attention</a>
                <a href="https://www.safjan.com/tag/self-attention/" class="article-tag">self-attention</a>
                <a href="https://www.safjan.com/tag/natural-language-processing/" class="article-tag">natural-language-processing</a>
                <a href="https://www.safjan.com/tag/nlp/" class="article-tag">nlp</a>
                <a href="https://www.safjan.com/tag/ai/" class="article-tag">AI</a>
                <a href="https://www.safjan.com/tag/deep-learning/" class="article-tag">deep-learning</a>
                <a href="https://www.safjan.com/tag/language-models/" class="article-tag">language-models</a>
                <a href="https://www.safjan.com/tag/gpt/" class="article-tag">GPT</a>
        </div>











    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "https://www.safjan.com",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="https://www.safjan.com/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false,
            basePath: "https://www.safjan.com/pagefind/"
        });
    });
</script>

<script src="https://www.safjan.com/theme/js/theme-switcher.js"></script>
</body>
</html>