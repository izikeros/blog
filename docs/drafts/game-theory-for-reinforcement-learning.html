
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

        <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/stylesheet/style.min.css">



        <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              href="https://www.safjan.com/theme/pygments/github.min.css">



    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/solid.css">

        <link rel="stylesheet" type="text/css"
              href="https://www.safjan.com/styles/custom.css">

        <link href="https://www.safjan.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Krystian Safjan's Blog Atom">

        <link href="https://www.safjan.com/feeds/all.rss.xml" type="application/rss+xml" rel="alternate"
              title="Krystian Safjan's Blog RSS">


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117080232-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-117080232-1');
</script>
<!-- End of Google tag (gtag.js) -->



    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content=""/>
    <meta name="keywords" content="machine-learning, reinforcement-learning, rl, game-theory">
    <meta expr:content="2023-02-08 00:00:00+01:00" itemprop='datePublished'/>
    <meta expr:content="2023-02-08 00:00:00+01:00" itemprop='dateModified'/>
    <meta property="article:modified_time" content="2023-02-08 00:00:00+01:00"/>
    <meta property="article:published_time" content="2023-02-08 00:00:00+01:00"/>
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Game theory for the Reinforcement Learning",
  "datePublished": "2023-02-08 00:00:00+01:00",
  "dateModified": "2023-02-08 00:00:00+01:00"
}



    </script>



  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Game theory for the Reinforcement Learning"/>
  <meta property="og:description" content=""/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://www.safjan.com/drafts/game-theory-for-reinforcement-learning.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2023-02-08 00:00:00+01:00"/>
  <meta property="article:modified_time" content="2023-02-08 00:00:00+01:00"/>
  <meta property="article:author" content="https://www.safjan.com/author/krystian-safjan/">
  <meta property="article:section" content="Machine Learning"/>
  <meta property="article:tag" content="machine-learning"/>
  <meta property="article:tag" content="reinforcement-learning"/>
  <meta property="article:tag" content="rl"/>
  <meta property="article:tag" content="game-theory"/>
  <meta property="og:image" content="https://www.safjan.com//images/zsh/inside-work-tree.jpg">

    <meta name="twitter:card" content="summary"/>
    <meta property="twitter:image" content="https://www.safjan.com//images/zsh/inside-work-tree.jpg">

    <meta name="twitter:label1" content="Est. reading time"/>
    <meta name="twitter:data1" content="4 min."/>
    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:description" content=""/>
    <meta name="twitter:title" content="Game theory for the Reinforcement Learning"/>


    <title>    Game theory for the Reinforcement Learning
</title>


<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body class="light-theme">
<aside>
    <div>
        <a href="https://www.safjan.com/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="https://www.safjan.com/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist | Researcher | Team Leader<br><br> working at Ernst &amp; Young and writing about <a href="/category/machine-learning.html">Data Science and Visualization</a>, on <a href="/category/data-science.html">Machine Learning, Deep Learning</a> and <a href="/tag/nlp/">NLP</a>. There are also some  <a href="/category/howto.html">howto</a> posts on tools and workflows.</p>




        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<style>
    .button {
        background-color: yellow;
        color: black;
        border: none;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 20px;
    }

    .center {
        text-align: center;
    }

    .container {
        display: grid;
        grid-template-columns: 1fr 1fr;
    }

    .left-col {
    }

    .right-col {
    }

    .image {
        border-radius: 0;
        width: 100%;
    }

    .book-list {
        padding-top:0;
        padding-bottom: 0;
        text-align:left;
        box-sizing: border-box;
        display: block;
        list-style-image: url(/images/shortcode-tick.webp);
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start:0;
        margin-inline-end:0;
        padding-inline-start:40px;
    }

    .book-list li {
        font-weight: bold;
    }

    @media (max-width: 600px) {
        .container {
            grid-template-columns: 1fr;
        }
    }

</style>
<div class="container">
    <div class="left-col">
        <a href="https://gumroad.com/">
            <img src="/images/mlop_interview_book_cover_3D_300px.jpg" class="image" alt="Interview Book Cover">
        </a>

        <div class="center">
            <a href="https://gumroad.com/">
                <button class="button">Get for $0.99</button>
            </a>
        </div>
    </div>
    <div class="right-col">
        <div>
            <ul class="book-list">
                <li>PDF, ePUB, mobi format ebook, no DRM</li>
                <li>50 questions and answers</li>
                <li>Stories from real projects</li>
                <li>100 multiple choice quizz questions</li>
                <li>178 pages</li>
            </ul>
        </div>
    </div>
</div>
</aside>
<main>

        <nav>
            <a href="https://www.safjan.com/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/tags.html">Tags</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>

                <a href="https://www.safjan.com/feeds/all.atom.xml">Atom</a>

                <a href="https://www.safjan.com/feeds/all.rss.xml">RSS</a>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2023-02-08 


                <br/>
            </p>
            <h1 id="game-theory-for-reinforcement-learning">Game theory for the Reinforcement Learning</h1>
            <div class="header-underline"></div>



        </header>


        <div>
            <p>up:[[game_theory]]</p>
<h1>blog/post-idea</h1>
<blockquote>
<p>Give me detailed outline of the tutorial targetted at data-scientists: "Game theory for the Reinforcement Learning"</p>
</blockquote>
<p>I. Introduction 
A. Overview of Game Theory 
1. Definition and Origins 
2. Key Concepts and Notions 
B. Overview of Reinforcement Learning
1. Definition and Applications
2. Key Concepts and Methods
C. Motivation for Using Game Theory in Reinforcement Learning
1. Understanding Interactions between Agents and the Environment 
2. Modeling Complex Decisions and Uncertainty 
D. Outline of the Tutorial</p>
<p>II. Fundamentals of Game Theory
A. Games and Players
1. Types of Games
    a. Zero-Sum Games
    b. Non-Zero-Sum Games
2. Players and Strategies
B. Equilibrium Concepts
1. Nash Equilibrium
2. Minimax and Maximin
C. Extensions to Dynamic Games
1. Stochastic Games
2. Repeated Games</p>
<p>III. Applications of Game Theory in Reinforcement Learning 
A. Modelling Interactions between Agents and the Environment 
1. Multi-Agent Reinforcement Learning 
2. Cooperative and Competitive Settings 
    B. Incorporating Uncertainty in Decision Making 
    1. Bayesian Reinforcement Learning 
    2. POMDPs and Partially Observable Markov Decision Processes 
    C. Case Studies 
    1. Application of Game Theory in Robotics 
    2. Application of Game Theory in Finance</p>
<p>IV. Implementation of Game Theory in Reinforcement Learning 
A. Algorithms 
1. Q-Learning 
2. SARSA 
3. Actor-Critic 
B. Tools and Frameworks 
1. OpenAI Gym 
2. TensorFlow and Keras 
C. Best Practices and Considerations
1. Choice of Game Theory Model
2. Hyperparameter Optimization
3. Model Selection and Validation</p>
<p>V. Conclusion
A. Summary of Key Points 
B. Future Trends and Opportunities 
C. Final Thoughts and Takeaways</p>
<p>VI. References 
A. Books and Articles 
B. Online Resources 
C. Datasets and Tools.</p>
<h1>book</h1>
<blockquote>
<p>give me long text for the next section: I.B. part from the outline you proposed - Use markdown hyperlinks on crucial terms and tools. Provide mathematical formulas in LaTeX in display format (not inline). If there is any process described provide mermaid diagram of mermaid gantt chart</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="n">style</span><span class="o">:</span><span class="w"> </span><span class="n">bullet</span>
<span class="n">min_depth</span><span class="o">:</span><span class="w"> </span><span class="mi">2</span>
<span class="n">max_depth</span><span class="o">:</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span>
<span class="n">title</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;**Contents**&quot;</span>
</code></pre></div>

<h2>Introduction</h2>
<h3>Overview of Game Theory</h3>
<p>Game theory is a branch of mathematics that deals with the study of strategic interactions between rational decision-makers. It was developed in the 1940s by John von Neumann and Oskar Morgenstern and has since found applications in a wide range of fields, including economics, political science, psychology, and biology.</p>
<p>Game theory provides a theoretical framework for understanding and modeling decision-making in situations where multiple rational agents are involved. In such situations, each agent must make decisions based on their own interests and beliefs, while also taking into account the decisions of other agents.</p>
<p>The key concepts and notions in game theory include:</p>
<ul>
<li>
<p>Games and players: In game theory, a game is a situation where multiple rational agents make decisions based on their own interests and beliefs. The agents are referred to as players.</p>
</li>
<li>
<p>Strategies: A strategy is a plan of action that a player uses to determine their decisions in a game. A player's strategy is a function of their beliefs and objectives.</p>
</li>
<li>
<p>Equilibria: An equilibrium is a state of a game in which each player's strategy is optimal given the strategies of the other players.</p>
</li>
<li>
<p>Nash Equilibrium: The Nash Equilibrium is a concept in game theory that refers to a state of a game in which each player's strategy is optimal given the strategies of the other players. It is named after the Nobel Prize-winning economist John Nash.</p>
</li>
</ul>
<p>Mathematically, the Nash Equilibrium can be defined as follows:</p>
<div class="math">$$\begin{aligned} x^* &amp;= \arg\max_{x_i} u_i(x_1, \ldots, x_n) \ \text{s.t.} &amp;\quad x_i \in X_i, \quad i = 1, \ldots, n \ &amp;\quad x_i = x^*_i, \quad i = 1, \ldots, n \end{aligned}$$</div>
<p>where <span class="math">\(u_i\)</span> is the utility function of player <span class="math">\(i\)</span>, <span class="math">\(x_i\)</span> is their strategy, <span class="math">\(X_i\)</span> is the set of strategies available to player <span class="math">\(i\)</span>, and <span class="math">\(n\)</span> is the number of players in the game.</p>
<ul>
<li>Minimax and Maximin: Minimax and maximin are concepts in game theory that refer to the worst-case and best-case scenarios for a player in a game, respectively.</li>
</ul>
<p>Mathematically, the minimax value of a player can be defined as follows:</p>
<div class="math">$$\min\limits_{x_i} \max\limits_{x_{-i}} u_i(x_i, x_{-i})$$</div>
<p>where <span class="math">\(x_{-i}\)</span> represents the strategies of all players except player <span class="math">\(i\)</span>.</p>
<p>Similarly, the maximin value of a player can be defined as follows:</p>
<div class="math">$$\max\limits_{x_i} \min\limits_{x_{-i}} u_i(x_i, x_{-i})$$</div>
<p>Minimax and maximin provide a way for players to reason about their worst-case and best-case scenarios in a game and make decisions accordingly.</p>
<ul>
<li>
<p>Dynamic Games: Dynamic games refer to games in which the decision-making process unfolds over time. In dynamic games, the strategies of the players can depend on the history of the game.</p>
</li>
<li>
<p>Stochastic Games: Stochastic games are a type of dynamic game in which the outcome of the game is uncertain. In stochastic games, the players make decisions based on probabilistic information.</p>
</li>
<li>
<p>Repeated Games: Repeated games are a type of dynamic game in which the same game is played multiple times. In repeated games, the players can use past experiences to inform their current and future decisions.</p>
</li>
<li>
<p>Evolutionary Games: Evolutionary games are a type of dynamic game in which the strategies of the players evolve over time. Evolutionary games are used to model the evolution of behaviors and strategies in populations of organisms.</p>
</li>
</ul>
<p>In conclusion, game theory provides a rich and powerful theoretical framework for understanding and modeling decision-making in situations involving multiple rational agents. The concepts and tools of game theory have been applied in a wide range of fields and have been used to shed light on a variety of phenomena, from market competition to animal behavior.</p>
<p>In the next section, we will discuss how game theory can be applied in the field of reinforcement learning, providing a deeper understanding of decision-making in complex, dynamic systems.</p>
<h3>Game Theory for Reinforcement Learning</h3>
<p>Reinforcement learning is a branch of machine learning that deals with the problem of learning from experience. In reinforcement learning, an agent interacts with an environment and receives rewards or penalties for its actions. The goal of the agent is to learn a policy that maximizes its long-term reward.</p>
<p>Reinforcement learning can be thought of as a type of game between an agent and an environment, in which the agent chooses actions and the environment responds with rewards. This interaction can be modeled as a Markov Decision Process (MDP), in which the state of the environment, the actions of the agent, and the rewards received form a Markov chain.</p>
<p>The use of game theory in reinforcement learning can provide a deeper understanding of the decision-making process in these systems and lead to the development of more effective and efficient algorithms. Some of the key ways in which game theory can be applied to reinforcement learning include:</p>
<ul>
<li>
<p>Modeling multi-agent reinforcement learning as a game: In multi-agent reinforcement learning, multiple agents interact with each other and with an environment. Game theory provides a natural framework for modeling and analyzing these interactions.</p>
</li>
<li>
<p>Using Nash Equilibria to optimize policies: Nash Equilibria can be used to find optimal policies for an agent in a multi-agent reinforcement learning scenario. The Nash Equilibrium represents a state in which no agent can improve its reward by changing its policy, given the policies of the other agents.</p>
</li>
<li>
<p>Modeling competition and cooperation: Game theory can be used to model and analyze the interactions between agents in a reinforcement learning scenario, including both competition and cooperation. This can lead to a better understanding of how these interactions affect the behavior and performance of the agents.</p>
</li>
<li>
<p>Incorporating uncertainty: Stochastic games can be used to model uncertainty in reinforcement learning scenarios. In stochastic games, the reward received by an agent is uncertain, and the agent must make decisions based on probabilistic information.</p>
</li>
<li>
<p>Exploiting repetition: Repeated games can be used to exploit the repetition of the reinforcement learning scenario. In repeated games, an agent can use past experiences to inform its current and future decisions.</p>
</li>
</ul>
<p>Mathematically, a reinforcement learning scenario can be modeled as a Markov Decision Process (MDP) with the following components:</p>
<ul>
<li>A set of states <span class="math">\(S\)</span></li>
<li>A set of actions <span class="math">\(A\)</span></li>
<li>A transition function <span class="math">\(T(s, a, s')\)</span> that describes the probability of transitioning from state <span class="math">\(s\)</span> to state <span class="math">\(s'\)</span> after taking action <span class="math">\(a\)</span></li>
<li>A reward function <span class="math">\(R(s, a)\)</span> that describes the reward received after taking action <span class="math">\(a\)</span> in state <span class="math">\(s\)</span></li>
</ul>
<p>The goal of the agent is to learn a policy <span class="math">\(\pi\)</span> that maximizes its expected reward over time:</p>
<div class="math">$$J(\pi) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t | \pi]$$</div>
<p>where <span class="math">\(r_t\)</span> is the reward received at time <span class="math">\(t\)</span>, and <span class="math">\(\gamma \in [0, 1]\)</span> is a discount factor that trades off the importance of immediate rewards versus future rewards.</p>
<p>In conclusion, game theory provides a powerful tool for understanding and modeling decision-making in reinforcement learning scenarios. The concepts and tools of game theory can be used to model multi-agent interactions, incorporate uncertainty, and exploit repetition. These insights can lead to the development of more effective and efficient algorithms for reinforcement learning.</p>
<h3>Solving Reinforcement Learning Problems Using Game Theory</h3>
<p>One of the main ways in which game theory can be applied to reinforcement learning is through the use of game-theoretic algorithms. These algorithms use the concepts and tools of game theory to solve reinforcement learning problems, such as finding optimal policies or equilibria in multi-agent reinforcement learning scenarios.</p>
<p>One example of a game-theoretic algorithm for reinforcement learning is the Nash Q-Learning algorithm. This algorithm combines the concepts of Nash Equilibria and Q-Learning, a popular reinforcement learning algorithm, to find optimal policies in multi-agent reinforcement learning scenarios.</p>
<p>The Nash Q-Learning algorithm works by modeling the reinforcement learning scenario as a game, with the agents as players and their policies as strategies. The algorithm uses Q-Learning to estimate the expected reward for each action in each state, and then uses Nash Equilibria to find the optimal policies for the agents.</p>
<p>The Nash Q-Learning algorithm can be described as follows:</p>
<ol>
<li>
<p>Initialize the Q-values for each state-action pair, <span class="math">\(Q(s, a)\)</span>.</p>
</li>
<li>
<p>For each episode:</p>
</li>
<li>
<p>a. Initialize the state, <span class="math">\(s_0\)</span>.</p>
</li>
<li>b. For each time step:<ul>
<li>i. Choose an action for each agent, <span class="math">\(a_t = \pi_i(s_t)\)</span>, based on its current policy.</li>
<li>ii. Execute the actions and observe the new state, <span class="math">\(s_{t+1}\)</span>, and the rewards, <span class="math">\(r_t\)</span>.</li>
<li>iii. Update the Q-values using the Q-Learning update rule: <div class="math">$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left(r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\right)$$</div>
</li>
</ul>
</li>
<li>
<p>c. Update the policies for each agent, <span class="math">\(\pi_i(s) = \arg\max_{a} Q(s, a)\)</span>, based on the estimated Q-values.</p>
</li>
<li>
<p>Repeat the steps 2-3 until convergence, i.e., the policies have stabilized and the Q-values have converged.</p>
</li>
</ol>
<p>The Nash Q-Learning algorithm can be used to find the Nash Equilibria of the game, i.e., the state in which no agent can improve its reward by changing its policy, given the policies of the other agents. This provides a way to find optimal policies in multi-agent reinforcement learning scenarios, taking into account the interactions between the agents.</p>
<p>Game-theoretic algorithms provide a way to solve reinforcement learning problems using the concepts and tools of game theory. The Nash Q-Learning algorithm is an example of such an algorithm, and it can be used to find optimal policies in multi-agent reinforcement learning scenarios.</p>
<h3>Conclusion and Future Work</h3>
<p>In this tutorial, we have explored the use of game theory in reinforcement learning, and how game-theoretic algorithms can be used to solve reinforcement learning problems. We have seen that game theory provides a way to model the interactions between agents in multi-agent reinforcement learning scenarios, and how game-theoretic concepts, such as Nash Equilibria, can be used to find optimal policies.</p>
<p>However, there is still much work to be done in this field, and many open questions that need to be addressed. For example, how to handle more complex game-theoretic models, such as games with incomplete information or games with continuous action spaces, in reinforcement learning scenarios. Additionally, there is a need for more research on the computational complexity of game-theoretic algorithms, and how to make these algorithms more efficient and scalable.</p>
<p>Another area for future work is the integration of game theory and deep reinforcement learning, where deep neural networks can be used to model the policies of the agents in the game. This can lead to the development of more sophisticated and powerful game-theoretic algorithms for reinforcement learning, and enable the application of game theory to new domains and use cases.</p>
<p>In conclusion, game theory has the potential to be a valuable tool for solving reinforcement learning problems, particularly in multi-agent scenarios. However, there is still much work to be done to fully realize this potential, and the field of game theory and reinforcement learning is an active and growing area of research.</p>
<h2>Fundamentals of Game Theory</h2>
<h3>Games and Players</h3>
<h4>Types of Games</h4>
<h5>Zero-Sum Games</h5>
<p>In game theory, a zero-sum game is a type of game where one player's gain is exactly equal to the other player's loss. In other words, the total payoff for the game is always zero. This means that if one player wins, the other player loses an equal amount, and vice versa.</p>
<p>A zero-sum game can be modeled as a matrix game, where the rows represent the actions of one player, and the columns represent the actions of the other player. The entries in the matrix represent the payoffs for each player, and the payoffs are typically represented as a pair of numbers, with the first number representing the payoff for one player and the second number representing the payoff for the other player.</p>
<p>For example, consider the following matrix game, where Player 1 has two actions, A and B, and Player 2 has two actions, C and D:</p>
<div class="highlight"><pre><span></span><code>   C  D
A  (1, -1)  (2, -2)
B  (-2, 2)  (-1, 1)
</code></pre></div>

<p>In this game, if Player 1 chooses action A and Player 2 chooses action C, then Player 1 receives a payoff of 1 and Player 2 receives a payoff of -1. If Player 1 chooses action B and Player 2 chooses action D, then Player 1 receives a payoff of -1 and Player 2 receives a payoff of 1.</p>
<h5>Non-Zero-Sum Games</h5>
<p>In contrast to zero-sum games, non-zero-sum games are games where the total payoff is not always zero. This means that the payoffs for the players are not necessarily equal and opposite.</p>
<p>A non-zero-sum game can also be modeled as a matrix game, with the rows representing the actions of one player, and the columns representing the actions of the other player. However, in this case, the entries in the matrix represent the payoffs for each player, and the payoffs are typically represented as a pair of numbers, with the first number representing the payoff for one player and the second number representing the payoff for the other player.</p>
<p>For example, consider the following matrix game, where Player 1 has two actions, A and B, and Player 2 has two actions, C and D:</p>
<div class="highlight"><pre><span></span><code>   C  D
A  (1, 2)  (3, 1)
B  (2, 1)  (2, 3)
</code></pre></div>

<p>In this game, if Player 1 chooses action A and Player 2 chooses action C, then Player 1 receives a payoff of 1 and Player 2 receives a payoff of 2. If Player 1 chooses action B and Player 2 chooses action D, then Player 1 receives a payoff of 2 and Player 2 receives a payoff of 3.</p>
<p>It is important to note that in non-zero-sum games, there is no clear winner or loser, as the payoffs for the players are not necessarily equal and opposite. Instead, the goal in a non-zero-sum game is to find a mutually beneficial outcome for both players.</p>
<h5>Real-world examples</h5>
<p>Here are a few examples of real-world situations that can be modeled as <strong>zero-sum games</strong>:</p>
<ol>
<li>
<p>Chess: In chess, both players start with equal pieces, and one player wins only if the other player loses. The outcome of the game results in a win or loss for each player, and the total sum of wins and losses is zero.</p>
</li>
<li>
<p>Poker: In poker, the total amount of money in the pot remains constant throughout the game. When one player wins, another player loses an equal amount, so the total sum of wins and losses is zero.</p>
</li>
<li>
<p>Trading: In financial trading, if one trader buys a stock, another trader sells the same stock. The price paid by the buyer is equal to the price received by the seller, so the total sum of wins and losses is zero.</p>
</li>
<li>
<p>Negotiations: In negotiations, if one party gains more than the other party, it results in a zero-sum game. For example, in a salary negotiation, if one party receives a higher salary, the employer will have to pay more, and the total sum of gains and losses is zero.</p>
</li>
</ol>
<p>These are just a few examples of real-world situations that can be modeled as zero-sum games. These models can be useful in understanding the decision-making processes and the outcome of interactions between parties with conflicting goals.</p>
<p>Here are a few examples of real-world situations that can be modeled as <strong>non-zero-sum games</strong>:</p>
<ol>
<li>
<p>Collaboration: In a collaborative project, the total benefit for all participants can be greater than the sum of individual benefits. For example, in a team project, all members can contribute to the success of the project and share in the rewards.</p>
</li>
<li>
<p>Competition and cooperation: In some situations, both competition and cooperation can exist simultaneously. For example, in a market, companies compete for customers while also cooperating in setting standards and creating a stable business environment.</p>
</li>
<li>
<p>Environmental protection: In environmental protection, all parties can benefit from reducing pollution and preserving natural resources. For example, industries can reduce waste and conserve energy, while also benefiting from a cleaner environment.</p>
</li>
<li>
<p>International relations: In international relations, nations can both compete and cooperate in various ways. For example, nations can compete for resources and economic advantage, while also cooperating to address global challenges such as climate change and security.</p>
</li>
</ol>
<h3>Players and Strategies</h3>
<p>These are just a few examples of real-world situations that can be modeled as non-zero-sum games. Non-zero-sum games can be useful in understanding the potential for mutually beneficial outcomes and the balancing of conflicting interests in complex, interdependent systems.</p>
<p>In the field of game theory, the concept of an equilibrium is crucial for understanding the behavior of players in a game. Equilibrium refers to a state where the players have chosen the best strategy for themselves given the strategies of the other players. There are several different types of equilibria, including Nash Equilibrium and Minimax/Maximin.</p>
<p>2.B.1 Nash Equilibrium The Nash Equilibrium, named after Nobel Prize-winning economist John Nash, is a concept in game theory where each player's strategy is optimal given the strategies of the other players. In other words, no player can improve their outcome by unilaterally changing their strategy. Formally, a Nash Equilibrium is defined as a set of strategies, one for each player, such that no player has an incentive to change their strategy given the strategies of the other players.</p>
<p>Mathematically, a Nash Equilibrium can be represented as:</p>
<div class="math">$$
\left(\textbf{s}_1^_, \textbf{s}_2^_, ..., \textbf{s}_n^_\right) \text{ where } \forall i, \textbf{s}_i^_ \text{ is a best response to } \left(\textbf{s}_1^_, \textbf{s}_2^_, ..., \textbf{s}_{i-1}^*, \textbf{s}_{i+1}^_, ..., \textbf{s}_n^_\right)
$$</div>
<p>The Nash Equilibrium is a commonly used solution concept in game theory and has applications in fields such as economics, political science, and computer science.</p>
<p>Here are a few examples of data science problems that can be solved using Nash Equilibrium:</p>
<ol>
<li>
<p>Market Equilibrium: A common example of Nash Equilibrium in economics is the determination of market prices. If there are multiple firms selling the same product, each firm will choose its price based on the prices of other firms, and the demand for its product. The market will reach a Nash Equilibrium when no firm can increase its profits by changing its price.</p>
</li>
<li>
<p>Resource Allocation: In many real-world problems, multiple agents must compete for limited resources. For instance, in cloud computing, multiple users may want to use the same computing resources. Nash Equilibrium can be used to determine the allocation of resources that is fair to all agents, and ensures that no agent can benefit from changing its resource allocation strategy.</p>
</li>
<li>
<p>A/B Testing: In online advertising, A/B testing is used to determine which of two or more advertisements is more effective. The advertisements are displayed to different users, and the conversion rate of each advertisement is measured. Nash Equilibrium can be used to determine the optimal number of users to be assigned to each advertisement, and to ensure that no advertiser can improve its conversion rate by changing its advertising strategy.</p>
</li>
<li>
<p>Collaborative Filtering: Collaborative filtering is a widely used technique in recommendation systems. In collaborative filtering, users rate items, and the system recommends items to users based on their ratings. Nash Equilibrium can be used to determine the optimal ratings that users should give to items, and to ensure that no user can improve its recommendations by changing its rating strategy.</p>
</li>
</ol>
<p>In these examples, Nash Equilibrium provides a way of understanding the behavior of multiple players and finding a solution that benefits all parties involved. It helps to identify a stable outcome that will persist, even if the players are aware of each other's strategies.</p>
<p>2.B.2 Minimax and Maximin Minimax and Maximin are concepts used in two-player, zero-sum games to describe the strategies of the players. In a zero-sum game, the total benefit or loss is equal to zero, meaning that one player's gain is another player's loss. In a Minimax strategy, a player minimizes their maximum loss, while in a Maximin strategy, a player maximizes their minimum gain.</p>
<p>The Minimax strategy can be represented mathematically as:</p>
<div class="math">$$\textbf{s}_i^* = \arg\min_{\textbf{s}_i} \left(\max_{\textbf{s}_j} u_i(\textbf{s}_i, \textbf{s}_j)\right) \text{ for player } i$$</div>
<p>The Maximin strategy can be represented mathematically as:</p>
<div class="math">$$\textbf{s}_i^* = \arg\max_{\textbf{s}_i} \left(\min_{\textbf{s}_j} u_i(\textbf{s}_i, \textbf{s}_j)\right) \text{ for player } i$$</div>
<p>Minimax and Maximin strategies are commonly used in decision-making problems, such as in game playing and resource allocation. These strategies can provide a robust and conservative approach to decision-making under uncertainty.</p>
<p>Minimax and Maximin are closely related concepts in game theory that can be used to solve several real-world problems:</p>
<ol>
<li>
<p>Resource Allocation: In many resource allocation problems, decision makers must allocate resources among several competing objectives. The maximin strategy involves selecting the option that maximizes the minimum pay-off across all objectives. For instance, in the allocation of limited funds among several disaster relief programs, a government might choose the program with the highest minimum impact in order to ensure that its resources are being used effectively.</p>
</li>
<li>
<p>Portfolio Optimization: In finance, the maximin strategy can be used to optimize a portfolio of investments. For example, an investor might choose a portfolio that maximizes the minimum expected return across a set of investments in order to minimize risk.</p>
</li>
<li>
<p>Decision Making Under Uncertainty: In decision making problems where there is a great deal of uncertainty, the maximin strategy can be used to make robust decisions. For instance, in a supply chain management problem, a firm might choose the supplier that provides the highest minimum quality level in order to ensure that its supply is secure.</p>
</li>
</ol>
<p>In these examples, the maximin strategy provides a way to make decisions in situations where the outcomes are uncertain. It helps to ensure that decisions are made in a way that minimizes the risk of poor outcomes and maximizes the potential for good ones.</p>
<p>The Minimax strategy is a concept in game theory that can be used to solve several real-world problems:</p>
<ol>
<li>
<p>Competitive Resource Allocation: In many resource allocation problems, decision makers must allocate resources among several competing objectives. The Minimax strategy involves selecting the option that minimizes the maximum pay-off across all objectives. For instance, in the allocation of limited funds among several disaster relief programs, a government might choose the program that minimizes the maximum impact of a failure in order to ensure that its resources are being used effectively.</p>
</li>
<li>
<p>Risk Management: In many decision-making problems, the Minimax strategy can be used to minimize the risk of a poor outcome. For example, in a project management problem, a firm might choose the project with the lowest maximum cost in order to minimize the risk of a project failure.</p>
</li>
<li>
<p>Game Theory: The Minimax strategy is also used in game theory as a way to determine the optimal strategy in two-player games. In a two-player game, each player must choose a strategy that minimizes their maximum potential loss, given the other player's strategy.</p>
</li>
</ol>
<p>In these examples, the Minimax strategy provides a way to make decisions in situations where the outcomes are uncertain. It helps to ensure that decisions are made in a way that minimizes the risk of poor outcomes and balances competing objectives.</p>
<h3>Extensions to Dynamic Games</h3>
<p>In the previous section, we discussed the concept of Nash Equilibrium and Minimax/Maximin strategies in static games. However, in many real-world problems, the environment and pay-offs are not static and can change over time. To address these problems, game theorists have developed two important extensions to static game theory: Stochastic Games and Repeated Games.</p>
<ol>
<li>Stochastic Games: Stochastic Games are dynamic games that involve randomness or uncertainty in pay-offs and/or transition probabilities. In these games, the pay-offs and/or transition probabilities are drawn from some probability distribution. The objective of a player in a Stochastic Game is to choose a strategy that maximizes the expected value of their pay-off.</li>
</ol>
<p>Stochastic Games have been applied in several areas, including finance, economics, and resource allocation. For example, in financial markets, the returns on investments are uncertain, and investors must choose their investment strategies accordingly. Stochastic Games can be used to model these problems and to determine the optimal investment strategies under different market conditions.</p>
<ol>
<li>Repeated Games: Repeated Games are games that are played multiple times between the same players. In these games, players can use the history of previous play to inform their current and future strategies. The objective of a player in a Repeated Game is to choose a strategy that maximizes their long-term pay-off.</li>
</ol>
<p>Repeated Games have been used to model a variety of real-world problems, including pricing and advertising strategies in oligopolistic markets, environmental policies, and international relations. For example, in an oligopolistic market, firms must choose their pricing and advertising strategies, taking into account the strategies of their competitors. The repeated nature of these interactions allows firms to develop long-term strategies that can lead to mutually beneficial outcomes.</p>
<p>In summary, the extensions to dynamic games - Stochastic Games and Repeated Games - provide a framework for modeling and solving problems that involve uncertainty and/or repeated interactions between players. These extensions are particularly useful in modeling problems in finance, economics, and resource allocation, as well as in many other areas.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>


        <div class="tag-cloud">
            <p>
                    <br/><br/>Tags:&nbsp;
                        <a href="https://www.safjan.com/tag/machine-learning/">machine-learning</a>
                        <a href="https://www.safjan.com/tag/reinforcement-learning/">reinforcement-learning</a>
                        <a href="https://www.safjan.com/tag/rl/">rl</a>
                        <a href="https://www.safjan.com/tag/game-theory/">game-theory</a>
            </p>
        </div>


  <div class="neighbors">
  </div>

            <div class="related-posts">
                <h4>You might enjoy</h4>
                <ul class="related-posts">
                        <li><a href="https://www.safjan.com/contextual_understanding-speech-to-text/">Contextual Understanding in Automated Speech-to-Text Transcription: Machine Learning Techniques and Challenges</a></li>
                        <li><a href="https://www.safjan.com/best-alternatives-to-jupyter-notebooks/">Best Alternatives to Jupyter Notebooks</a></li>
                        <li><a href="https://www.safjan.com/how-to-save-numpy-array-to-file/">How to save numpy array to file?</a></li>
                        <li><a href="https://www.safjan.com/illusion-of-expertise/">Rethinking the Link Between Speech and Expertise</a></li>
                        <li><a href="https://www.safjan.com/model-stacking-bagging-ensembling-boosting-5yo/">Model Stacking, Bagging, Ensembling, and Boosting Explained with LEGO metaphor</a></li>
                </ul>
            </div>


    </article>

    <footer>
<p>
  &copy; 2023 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
</main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Krystian Safjan's Blog ",
  "url" : "https://www.safjan.com",
  "image": "/images/profile_new.jpg",
  "description": ""
}
</script>

</body>
</html>