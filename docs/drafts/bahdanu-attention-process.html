
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

        <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/stylesheet/style.min.css">



        <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              href="https://www.safjan.com/theme/pygments/github.min.css">



    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/solid.css">

        <link rel="stylesheet" type="text/css"
              href="https://www.safjan.com/styles/custom.css">

        <link href="https://www.safjan.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Krystian Safjan's Blog Atom">

        <link href="https://www.safjan.com/feeds/all.rss.xml" type="application/rss+xml" rel="alternate"
              title="Krystian Safjan's Blog RSS">


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RM2PKDCCYM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-RM2PKDCCYM');
</script>
<!-- End of Google tag (gtag.js) -->



    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content=""/>
    <meta name="keywords" content="machine-learning, python">
    <meta expr:content="2023-04-18 00:00:00+02:00" itemprop='datePublished'/>
    <meta expr:content="2023-04-18 00:00:00+02:00" itemprop='dateModified'/>
    <meta property="article:modified_time" content="2023-04-18 00:00:00+02:00"/>
    <meta property="article:published_time" content="2023-04-18 00:00:00+02:00"/>
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bahdanu Attention Process",
  "datePublished": "2023-04-18 00:00:00+02:00",
  "dateModified": "2023-04-18 00:00:00+02:00"
}








    </script>



  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Bahdanu Attention Process"/>
  <meta property="og:description" content=""/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://www.safjan.com/drafts/bahdanu-attention-process.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2023-04-18 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2023-04-18 00:00:00+02:00"/>
  <meta property="article:author" content="https://www.safjan.com/author/krystian-safjan/">
  <meta property="article:section" content="Machine Learning"/>
  <meta property="article:tag" content="machine-learning"/>
  <meta property="article:tag" content="python"/>
  <meta property="og:image" content="https://www.safjan.com//images/head/abstract_1.jpg">

    <meta name="twitter:card" content="summary"/>
    <meta property="twitter:image" content="https://www.safjan.com//images/head/abstract_1.jpg">

    <meta name="twitter:label1" content="Est. reading time"/>
    <meta name="twitter:data1" content="4 min."/>
    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:description" content=""/>
    <meta name="twitter:title" content="Bahdanu Attention Process"/>


    <title>    Bahdanu Attention Process
</title>


<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body class="light-theme">
<aside>
    <div>
        <a href="https://www.safjan.com/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="https://www.safjan.com/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist | Researcher | Team Leader<br><br> working at Ernst &amp; Young and writing about <a href="/category/machine-learning.html">Data Science and Visualization</a>, on <a href="/category/machine-learning.html">Machine Learning, Deep Learning</a> and <a href="/tag/nlp/">NLP</a>. There are also some  <a href="/category/howto.html">howto</a> posts on tools and workflows.</p>




        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<style>
    .button {
        background-color: yellow;
        color: black;
        border: none;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 20px;
    }

    .center {
        text-align: center;
    }

    .container {
        display: grid;
        grid-template-columns: 1fr 1fr;
    }

    .left-col {
    }

    .right-col {
    }

    .image {
        border-radius: 0;
        width: 100%;
    }

    .book-list {
        padding-top:0;
        padding-bottom: 0;
        text-align:left;
        box-sizing: border-box;
        display: block;
        list-style-image: url(/images/shortcode-tick.webp);
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start:0;
        margin-inline-end:0;
        padding-inline-start:40px;
    }

    .book-list li {
        font-weight: bold;
    }

    @media (max-width: 600px) {
        .container {
            grid-template-columns: 1fr;
        }
    }

</style>
<div class="container">
    <div class="left-col">
        <a href="https://ksafjanuser.gumroad.com/l/mlops">
            <img src="/images/mlop_interview_book_cover_3D_300px.jpg" class="image" alt="Interview Book Cover">
        </a>

        <div class="center">
            <a href="https://ksafjanuser.gumroad.com/l/mlops">
                <button class="button">Get for $2.99</button>
            </a>
        </div>
    </div>
    <div class="right-col">
        <div>
            <ul class="book-list">
                <li>PDF, ePUB, mobi format ebook, no DRM</li>
                <li>50 questions and answers</li>
                <li>Stories from real projects</li>
                <li>92 multiple choice quiz questions</li>
                <li>80 pages</li>
            </ul>
        </div>
    </div>
</div>
</aside>
<main>

        <nav>
            <a href="https://www.safjan.com/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/tags.html">Tags</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>

                <a href="https://www.safjan.com/feeds/all.atom.xml">Atom</a>

                <a href="https://www.safjan.com/feeds/all.rss.xml">RSS</a>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2023-04-18 


                <br/>
            </p>
            <h1 id="bahdanu-attention-process">Bahdanu Attention Process</h1>
            <div class="header-underline"></div>



        </header>


        <div>
            <p>inspiration: <a href="https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/">Intuitive Introduction to Neural Machine Translation with Bahdanau and Luong Attention</a></p>
<p>Bahdanau Attention is a neural network mechanism used to improve the performance of sequence-to-sequence models in natural language processing tasks. It was introduced by <a href="https://arxiv.org/abs/1409.0473">Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio in 2014</a>, and has since become a widely used method in machine translation, speech recognition, and other sequence-to-sequence applications.</p>
<p>At a high level, Bahdanau Attention allows a model to selectively focus on different parts of the input sequence when producing the output sequence. This is particularly useful when dealing with long input sequences or when the relationship between the input and output sequences is complex.</p>
<p>The attention mechanism works by computing a set of attention weights, which indicate how important each input element is for generating the output sequence. These attention weights are computed by a small neural network that takes as input the current state of the decoder (i.e., the current output sequence generated so far) and a set of encoder outputs (i.e., the hidden states of the encoder that were computed for each input element).</p>
<p>The neural network used to compute the attention weights is often called the attention module or the alignment model. It typically consists of one or more feedforward layers that take as input the concatenation of the decoder state and the encoder output, and produce a scalar score for each input element. These scores are then normalized using the softmax function, which ensures that they sum up to one.</p>
<p>The attention weights are then used to compute a weighted sum of the encoder outputs, where the weights are the attention weights. This weighted sum is then concatenated with the current decoder state and fed through another feedforward layer to produce the next output symbol.</p>
<p>The process of computing the attention weights and the weighted sum is repeated for each output symbol until the end-of-sequence token is generated, at which point the decoder stops.</p>
<p>One of the key benefits of Bahdanau Attention is that it allows the model to selectively focus on different parts of the input sequence at different points in time. This is particularly useful for machine translation, where the input and output sequences can be of vastly different lengths and the relationship between them is complex.</p>
<p>For example, consider the following English sentence and its French translation:</p>
<p>"John loves Mary" -&gt; "Jean aime Marie"</p>
<p>In this case, the model needs to learn to align the English words "John", "loves", and "Mary" with the corresponding French words "Jean", "aime", and "Marie". This alignment is not straightforward, as the word order and sentence structure can be quite different between the two languages.</p>
<p>Bahdanau Attention allows the model to learn this alignment by selectively focusing on different parts of the input sequence at different points in time. For example, when generating the French word "Jean", the model might focus more on the English word "John" and less on the other words, while when generating the French word "aime", the model might focus more on the English word "loves" and less on the other words.</p>
<p>Overall, Bahdanau Attention has proven to be a powerful and flexible mechanism for improving the performance of sequence-to-sequence models in natural language processing and other applications. It has enabled state-of-the-art results in machine translation, speech recognition, and other tasks, and is likely to continue to play an important role in the development of future AI systems.</p>
        </div>




            <div class="bibtex-note">
                <p><b>To cite this article:</b></p>
                <pre>@article{Saf2023Bahdanu,
    author  = {Krystian Safjan},
    title   = {Bahdanu Attention Process},
    journal = {Krystian's Safjan Blog},
    year    = {2023},
}</pre>
            </div>
        <div class="tag-cloud">
            <p>
                    <br/><br/>Tags:&nbsp;
                        <a href="https://www.safjan.com/tag/machine-learning/">machine-learning</a>
                        <a href="https://www.safjan.com/tag/python/">python</a>
            </p>
        </div>








            <div class="related-posts">
                <h4>You might enjoy</h4>
                <ul class="related-posts">
                        <li><a href="https://www.safjan.com/python-dependency-injection-for-the-testability/">Harnessing the Power of Dependency Injection for Improved Testability in Python</a></li>
                        <li><a href="https://www.safjan.com/the-role-and-responsibilities-of-a-forward-deployed-engineer/">The Role and Responsibilities of a Forward Deployed Engineer - Bridging the Gap Between Software Products and Customer Needs</a></li>
                        <li><a href="https://www.safjan.com/similarity-search-using-ivfpq/">Similarity search using IVFPQ</a></li>
                        <li><a href="https://www.safjan.com/the-best-vector-databases-for-storing-embeddings/">The Best Vector Databases for Storing Embeddings</a></li>
                        <li><a href="https://www.safjan.com/attacking-differential-privacy-using-the-correlation-between-the-features/">Attacking Differential Privacy Using the Correlation Between the Features</a></li>
                </ul>
            </div>

  <div class="neighbors">
  </div>


    </article>

    <footer>
<p>
  &copy; 2023 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
</main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Krystian Safjan's Blog ",
  "url" : "https://www.safjan.com",
  "image": "/images/profile_new.jpg",
  "description": ""
}
</script>

</body>
</html>