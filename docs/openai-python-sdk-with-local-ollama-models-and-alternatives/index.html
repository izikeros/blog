
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="http://127.0.0.1:8000/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="http://127.0.0.1:8000/styles/custom.css">








    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="I&#39;ve been diving into how to use the official openai Python package to talk to local Ollama models—and when it makes sense to bring in abstraction layers like LiteLLM. Let me walk you through what I learned. 1. Can I use …"/>
    <meta name="keywords" content="openai, openai-sdk, ollama, litellm, langchain, llama-index, guidance, instructor">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)"/>
  <meta property="og:description" content="I&#39;ve been diving into how to use the official openai Python package to talk to local Ollama models—and when it makes sense to bring in abstraction layers like LiteLLM. Let me walk you through what I learned. 1. Can I use …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="http://127.0.0.1:8000/openai-python-sdk-with-local-ollama-models-and-alternatives/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2025-07-29 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2025-07-29 00:00:00+02:00"/>
  <meta property="article:author" content="http://127.0.0.1:8000/author/krystian-safjan/"/>
  <meta property="article:section" content="note"/>
  <meta property="article:tag" content="openai"/>
  <meta property="article:tag" content="openai-sdk"/>
  <meta property="article:tag" content="ollama"/>
  <meta property="article:tag" content="litellm"/>
  <meta property="article:tag" content="langchain"/>
  <meta property="article:tag" content="llama-index"/>
  <meta property="article:tag" content="guidance"/>
  <meta property="article:tag" content="instructor"/>
  <meta property="og:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:image:alt" content="Krystian Safjan's Blog"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)"/>
    <meta name="twitter:description" content="I've been diving into how to use the official openai Python package to talk to local Ollama models—and when it makes sense to bring in abstraction layers like LiteLLM. Let me walk you through..."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://127.0.0.1:8000/openai-python-sdk-with-local-ollama-models-and-alternatives/"
  },
  "headline": "Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)",
  "datePublished": "2025-07-29T00:00:00+02:00",
  "dateModified": "2025-07-29T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "http://127.0.0.1:8000/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "/images/profile_new.jpg",  "url": "http://127.0.0.1:8000/openai-python-sdk-with-local-ollama-models-and-alternatives/",
  "description": "I've been diving into how to use the official openai Python package to talk to local Ollama models—and when it makes sense to bring in abstraction layers..."
}
</script>

    <title>    Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="http://127.0.0.1:8000/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="http://127.0.0.1:8000/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist | Researcher | Team Leader<br><br> working at Ernst &amp; Young and writing about <a href="/category/machine-learning.html">Data Science and Visualization</a>, on <a href="/category/machine-learning.html">Machine Learning, Deep Learning</a> and <a href="/tag/nlp/">NLP</a>. There are also some <a href="/category/howto.html">howto</a> posts on tools and workflows.</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A • PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav>
            <a href="http://127.0.0.1:8000/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>



            <div id="search" class="nav-search"></div>
            <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2025-07-29 


                <br/>
            </p>
            <h1 id="openai-python-sdk-with-local-ollama-models-and-alternatives">Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)</h1>
            <div class="header-underline"></div>



        </header>



        <details class="toc-details" id="toc-container">
            <summary>Table of Contents</summary>
            <nav class="toc" aria-label="Table of Contents">
                <ul class="toc-list"></ul>
            </nav>
        </details>

        <div class="article-content">
            <p>I've been diving into how to use the official <code>openai</code> Python package to <strong>talk to local Ollama models</strong>—and when it makes sense to bring in abstraction layers like <strong>LiteLLM</strong>. Let me walk you through what I learned.</p>
<h3 id="1-can-i-use-the-openai-python-package-for-local-ollama-models">1. Can I use the OpenAI Python package for local Ollama models?</h3>
<p><strong>Yes!</strong> Since early February 2024, Ollama supports the <strong>OpenAI Chat Completions API</strong>, exposing compatible endpoints locally. You can simply point the OpenAI client at <code>"http://localhost:11434/v1"</code>, pass a dummy API key, and call completions just like you would to OpenAI’s hosted API (see <a href="https://ollama.com/blog/openai-compatibility?utm_source=chatgpt.com" title="OpenAI compatibility · Ollama Blog">Ollama blog</a>).</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:11434/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;unused-key&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama2&quot;</span><span class="p">,</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What’s the capital of France?&quot;</span><span class="p">}</span>
  <span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div>

<p>You can also do embeddings similarly:</p>
<div class="highlight"><pre><span></span><code><span class="n">resp</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama2&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;Hello world!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
</code></pre></div>

<p>So for fairly simple local projects, the OpenAI SDK works perfectly with Ollama.</p>
<h3 id="2-when-should-i-use-litellm-instead">2. When should I use LiteLLM instead?</h3>
<p><strong>LiteLLM</strong> is a lightweight Python SDK (and optional proxy server) that provides a <strong>unified API</strong> for over 100 LLM providers—including OpenAI, Anthropic, HuggingFace—and crucially, <strong>Ollama/local models</strong> ( nice example with minimal Flask app - poem generator <a href="https://notes.kodekloud.com/docs/Running-Local-LLMs-With-Ollama/Building-AI-Applications/Demo-Creating-an-App-Using-Ollama-OpenAI-Python-Client?utm_source=chatgpt.com" title="Demo Creating an App Using Ollama OpenAI Python Client">KodeKloud Notes</a>).</p>
<p>Here are some benefits of using LiteLLM:
- It standardizes completions, embeddings, streaming, retries, and fallback logic<br>
- You can swap providers (e.g. <code>openai/gpt‑4</code>, <code>anthropic/claude</code>, <code>ollama/llama2</code>) with no code changes<br>
- Proxy server mode offers observability, logging, rate limiting, and cost tracking across providers (see LiteLLM documentation: <a href="https://docs.litellm.ai/?utm_source=chatgpt.com" title="LiteLLM - Getting Started | liteLLM">LiteLLM</a>)</p>
<h3 id="3-example-using-litellm-python-sdk">3. Example: using LiteLLM Python SDK</h3>
<p>First install:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>litellm
</code></pre></div>

<p>Then in Python:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">litellm</span> <span class="kn">import</span> <span class="n">completion</span><span class="p">,</span> <span class="n">embeddings</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;dummy&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LITELLM_OLLAMA_BASE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:11434/v1&quot;</span>

<span class="c1"># Completion via Ollama</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;ollama/llama2&quot;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Hello!&quot;</span><span class="p">}])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resp</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>

<span class="c1"># Embeddings via Ollama</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;ollama/llama2&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">emb</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;embedding&quot;</span><span class="p">])</span>
</code></pre></div>

<p>Later you can just switch to <code>openai/gpt-4o</code> or another provider. You keep the same <code>completion(...)</code> call. No branching logic in your app (<a href="https://langfuse.com/integrations/frameworks/litellm-sdk?utm_source=chatgpt.com" title="Open Source Observability for LiteLLM SDK - Langfuse">Langfuse</a>, <a href="https://docs.litellm.ai/?utm_source=chatgpt.com" title="LiteLLM - Getting Started | liteLLM">LiteLLM</a>, <a href="https://docs.litellm.ai/docs/?utm_source=chatgpt.com" title="LiteLLM - Getting Started">LiteLLM</a>).</p>
<h3 id="5-alternatives-to-litellm">5. Alternatives to LiteLLM</h3>
<p>There are several other frameworks you may consider:</p>
<ul>
<li>
<p><strong>Langchain</strong>, <strong>Llama‑Index</strong>, <strong>Guidance</strong>, <strong>instructor</strong> – great for structured output, chaining, tool-use, agents, prompt templating. Read more in these sources:</p>
<ul>
<li><a href="https://medium.com/towardsdev/4-proven-ways-to-use-ollama-locally-openai-apis-in-python-fast-flexible-and-scalable-216dca893b1c">4 Proven Ways to Use Ollama Locally &amp; OpenAI APIs in Python: Fast, Flexible, and Scalable | by Brain Glitch | Jun, 2025 | Towards Dev</a></li>
<li><a href="https://medium.com/%40hajraali730/unlocking-the-power-of-litellm-a-lightweight-unified-interface-for-llms-5dc09cece265">Unlocking the Power of LiteLLM: A Lightweight, Unified Interface for LLMs</a></li>
<li><a href="https://www.truefoundry.com/blog/litellm-alternatives">Top 5 LiteLLM alternatives of 2025</a></li>
<li><a href="https://simmering.dev/blog/structured_output/">The best library for structured LLM output – Paul Simmering</a> - gives different recommendations for four use cases.</li>
</ul>
</li>
<li>
<p><strong>TrueFoundry</strong> – a more enterprise‑ready orchestration layer with observability, scaling, and deployment support, but heavier than LiteLLM (<a href="https://www.truefoundry.com/blog/litellm-alternatives?utm_source=chatgpt.com" title="Top 5 LiteLLM alternatives of 2025 - TrueFoundry">truefoundry.com</a>)</p>
</li>
</ul>
<h3 id="summary">Summary</h3>
<p>I like using the <strong>OpenAI Python SDK</strong> with Ollama—it’s quick, reliable, and simple for local use cases. But as soon as I need to add other providers, handle retries/fallbacks, use embeddings, or manage observability and switching logic, <strong>LiteLLM</strong> becomes more convenient. And if I’m building complex agent pipelines or need structure, then libraries like <strong>Langchain</strong> or <strong>TrueFoundry</strong> fit right in.</p>
        </div>


        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="http://127.0.0.1:8000/tag/openai/" class="article-tag">openai</a>
                <a href="http://127.0.0.1:8000/tag/openai-sdk/" class="article-tag">openai-sdk</a>
                <a href="http://127.0.0.1:8000/tag/ollama/" class="article-tag">ollama</a>
                <a href="http://127.0.0.1:8000/tag/litellm/" class="article-tag">litellm</a>
                <a href="http://127.0.0.1:8000/tag/langchain/" class="article-tag">langchain</a>
                <a href="http://127.0.0.1:8000/tag/llama-index/" class="article-tag">llama-index</a>
                <a href="http://127.0.0.1:8000/tag/guidance/" class="article-tag">guidance</a>
                <a href="http://127.0.0.1:8000/tag/instructor/" class="article-tag">instructor</a>
        </div>









    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "http://127.0.0.1:8000",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="http://127.0.0.1:8000/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false,
            basePath: "http://127.0.0.1:8000/pagefind/"
        });
    });
</script>

<script src="http://127.0.0.1:8000/theme/js/theme-switcher.js"></script>
</body>
</html>