
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="https://www.safjan.com/styles/custom.css">

        <link href="https://www.safjan.com/feeds/all.atom.xml?utm_source=rss&utm_medium=feed&utm_campaign=rss-feed" type="application/atom+xml" rel="alternate"
              title="Krystian Safjan's Blog Atom">

        <link href="https://www.safjan.com/feeds/all.rss.xml?utm_source=rss&utm_medium=feed&utm_campaign=rss-feed" type="application/rss+xml" rel="alternate"
              title="Krystian Safjan's Blog RSS">


    <link rel="apple-touch-icon" sizes="180x180" href="https://www.safjan.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://www.safjan.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://www.safjan.com/favicon-16x16.png">
    <link rel="shortcut icon" href="https://www.safjan.com/favicon.ico">
    <link rel="manifest" href="https://www.safjan.com/site.webmanifest">
    <meta name="theme-color" content="#333333">
    <meta name="apple-mobile-web-app-title" content="Krystian Safjan's Blog">

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RM2PKDCCYM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-RM2PKDCCYM');
</script>
<!-- End of Google tag (gtag.js) -->



    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="Learn how to use the official openai Python package with local Ollama models and when it&#39;s better to opt for LiteLLM as a more unified alternative."/>
    <meta name="keywords" content="openai, openai-sdk, ollama, litellm, langchain, llama-index, guidance, instructor">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)"/>
  <meta property="og:description" content="Learn how to use the official openai Python package with local Ollama models and when it&#39;s better to opt for LiteLLM as a more unified alternative."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://www.safjan.com/openai-python-sdk-with-local-ollama-models-and-alternatives/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2025-07-29 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2025-07-29 00:00:00+02:00"/>
  <meta property="article:author" content="https://www.safjan.com/author/krystian-safjan/"/>
  <meta property="article:section" content="note"/>
  <meta property="article:tag" content="openai"/>
  <meta property="article:tag" content="openai-sdk"/>
  <meta property="article:tag" content="ollama"/>
  <meta property="article:tag" content="litellm"/>
  <meta property="article:tag" content="langchain"/>
  <meta property="article:tag" content="llama-index"/>
  <meta property="article:tag" content="guidance"/>
  <meta property="article:tag" content="instructor"/>
  <meta property="og:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:image:alt" content="Krystian Safjan's Blog"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)"/>
    <meta name="twitter:description" content="Learn how to use the official openai Python package with local Ollama models and when it&#39;s better to opt for LiteLLM as a more unified alternative."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.safjan.com/openai-python-sdk-with-local-ollama-models-and-alternatives/"
  },
  "headline": "Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)",
  "datePublished": "2025-07-29T00:00:00+02:00",
  "dateModified": "2025-07-29T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "https://www.safjan.com/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "/images/profile_new.jpg",  "url": "https://www.safjan.com/openai-python-sdk-with-local-ollama-models-and-alternatives/",
  "description": "Learn how to use the official openai Python package with local Ollama models and when it's better to opt for LiteLLM as a more unified alternative.",
  "articleSection": "note",
  "inLanguage": "en",
  "keywords": "openai, openai-sdk, ollama, litellm, langchain, llama-index, guidance, instructor",
  "wordCount": 485,
  "speakable": {
    "@type": "SpeakableSpecification",
    "cssSelector": ["article h1", ".summary", ".tldr", "article \u003e p:first-of-type"]
  }}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://www.safjan.com/"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "note",
      "item": "https://www.safjan.com/category/note.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "Using OpenAI Python SDK with Local Ollama..."
    }
  ]
}
</script>



<meta name="ai:summary" content="Learn how to use the official openai Python package with local Ollama models and when it&#39;s better to opt for LiteLLM as a more unified alternative.">

<meta name="ai:topics" content="openai, openai-sdk, ollama, litellm, langchain, llama-index, guidance, instructor">



<meta name="ai:word-count" content="485">
<meta name="ai:reading-time" content="2 min">

<meta name="ai:section" content="note">



<meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">

    <title>    Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="https://www.safjan.com/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="https://www.safjan.com/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist and Team Leader writing about Machine Learning, MLOps, and Python</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A • PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav aria-label="Main navigation">
            <a href="https://www.safjan.com/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>

                <a href="https://www.safjan.com/feeds/all.atom.xml">Atom</a>

                <a href="https://www.safjan.com/feeds/all.rss.xml">RSS</a>

            <div id="search" class="nav-search"></div>
            <button type="button" id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2025-07-29 


                <br/>
            </p>
            <h1>Using OpenAI Python SDK with Local Ollama Models (and When to Opt for Alternatives)</h1>
            <div class="header-underline"></div>



        </header>




        <div class="article-content">
            <p>I've been diving into how to use the official <code>openai</code> Python package to <strong>talk to local Ollama models</strong>—and when it makes sense to bring in abstraction layers like <strong>LiteLLM</strong>. Let me walk you through what I learned.</p>
<h3 id="1-can-i-use-the-openai-python-package-for-local-ollama-models">1. Can I use the OpenAI Python package for local Ollama models?</h3>
<p><strong>Yes!</strong> Since early February 2024, Ollama supports the <strong>OpenAI Chat Completions API</strong>, exposing compatible endpoints locally. You can simply point the OpenAI client at <code>"http://localhost:11434/v1"</code>, pass a dummy API key, and call completions just like you would to OpenAI’s hosted API (see <a href="https://ollama.com/blog/openai-compatibility?utm_source=chatgpt.com" title="OpenAI compatibility · Ollama Blog">Ollama blog</a>).</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:11434/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;unused-key&quot;</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama2&quot;</span><span class="p">,</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What’s the capital of France?&quot;</span><span class="p">}</span>
  <span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div>

<p>You can also do embeddings similarly:</p>
<div class="highlight"><pre><span></span><code><span class="n">resp</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama2&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;Hello world!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resp</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
</code></pre></div>

<p>So for fairly simple local projects, the OpenAI SDK works perfectly with Ollama.</p>
<h3 id="2-when-should-i-use-litellm-instead">2. When should I use LiteLLM instead?</h3>
<p><strong>LiteLLM</strong> is a lightweight Python SDK (and optional proxy server) that provides a <strong>unified API</strong> for over 100 LLM providers—including OpenAI, Anthropic, HuggingFace—and crucially, <strong>Ollama/local models</strong> ( nice example with minimal Flask app - poem generator <a href="https://notes.kodekloud.com/docs/Running-Local-LLMs-With-Ollama/Building-AI-Applications/Demo-Creating-an-App-Using-Ollama-OpenAI-Python-Client?utm_source=chatgpt.com" title="Demo Creating an App Using Ollama OpenAI Python Client">KodeKloud Notes</a>).</p>
<p>Here are some benefits of using LiteLLM:
- It standardizes completions, embeddings, streaming, retries, and fallback logic<br>
- You can swap providers (e.g. <code>openai/gpt‑4</code>, <code>anthropic/claude</code>, <code>ollama/llama2</code>) with no code changes<br>
- Proxy server mode offers observability, logging, rate limiting, and cost tracking across providers (see LiteLLM documentation: <a href="https://docs.litellm.ai/?utm_source=chatgpt.com" title="LiteLLM - Getting Started | liteLLM">LiteLLM</a>)</p>
<h3 id="3-example-using-litellm-python-sdk">3. Example: using LiteLLM Python SDK</h3>
<p>First install:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>litellm
</code></pre></div>

<p>Then in Python:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">litellm</span> <span class="kn">import</span> <span class="n">completion</span><span class="p">,</span> <span class="n">embeddings</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;dummy&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LITELLM_OLLAMA_BASE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:11434/v1&quot;</span>

<span class="c1"># Completion via Ollama</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">completion</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;ollama/llama2&quot;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Hello!&quot;</span><span class="p">}])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resp</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">])</span>

<span class="c1"># Embeddings via Ollama</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;ollama/llama2&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">emb</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;embedding&quot;</span><span class="p">])</span>
</code></pre></div>

<p>Later you can just switch to <code>openai/gpt-4o</code> or another provider. You keep the same <code>completion(...)</code> call. No branching logic in your app (<a href="https://langfuse.com/integrations/frameworks/litellm-sdk?utm_source=chatgpt.com" title="Open Source Observability for LiteLLM SDK - Langfuse">Langfuse</a>, <a href="https://docs.litellm.ai/?utm_source=chatgpt.com" title="LiteLLM - Getting Started | liteLLM">LiteLLM</a>, <a href="https://docs.litellm.ai/docs/?utm_source=chatgpt.com" title="LiteLLM - Getting Started">LiteLLM</a>).</p>
<h3 id="5-alternatives-to-litellm">5. Alternatives to LiteLLM</h3>
<p>There are several other frameworks you may consider:</p>
<ul>
<li>
<p><strong>Langchain</strong>, <strong>Llama‑Index</strong>, <strong>Guidance</strong>, <strong>instructor</strong> – great for structured output, chaining, tool-use, agents, prompt templating. Read more in these sources:</p>
<ul>
<li><a href="https://medium.com/towardsdev/4-proven-ways-to-use-ollama-locally-openai-apis-in-python-fast-flexible-and-scalable-216dca893b1c">4 Proven Ways to Use Ollama Locally &amp; OpenAI APIs in Python: Fast, Flexible, and Scalable | by Brain Glitch | Jun, 2025 | Towards Dev</a></li>
<li><a href="https://medium.com/%40hajraali730/unlocking-the-power-of-litellm-a-lightweight-unified-interface-for-llms-5dc09cece265">Unlocking the Power of LiteLLM: A Lightweight, Unified Interface for LLMs</a></li>
<li><a href="https://www.truefoundry.com/blog/litellm-alternatives">Top 5 LiteLLM alternatives of 2025</a></li>
<li><a href="https://simmering.dev/blog/structured_output/">The best library for structured LLM output – Paul Simmering</a> - gives different recommendations for four use cases.</li>
</ul>
</li>
<li>
<p><strong>TrueFoundry</strong> – a more enterprise‑ready orchestration layer with observability, scaling, and deployment support, but heavier than LiteLLM (<a href="https://www.truefoundry.com/blog/litellm-alternatives?utm_source=chatgpt.com" title="Top 5 LiteLLM alternatives of 2025 - TrueFoundry">truefoundry.com</a>)</p>
</li>
</ul>
<h3 id="summary">Summary</h3>
<p>I like using the <strong>OpenAI Python SDK</strong> with Ollama—it’s quick, reliable, and simple for local use cases. But as soon as I need to add other providers, handle retries/fallbacks, use embeddings, or manage observability and switching logic, <strong>LiteLLM</strong> becomes more convenient. And if I’m building complex agent pipelines or need structure, then libraries like <strong>Langchain</strong> or <strong>TrueFoundry</strong> fit right in.</p>
        </div>


        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="https://www.safjan.com/tag/openai/" class="article-tag">openai</a>
                <a href="https://www.safjan.com/tag/openai-sdk/" class="article-tag">openai-sdk</a>
                <a href="https://www.safjan.com/tag/ollama/" class="article-tag">ollama</a>
                <a href="https://www.safjan.com/tag/litellm/" class="article-tag">litellm</a>
                <a href="https://www.safjan.com/tag/langchain/" class="article-tag">langchain</a>
                <a href="https://www.safjan.com/tag/llama-index/" class="article-tag">llama-index</a>
                <a href="https://www.safjan.com/tag/guidance/" class="article-tag">guidance</a>
                <a href="https://www.safjan.com/tag/instructor/" class="article-tag">instructor</a>
        </div>







            <div class="related-posts">
                <h4 class="related-posts-title">You might also like</h4>
                <div class="related-posts-grid">
                        <a href="https://www.safjan.com/problems-with-Langchain-and-how-to-minimize-their-impact/" class="related-post-card">
                            <span class="related-post-title">Problems with Langchain and how to minimize their impact</span>
                            <span class="related-post-date">Sep 01, 2023</span>
                        </a>
                        <a href="https://www.safjan.com/azure-openai-langchain-configuration/" class="related-post-card">
                            <span class="related-post-title">Azure OpenAI Langchain configuration</span>
                            <span class="related-post-date">Aug 02, 2023</span>
                        </a>
                        <a href="https://www.safjan.com/creating-a-powerpoint-presentation-with-a-language-model/" class="related-post-card">
                            <span class="related-post-title">Creating a PowerPoint Presentation with a Language Model</span>
                            <span class="related-post-date">Jul 17, 2023</span>
                        </a>
                        <a href="https://www.safjan.com/token-split-text/" class="related-post-card">
                            <span class="related-post-title">Introducing a Python Module for Splitting Text Into Parts Based on Token Limit</span>
                            <span class="related-post-date">Jul 03, 2023</span>
                        </a>
                </div>
            </div>




    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "https://www.safjan.com",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false,
            basePath: "/pagefind/"
        });
    });
</script>

<script src="https://www.safjan.com/theme/js/theme-switcher.js"></script>

<style>
.video-embed {
    position: relative;
    width: 100%;
    padding-bottom: 56.25%;
    background: #000;
    border-radius: 0.375rem;
    overflow: hidden;
}
.video-embed iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border: 0;
}
.video-embed-wrapper {
    margin-bottom: 1rem;
}
.video-embed-info {
    padding: 0.5rem 0.75rem;
    background: var(--color-bg-secondary, #f6f8fa);
    border-radius: 0 0 0.375rem 0.375rem;
    font-size: 0.875rem;
    margin-top: -0.375rem;
}
.video-embed-info a {
    text-decoration: none;
    color: inherit;
}
.video-embed-info a:hover {
    text-decoration: underline;
}
</style>
<script>
document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('pre > code').forEach(function(code) {
        var text = code.textContent.trim();
        var lines = text.split('\n');
        var url = lines[0].trim();

        var videoId = null;
        var embedUrl = null;
        var platform = null;

        // YouTube detection
        var ytMatch = url.match(/(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})/);
        if (ytMatch) {
            videoId = ytMatch[1];
            embedUrl = 'https://www.youtube-nocookie.com/embed/' + videoId;
            platform = 'youtube';
        }

        // Vimeo detection
        var vimeoMatch = url.match(/vimeo\.com\/(?:video\/)?(\d+)/);
        if (vimeoMatch) {
            videoId = vimeoMatch[1];
            embedUrl = 'https://player.vimeo.com/video/' + videoId + '?dnt=1';
            platform = 'vimeo';
        }

        if (!videoId || !embedUrl) return;

        var meta = {};
        lines.slice(1).forEach(function(line) {
            var m = line.match(/^(\w+):\s*(.+)$/);
            if (m) meta[m[1].toLowerCase()] = m[2].trim();
        });

        var wrapper = document.createElement('div');
        wrapper.className = 'video-embed-wrapper';

        var embed = document.createElement('div');
        embed.className = 'video-embed';
        embed.innerHTML = '<iframe src="' + embedUrl +
            '" allowfullscreen loading="lazy" title="' + (meta.title || (platform === 'youtube' ? 'YouTube video' : 'Vimeo video')).replace(/"/g, '&quot;') + '"></iframe>';
        wrapper.appendChild(embed);

        if (meta.title || meta.author) {
            var info = document.createElement('div');
            info.className = 'video-embed-info';
            var html = meta.title ? '<strong>' + meta.title + '</strong>' : '';
            if (meta.author) {
                html += (meta.title ? ' &bull; ' : '');
                html += meta.authorurl ? '<a href="' + meta.authorurl + '" target="_blank" rel="noopener">' + meta.author + '</a>' : meta.author;
            }
            info.innerHTML = html;
            wrapper.appendChild(info);
        }
        code.closest('pre').replaceWith(wrapper);
    });
});
</script>

<style>
.tweet-embed-wrapper {
    margin-bottom: 1rem;
}
.tweet-embed-loading {
    padding: 1rem;
    background: var(--color-bg-secondary, #f6f8fa);
    border-radius: 0.375rem;
    color: var(--color-text-secondary, #6c757d);
    font-size: 0.875rem;
}
</style>
<script>
document.addEventListener('DOMContentLoaded', function() {
    var twitterScriptLoaded = false;

    function loadTwitterScript() {
        if (twitterScriptLoaded) return;
        twitterScriptLoaded = true;
        var script = document.createElement('script');
        script.src = 'https://platform.twitter.com/widgets.js';
        script.async = true;
        document.body.appendChild(script);
    }

    document.querySelectorAll('pre > code.language-tweet').forEach(function(code) {
        var text = code.textContent.trim();
        var url = text.split('\n')[0].trim();

        // Twitter/X URL detection
        var tweetMatch = url.match(/^https?:\/\/(twitter\.com|x\.com)\/\w+\/status\/(\d+)/);
        if (!tweetMatch) return;

        var wrapper = document.createElement('div');
        wrapper.className = 'tweet-embed-wrapper';
        wrapper.innerHTML = '<div class="tweet-embed-loading">Loading tweet...</div>';

        code.closest('pre').replaceWith(wrapper);

        // Fetch oEmbed data
        var oembedUrl = 'https://publish.twitter.com/oembed?url=' + encodeURIComponent(url) + '&dnt=true&omit_script=true';

        fetch(oembedUrl)
            .then(function(response) { return response.json(); })
            .then(function(data) {
                wrapper.innerHTML = data.html;
                loadTwitterScript();
                if (window.twttr && window.twttr.widgets) {
                    window.twttr.widgets.load(wrapper);
                }
            })
            .catch(function() {
                wrapper.innerHTML = '<div class="tweet-embed-loading">Failed to load tweet. <a href="' + url + '" target="_blank" rel="noopener">View on Twitter</a></div>';
            });
    });
});
</script>
</body>
</html>