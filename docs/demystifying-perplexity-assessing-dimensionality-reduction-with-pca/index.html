
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="http://127.0.0.1:8000/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="http://127.0.0.1:8000/styles/custom.css">








    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="Intrinsic vs. Extrinsic Evaluation - What&#39;s the Best Way to Measure Embedding Quality? Perplexity is a measure commonly used in machine learning, particularly in the field of dimensionality reduction techniques such as Principal Component Analysis (PCA). It provides a way to evaluate and ‚Ä¶"/>
    <meta name="keywords" content="pca, perplexity, dimensionality-reduction, probability, TSNE, distance">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Demystifying Perplexity - Assessing Dimensionality Reduction With PCA"/>
  <meta property="og:description" content="Intrinsic vs. Extrinsic Evaluation - What&#39;s the Best Way to Measure Embedding Quality? Perplexity is a measure commonly used in machine learning, particularly in the field of dimensionality reduction techniques such as Principal Component Analysis (PCA). It provides a way to evaluate and ‚Ä¶"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="http://127.0.0.1:8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2023-06-30 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2023-07-12 00:00:00+02:00"/>
  <meta property="article:author" content="http://127.0.0.1:8000/author/krystian-safjan/"/>
  <meta property="article:section" content="note"/>
  <meta property="article:tag" content="pca"/>
  <meta property="article:tag" content="perplexity"/>
  <meta property="article:tag" content="dimensionality-reduction"/>
  <meta property="article:tag" content="probability"/>
  <meta property="article:tag" content="TSNE"/>
  <meta property="article:tag" content="distance"/>
  <meta property="og:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:image:alt" content="Krystian Safjan's Blog"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="Demystifying Perplexity - Assessing Dimensionality Reduction With PCA"/>
    <meta name="twitter:description" content="Intrinsic vs. Extrinsic Evaluation - What's the Best Way to Measure Embedding Quality? Perplexity is a measure commonly used in machine learning, particularly in the field of dimensionality..."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://127.0.0.1:8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/"
  },
  "headline": "Demystifying Perplexity - Assessing Dimensionality Reduction With PCA",
  "datePublished": "2023-06-30T00:00:00+02:00",
  "dateModified": "2023-07-12T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "http://127.0.0.1:8000/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "/images/profile_new.jpg",  "url": "http://127.0.0.1:8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/",
  "description": "Intrinsic vs. Extrinsic Evaluation - What's the Best Way to Measure Embedding Quality? Perplexity is a measure commonly used in machine learning,..."
}
</script>

    <title>    Demystifying Perplexity - Assessing Dimensionality Reduction With PCA
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="http://127.0.0.1:8000/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="http://127.0.0.1:8000/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist | Researcher | Team Leader<br><br> working at Ernst &amp; Young and writing about <a href="/category/machine-learning.html">Data Science and Visualization</a>, on <a href="/category/machine-learning.html">Machine Learning, Deep Learning</a> and <a href="/tag/nlp/">NLP</a>. There are also some <a href="/category/howto.html">howto</a> posts on tools and workflows.</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A ‚Ä¢ PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav>
            <a href="http://127.0.0.1:8000/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>



            <div id="search" class="nav-search"></div>
            <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2023-06-30 


                    <span class="share-icons">
                        <a href="https://twitter.com/intent/tweet?text=Demystifying%20Perplexity%20-%20Assessing%20Dimensionality%20Reduction%20With%20PCA&url=http%3A//127.0.0.1%3A8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/&via=izikeros&hashtags=pca,perplexity,dimensionality-reduction,probability,tsne,distance" target="_blank" title="Share on Twitter" class="share-icon"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A//127.0.0.1%3A8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/" target="_blank" title="Share on Facebook" class="share-icon"><i class="fab fa-facebook"></i></a>
                        <a href="https://news.ycombinator.com/submitlink?t=Demystifying%20Perplexity%20-%20Assessing%20Dimensionality%20Reduction%20With%20PCA&u=http%3A//127.0.0.1%3A8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/" target="_blank" title="Share on Hacker News" class="share-icon"><i class="fab fa-hacker-news"></i></a>
                        <a href="https://www.reddit.com/submit?url=http%3A//127.0.0.1%3A8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/&title=Demystifying%20Perplexity%20-%20Assessing%20Dimensionality%20Reduction%20With%20PCA" target="_blank" title="Share on Reddit" class="share-icon"><i class="fab fa-reddit"></i></a>
                    </span>
                <br/>
            </p>
            <h1 id="demystifying-perplexity-assessing-dimensionality-reduction-with-pca">Demystifying Perplexity - Assessing Dimensionality Reduction With PCA</h1>
            <div class="header-underline"></div>



        </header>



        <details class="toc-details" id="toc-container">
            <summary>Table of Contents</summary>
            <nav class="toc" aria-label="Table of Contents">
                <ul class="toc-list"></ul>
            </nav>
        </details>

        <div class="article-content">
            <p><a href="http://127.0.0.1:8000/measure-quality-of-embeddings-intrinsic-vs-extrinsic/">Intrinsic vs. Extrinsic Evaluation - What's the Best Way to Measure Embedding Quality?</a></p>
<p>Perplexity is a measure commonly used in machine learning, particularly in the field of dimensionality reduction techniques such as Principal Component Analysis (PCA). It provides a way to evaluate and compare the quality of dimensionality reduction algorithms by quantifying how well they preserve the structure of the original data.</p>
<p>In this blog post, we will look into the concept of perplexity, its application in PCA, and its importance in assessing the performance of dimensionality reduction methods. We will also provide code examples in Python to demonstrate its practical implementation.</p>
<h2 id="understanding-perplexity">Understanding Perplexity</h2>
<p>Perplexity is a measure originally developed for evaluating probabilistic models, particularly in the field of natural language processing. It represents the level of uncertainty or confusion in predicting the next item in a sequence. In the context of dimensionality reduction, perplexity provides an estimation of the number of nearest neighbors that should be considered when reconstructing a data point in a lower-dimensional space.</p>
<p>Given a high-dimensional dataset, PCA aims to find a lower-dimensional representation that captures the most significant features or patterns of the original data. The idea behind perplexity is to preserve the local structure of the data, ensuring that neighboring points in the high-dimensional space remain close to each other in the reduced space.</p>
<h2 id="perplexity-in-pca">Perplexity in PCA</h2>
<p>To understand how perplexity is used in PCA, let's consider a high-dimensional dataset with ùëÅ data points. PCA involves projecting this dataset onto a lower-dimensional space while retaining the maximum amount of variance. The reduced dataset can be represented by ùëÄ principal components, where ùëÄ &lt; ùëÅ.</p>
<p>In PCA, the perplexity of a data point ùë•ùëñ is calculated based on the conditional probability distribution of its neighbors given a particular variance or similarity scale. This distribution can be modeled using a Gaussian kernel centered at ùë•ùëñ:</p>
<div class="math">$$
P(\mathbf{y}_j|\mathbf{x}_i) = \frac{{\exp\left(-\frac{{\|\mathbf{y}_j - \mathbf{x}_i\|^2}}{{2\sigma_i^2}}\right)}}{{\sum_{k\neq j}\exp\left(-\frac{{\|\mathbf{y}_k - \mathbf{x}_i\|^2}}{{2\sigma_i^2}}\right)}}
$$</div>
<p>Here, ùëÉ(ùë¶ùëó|ùë•ùëñ) represents the conditional probability of observing data point ùë¶ùëó as a neighbor of ùë•ùëñ in the lower-dimensional space. The variance or similarity scale ùúéùëñ determines the spread of the Gaussian kernel for each data point ùë•ùëñ.</p>
<p>The perplexity of ùë•ùëñ, denoted as ùëÉùëñ, is then defined as the Shannon entropy of the conditional distribution:</p>
<div class="math">$$
P_i = 2^{-\sum_j P(\mathbf{y}_j|\mathbf{x}_i)\log_2 P(\mathbf{y}_j|\mathbf{x}_i)}
$$</div>
<p>In practice, finding the optimal variance scale ùúéùëñ that results in the desired perplexity can be challenging. One common approach is to perform a binary search to find the value of ùúéùëñ that achieves a target perplexity value. The binary search is performed by iteratively adjusting the value of ùúéùëñ until the entropy of the conditional distribution matches the target perplexity.</p>
<h2 id="evaluating-dimensionality-reduction-with-perplexity">Evaluating Dimensionality Reduction with Perplexity</h2>
<p>Perplexity is a crucial metric for evaluating the performance of dimensionality reduction techniques, including PCA. By preserving the local structure of the data, a good dimensionality reduction method should ensure that neighboring points remain close to each other in the lower-dimensional space.</p>
<p>To evaluate the effectiveness of a dimensionality reduction algorithm, we can compare the perplexity of the original high-dimensional data with the perplexity of the reduced data. If the perplexity remains similar after dimensionality reduction, it suggests that the algorithm successfully preserves the local structure of the data.</p>
<p>In practice, perplexity is often used in conjunction with other evaluation metrics, such as visualization techniques like t-SNE (t-Distributed Stochastic Neighbor Embedding). t-SNE is a nonlinear dimensionality reduction method that can be used to visualize high-dimensional data in two or three dimensions while preserving the local structure. By comparing the perplexity of t-SNE embeddings with the perplexity of the original data, we can gain insights into the quality of the dimensionality reduction.</p>
<h2 id="implementation-in-python">Implementation in Python</h2>
<p>Let's now demonstrate the calculation of perplexity and its application in evaluating dimensionality reduction using PCA in Python. We will use the scikit-learn library, which provides a simple and efficient implementation of PCA and other machine learning algorithms.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">squareform</span>

<span class="k">def</span> <span class="nf">perplexity</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">perplexity_value</span><span class="p">):</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
    <span class="n">sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">beta_min</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">beta_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">affinities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">distances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">betas</span><span class="p">)</span>
            <span class="n">sum_affinities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">affinities</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">affinities</span> <span class="o">/</span> <span class="n">sum_affinities</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">affinities</span> <span class="o">/</span> <span class="n">sum_affinities</span><span class="p">))</span>
            <span class="n">perplexity_diff</span> <span class="o">=</span> <span class="n">entropy</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">perplexity_value</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">perplexity_diff</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="k">if</span> <span class="n">perplexity_diff</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">beta_min</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">beta_max</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="ow">or</span> <span class="n">beta_max</span> <span class="o">==</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
                    <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">beta_max</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">beta_min</span> <span class="o">==</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="ow">or</span> <span class="n">beta_min</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
                    <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">affinities</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">affinities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">P</span>

<span class="c1"># Generate random high-dimensional data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Apply PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_reduced</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Calculate perplexity of original data</span>
<span class="n">original_perplexity</span> <span class="o">=</span> <span class="n">perplexity</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">perplexity_value</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># Calculate perplexity of reduced data</span>
<span class="n">reduced_perplexity</span> <span class="o">=</span> <span class="n">perplexity</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">,</span> <span class="n">perplexity_value</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Perplexity of original data:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">original_perplexity</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Perplexity of reduced data:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">reduced_perplexity</span><span class="p">))</span>
</code></pre></div>

<p>In the above example, we generate a random high-dimensional dataset using NumPy and apply PCA to reduce its dimensionality to 2. We then calculate the perplexity of the original data and the reduced data using the <code>perplexity</code> function. Finally, we print the mean perplexity values for comparison.</p>
<p>By examining the perplexity values, we can gain insights into how well PCA preserves the local structure of the data. If the perplexity of the reduced data is close to the perplexity of the original data, it suggests that PCA successfully retains the essential information during dimensionality reduction.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this blog post, we explored the concept of perplexity in the context of dimensionality reduction, specifically in PCA. Perplexity provides a measure of the level of uncertainty or confusion in predicting the neighbors of a data point in a lower-dimensional space. It is used to assess the quality of dimensionality reduction algorithms by evaluating how well they preserve the local structure of the data.</p>
<p>We also provided a Python implementation to calculate perplexity and demonstrated its application in evaluating dimensionality reduction using PCA. By comparing the perplexity of the original data with the perplexity of the reduced data, we can assess the effectiveness of PCA in preserving the essential information.</p>
<p>Perplexity is a valuable tool in the evaluation and comparison of dimensionality reduction methods. It offers insights into the preservation of the local structure and can guide the selection of appropriate techniques for different datasets and applications.</p>
<p>See also:
<a href="https://distill.pub/2016/misread-tsne/">How to Use t-SNE Effectively</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>


        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="http://127.0.0.1:8000/tag/pca/" class="article-tag">pca</a>
                <a href="http://127.0.0.1:8000/tag/perplexity/" class="article-tag">perplexity</a>
                <a href="http://127.0.0.1:8000/tag/dimensionality-reduction/" class="article-tag">dimensionality-reduction</a>
                <a href="http://127.0.0.1:8000/tag/probability/" class="article-tag">probability</a>
                <a href="http://127.0.0.1:8000/tag/tsne/" class="article-tag">TSNE</a>
                <a href="http://127.0.0.1:8000/tag/distance/" class="article-tag">distance</a>
        </div>





            <div class="related-posts">
                <h4 class="related-posts-title">You might also like</h4>
                <div class="related-posts-grid">
                        <a href="http://127.0.0.1:8000/what-is-the-key-difference-between-pca-and-svd/" class="related-post-card">
                            <span class="related-post-title">What Is the Key Difference Between PCA and SVD?</span>
                            <span class="related-post-date">Nov 06, 2023</span>
                        </a>
                        <a href="http://127.0.0.1:8000/before-pca/" class="related-post-card">
                            <span class="related-post-title">Checks and Data Preprocessing Steps Before Applying PCA</span>
                            <span class="related-post-date">Feb 20, 2023</span>
                        </a>
                        <a href="http://127.0.0.1:8000/measure-quality-of-embeddings-intrinsic-vs-extrinsic/" class="related-post-card">
                            <span class="related-post-title">Intrinsic vs. Extrinsic Evaluation - What's the Best Way to Measure Embedding Quality?</span>
                            <span class="related-post-date">Apr 18, 2023</span>
                        </a>
                        <a href="http://127.0.0.1:8000/tsne-tutorial/" class="related-post-card">
                            <span class="related-post-title">Unleashing the Power of T-Sne for Dimensionality Reduction in Python</span>
                            <span class="related-post-date">Mar 15, 2021</span>
                        </a>
                </div>
            </div>




    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "http://127.0.0.1:8000",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="http://127.0.0.1:8000/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false
        });
    });
</script>

<script src="http://127.0.0.1:8000/theme/js/theme-switcher.js"></script>
</body>
</html>