<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Krystian Safjan's Blog</title><link href="http://127.0.0.1:8000/" rel="alternate"/><link href="http://127.0.0.1:8000/feeds/all-en.atom.xml" rel="self"/><id>http://127.0.0.1:8000/</id><updated>2025-11-22T00:00:00+01:00</updated><subtitle>Data Scientist | Researcher | Team Leader&lt;br&gt;&lt;br&gt; working at Ernst &amp;amp; Young and writing about &lt;a href="/category/machine-learning.html"&gt;Data Science and Visualization&lt;/a&gt;, on &lt;a href="/category/machine-learning.html"&gt;Machine Learning, Deep Learning&lt;/a&gt; and &lt;a href="/tag/nlp/"&gt;NLP&lt;/a&gt;. There are also some &lt;a href="/category/howto.html"&gt;howto&lt;/a&gt; posts on tools and workflows.</subtitle><entry><title>Six Weeks, Real Progress - Exploring Shape Up for Product Work</title><link href="http://127.0.0.1:8000/six-weeks-real-progress-exploring-shape-up-for-product-work/" rel="alternate"/><published>2025-10-06T00:00:00+02:00</published><updated>2025-11-22T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2025-10-06:/six-weeks-real-progress-exploring-shape-up-for-product-work/</id><summary type="html">&lt;p&gt;Shape Up offers a focused alternative to sprint-driven development, trading constant ceremonies and tight iterations for clear boundaries and six-week cycles. This article explores when it works well, when it doesn’t, and why it may help teams overwhelmed by process overhead.&lt;/p&gt;</summary><content type="html">&lt;h1 id="agile-scrum-shape-up-basecamp-37-signals-kanban"&gt;agile #scrum #shape-up #basecamp #37-signals #kanban&lt;/h1&gt;
&lt;p&gt;I've been thinking a lot lately about how we build software. Not the code itself—that's the easy part, really—but the &lt;em&gt;process&lt;/em&gt;. The endless ceremonies, the sprint planning that somehow takes three hours, the daily standups that drift into problem-solving sessions, the backlog that's become this unwieldy beast no one wants to groom.&lt;/p&gt;
&lt;p&gt;For years, we've been told that Agile is the answer. Scrum, Kanban, SAFe—pick your flavor. And look, these frameworks have helped countless teams ship better software. But somewhere along the way, something started feeling off. The two-week sprints that fragment work into artificial chunks. The constant context switching without possibility to think deeply about the problem on the table. The &lt;strong&gt;weird pressure to estimate story points for work we barely understand yet&lt;/strong&gt;. The nagging feeling that we're &lt;strong&gt;spending more time &lt;em&gt;managing&lt;/em&gt; the process than actually building things&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Then I stumbled onto &lt;a href="https://basecamp.com/shapeup"&gt;Shape Up&lt;/a&gt;, the approach developed by Basecamp (now &lt;a href="https://37signals.com/"&gt;37signals&lt;/a&gt;), and it felt like someone had articulated frustrations I didn't even know I had.&lt;/p&gt;
&lt;h2 id="what-makes-shape-up-different"&gt;What Makes Shape Up Different?&lt;/h2&gt;
&lt;p&gt;Shape Up isn't just Agile with different terminology. It's a fundamentally different way of thinking about product development cycles.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The core idea&lt;/strong&gt;
Work in &lt;strong&gt;six-week cycles&lt;/strong&gt;. Not two weeks. Six. &lt;strong&gt;Long enough to build something meaningful, short enough to stay focused&lt;/strong&gt;. "Plus it gives you about eight chances a year to recalibrate and decide what to work on next." (see: &lt;a href="https://37signals.com/06"&gt;37signals&lt;/a&gt;). Between cycles, there's a two-week cooldown where teams can breathe, fix bugs, explore ideas, or just catch up on that technical debt everyone's been ignoring.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But here's where it gets interesting: before a cycle starts, senior people do what Basecamp calls "&lt;strong&gt;&lt;a href="https://basecamp.com/shapeup/0.3-chapter-01#shaping-the-work"&gt;shaping&lt;/a&gt;.&lt;/strong&gt;" They take raw ideas and turn them into bounded, well-considered pitches. Not detailed specifications—that would defeat the purpose—but something with clear boundaries, key insights, and thoughtful limitations. Crucially, they also identify what you're &lt;em&gt;not&lt;/em&gt; going to build.&lt;/p&gt;
&lt;p&gt;Then &lt;strong&gt;small teams&lt;/strong&gt; (typically a &lt;strong&gt;designer&lt;/strong&gt; and &lt;strong&gt;one or two programmers&lt;/strong&gt;) take these shaped projects and run with them. No daily standups. No one breathing down their necks asking for status updates. They're trusted to figure out the details, make trade-offs, and solve problems. The appetite is fixed—six weeks—but the scope is flexible within reason. If something's taking too long, you cut scope, not time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's no backlog&lt;/strong&gt;. Ideas that don't make it into a cycle just… don't exist anymore. &lt;strong&gt;If they're good, they'll come back&lt;/strong&gt;. If they're not, well, you just saved yourself from maintaining a graveyard of forgotten tickets.&lt;/p&gt;
&lt;h2 id="when-shape-up-really-shines"&gt;When Shape Up Really Shines&lt;/h2&gt;
&lt;p&gt;I've seen Shape Up work beautifully for product teams that are tired of the sprint treadmill. It's particularly powerful when you're &lt;strong&gt;building features that need genuine design thinking&lt;/strong&gt; and integration work—the kind of projects that get mangled when you try to slice them into two-week chunks.&lt;/p&gt;
&lt;p&gt;It works when your &lt;strong&gt;team is experienced enough&lt;/strong&gt; to operate with autonomy. When people can make good decisions without needing approval for every little thing. When you trust your developers to be adults who can communicate when they're stuck without needing a daily check-in.&lt;/p&gt;
&lt;p&gt;The six-week cycles are a game-changer for &lt;strong&gt;work that has natural complexity&lt;/strong&gt;. You can actually design something properly, build it thoughtfully, and refine it without this artificial urgency every two weeks. Teams report feeling less fragmented, more focused, and paradoxically more creative because they have room to think.&lt;/p&gt;
&lt;p&gt;The "no backlog" philosophy is incredibly liberating. You stop maintaining this guilt-inducing list of things you'll never do. You stop feeling bad about all those tickets that have been sitting there for eighteen months. Each cycle is a fresh start.&lt;/p&gt;
&lt;h2 id="when-it-might-not-be-your-best-bet-shape-up-isnt-a-universal-solution"&gt;When It Might Not Be Your Best Bet - Shape Up isn't a universal solution.&lt;/h2&gt;
&lt;p&gt;If you're maintaining a &lt;strong&gt;mature product&lt;/strong&gt; with &lt;strong&gt;lots of small&lt;/strong&gt; customer &lt;strong&gt;requests&lt;/strong&gt; and &lt;strong&gt;bug fixes&lt;/strong&gt;, the six-week cycle might feel cumbersome. Sometimes you just need to knock out twenty small improvements, and cramming them into a "shaped" project feels forced. The cooldown weeks help, but if this is 80% of your work, you might be fighting the framework.&lt;/p&gt;
&lt;p&gt;It assumes a certain organizational maturity. &lt;strong&gt;The shaping process requires people who really understand both the business and the technology&lt;/strong&gt;. If you don't have &lt;strong&gt;senior folks&lt;/strong&gt; who can do this well, you'll end up with poorly defined projects that either explode in scope or leave teams floundering. And honestly? &lt;strong&gt;Good shaping is hard. It's a skill that takes time to develop.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Shape Up can be rough for teams that need predictability. If you're in a context where stakeholders need firm &lt;strong&gt;commitments about exactly what ships when&lt;/strong&gt;, the flexible scope model might cause friction. "We'll ship something good in six weeks" is a very different promise than "We'll deliver these fourteen story points by the end of Sprint 23."&lt;/p&gt;
&lt;p&gt;It's also &lt;strong&gt;not ideal if you're in true discovery mode&lt;/strong&gt;, experimenting rapidly with prototypes to find product-market fit. The &lt;strong&gt;six-week commitment is too heavy&lt;/strong&gt; when you need to pivot every few days based on user feedback. Shape Up assumes you have &lt;em&gt;some&lt;/em&gt; clarity about what you're building.&lt;/p&gt;
&lt;p&gt;And let's be honest: if your &lt;strong&gt;organization is deeply committed to Agile&lt;/strong&gt; ceremonies, metrics, and tooling, the transition can be difficult. No velocity charts. No burndown graphs. No story points. Some organizations aren't ready to let go of those security blankets.&lt;/p&gt;
&lt;h2 id="the-real-question"&gt;The Real Question&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The more I dig into Shape Up, the more I think the real question isn't "Is Shape Up better than Agile?" It's &lt;strong&gt;"What problems are we actually trying to solve?"&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Are you frustrated by &lt;strong&gt;fragmented work&lt;/strong&gt; and constant &lt;strong&gt;context switching&lt;/strong&gt;? Shape Up might help. Are you drowning in &lt;strong&gt;process overhead&lt;/strong&gt; and want to give teams &lt;strong&gt;more autonomy&lt;/strong&gt;? Worth exploring. Do you need to ship &lt;strong&gt;lots of small changes quickly and predictably&lt;/strong&gt;? Maybe stick with what you know.&lt;/p&gt;
&lt;p&gt;Shape Up' is opinionated. It makes trade-offs. It acknowledges that different contexts need different approaches.&lt;/p&gt;
&lt;p&gt;Not that Shape Up is better or worse than Scrum or Kanban, but that we should &lt;strong&gt;be more thoughtful about &lt;em&gt;why&lt;/em&gt; we work the way we do&lt;/strong&gt;. Remember: dare to question the defaults. &lt;/p&gt;
&lt;h2 id="further-reading"&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;overview article on shape up method - &lt;a href="https://agilefirst.io/what-is-shape-up/"&gt;Shape Up: a complete guide to this new development methodology (2024)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;free book on shape up - &lt;a href="https://basecamp.com/shapeup"&gt;Shape Up: Stop Running in Circles and Ship Work that Matters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://37signals.com/"&gt;37signals&lt;/a&gt; - This site is a catalog of liberating ideas about how they (company behind basecamp, Ruby on Rails,...) works, do business, live...&lt;/li&gt;
&lt;/ul&gt;</content><category term="Software Development"/><category term="agile, scrum, shape-up, basecamp, 37-signals, shape-up-method, six-week-cycles, product-development-process, agile-alternatives, autonomous-teams, shaping-phase, delivery-rhythm, software-process-design, software-development"/></entry><entry><title>Beyond Coverage - Building Truly Complete Test Suites with GitHub Copilot</title><link href="http://127.0.0.1:8000/beyond-coverage-building-truly-complete-test-suites-with-github-copilot/" rel="alternate"/><published>2025-06-15T00:00:00+02:00</published><updated>2025-06-15T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2025-06-15:/beyond-coverage-building-truly-complete-test-suites-with-github-copilot/</id><summary type="html">&lt;p&gt;This article explores how to move beyond simplistic code coverage metrics to build truly comprehensive test suites using GitHub Copilot. Drawing from practical experience, I demonstrate how AI-assisted testing can identify behavioral gaps, validate API contracts, generate maintainable tests, and address flaky tests - ultimately creating a sustainable quality assurance strategy focused on behaviors rather than coverage percentages. Learn specific techniques for behavioral auditing, integration testing, and continuous quality monitoring that have transformed our approach to software reliability.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the past year, I've found myself increasingly dissapointed with the traditional approach to test coverage. Sure, hitting 90% line coverage feels good, but I've watched too many "well-tested" codebases crumble under the weight of production bugs that somehow slipped through. The problem isn't just that we're measuring the wrong things - it's that we're treating testing as a checkbox exercise rather than a comprehensive quality assurance strategy.&lt;/p&gt;
&lt;p&gt;That's when I started experimenting with GitHub Copilot's Agent Mode, not just as a code completion tool, but as a systematic approach to building truly complete test suites. What I discovered was a fundamentally different way of thinking about testing - one that goes beyond coverage percentages to focus on behavioral completeness, integration reliability, and long-term maintainability.&lt;/p&gt;
&lt;h2 id="the-coverage-trap"&gt;The Coverage Trap&lt;/h2&gt;
&lt;p&gt;Let me start with a confession: I used to be obsessed with coverage numbers. There's something deeply satisfying about seeing that green 95% coverage badge, but recent research finds "disconcerting trends for maintainability" when we rely too heavily on automated tools without proper oversight. The truth is, coverage metrics can lull you into a false sense of security.&lt;/p&gt;
&lt;p&gt;I learned this the hard way when our system failed spectacularly in production due to a race condition in our retry logic. The lines were covered, but the behavior wasn't tested. That's when I realized we needed to &lt;strong&gt;shift from measuring what code runs to validating what the code actually does&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id="beyond-line-coverage-behavioral-auditing"&gt;Beyond Line Coverage: Behavioral Auditing&lt;/h2&gt;
&lt;p&gt;The first breakthrough came when I started using Copilot to audit our entire codebase for behavioral gaps. Instead of focusing on lines of code, I began asking it to identify untested public functions and methods. This simple prompt changed everything:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Scan the codebase identify all public functions and methods, then report which of them lack any direct test invocation. Group them by module."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(NOTE: you should include your code base as context in Agent mode with e.g. &lt;code&gt;#codebase&lt;/code&gt; or specific dir &lt;code&gt;#file:src&lt;/code&gt; )&lt;/p&gt;
&lt;p&gt;This might look similar to coverage testing but instead of covered lines you are getting information about functions that are not called directly by any of the tests.&lt;/p&gt;
&lt;p&gt;What emerged was startling. We had entire utility functions, error handling routines, and data transformation methods that had never been directly tested. They were covered by higher level tests, but their specific behaviors - especially edge cases remained completely unvalidated.&lt;/p&gt;
&lt;p&gt;This behavioral audit approach revealed gaps that traditional coverage tools simply can't detect. When you're validating input spaces rather than code paths, you uncover scenarios like empty inputs, malformed data, and maximum size payloads that can break your system in ways that line coverage never anticipates.&lt;/p&gt;
&lt;h2 id="the-api-first-testing-strategy"&gt;The API-First Testing Strategy&lt;/h2&gt;
&lt;p&gt;One of the most valuable insights from this journey has been the importance of API surface auditing. Every Flask endpoint, every REST API, every public interface represents a contract with the outside world. Breaking these contracts doesn't just cause bugs - it breaks trust with users and downstream systems.&lt;/p&gt;
&lt;p&gt;I started having Copilot systematically inventory all our endpoints and cross-reference them with our integration tests. The results were eye-opening: critical user journeys like password reset and account verification had comprehensive unit tests but no end-to-end validation. Copilot did the work of finding relevant files, extracting the relevant styles and patterns, and applying those forward to the new test suite that it generated, creating coherent integration tests that followed our established patterns.&lt;/p&gt;
&lt;p&gt;This approach catches issues that unit tests simply can't see serialization problems, authentication flows, error response formats, and the subtle ways that components interact when they're wired together in a real system.&lt;/p&gt;
&lt;h2 id="automating-the-tedious-parts"&gt;Automating the Tedious Parts&lt;/h2&gt;
&lt;p&gt;Once I had a clear picture of what needed testing, the next challenge was actually writing all those tests. This is where GitHub Copilot's ability to generate tests becomes invaluable - you can select the code you want to test, right-click in your IDE and select Copilot -&amp;gt; Generate Tests, or use slash commands to quickly scaffold test suites.&lt;/p&gt;
&lt;p&gt;But I discovered that the real power isn't in generating individual tests - it's in systematically working through entire modules. I'd point Copilot at a file like &lt;code&gt;payment_processor.py&lt;/code&gt; and ask it to generate pytest tests covering valid payments, negative amounts, and simulated network failures using mocks. The agent would create the test file, inject proper fixtures, write assertions, and even run the tests to check for immediate failures.&lt;/p&gt;
&lt;p&gt;More importantly, Copilot excels at converting repetitive test patterns into parameterized tests. Instead of five nearly identical tests for different input values, I could ask it to consolidate them into a single &lt;code&gt;@pytest.mark.parametrize&lt;/code&gt; block. This not only reduces maintenance overhead but makes it trivial to add new edge cases as you discover them.&lt;/p&gt;
&lt;h2 id="the-flaky-test-problem"&gt;The Flaky Test Problem&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Fleaky tests - the tests that pass sometimes and fail other times are the bane of every CI pipeline. They waste developer time, obscure real issues, and erode trust in your test suite.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No discussion of comprehensive testing is complete without addressing the elephant in the room: flaky tests. There are two main types of flaky tests: those that are flaky due to some external conditions, such as network issues, machine crashes, power outages, and those that are flaky due to test design issues.&lt;/p&gt;
&lt;p&gt;To spot flaky tests, you need to compare test results from multiple test runs. This analysis would be a time consuming process to perform manually, but fortunately, many CI servers detect flaky tests automatically. The key insight is that Copilot can go beyond just detection to root cause analysis and remediation.&lt;/p&gt;
&lt;p&gt;For timing related flakiness, it suggests explicit waits or better synchronization. For external dependency issues, it recommends proper mocking. For shared state problems, it proposes better isolation techniques. The goal isn't to eliminate all flakiness - that's impossible, but to make your test suite reliable enough that failures actually mean something.&lt;/p&gt;
&lt;h2 id="test-quality-as-a-first-class-concern"&gt;Test Quality as a First-Class Concern&lt;/h2&gt;
&lt;p&gt;As our test suite grew, I realized that test quality itself needed to become a first-class concern. Bad tests are worse than no tests - they give you false confidence while slowing down development. This is where Copilot's analytical capabilities really shine.&lt;/p&gt;
&lt;p&gt;I started having it audit our test directory for common anti-patterns: empty test functions, duplicated assertions, magic constants, and tests that rely on implicit ordering. The agent would flag these issues and suggest refactors-converting magic numbers to named constants, extracting common setup into fixtures, and consolidating duplicate logic.&lt;/p&gt;
&lt;p&gt;But the most valuable insight was learning to cross-reference coverage reports with module criticality. Not all code is equally important, and not all untested code represents the same level of risk. By having Copilot map coverage data against business-critical modules like payment processing and authentication, I could focus our testing efforts where they would have the most impact.&lt;/p&gt;
&lt;h2 id="integration-and-end-to-end-validation"&gt;Integration and End-to-End Validation&lt;/h2&gt;
&lt;p&gt;Unit tests form the foundation, but they can't catch the subtle ways that components interact in production. This is where integration and end-to-end testing become crucial, and where Copilot's ability to understand entire workflows becomes invaluable.&lt;/p&gt;
&lt;p&gt;I've had great success asking Copilot to generate integration tests that exercise entire user journeys - from account creation through data processing to final output. These tests use in-memory databases for speed but validate the complete data flow including serialization, authentication, and error handling.&lt;/p&gt;
&lt;p&gt;The key is to focus on critical user paths rather than trying to test every possible integration. A single end-to-end test that uploads a CSV file, triggers data ingestion, and verifies the resulting database entries can catch a surprising number of issues that unit tests miss entirely.&lt;/p&gt;
&lt;h2 id="looking-forward"&gt;Looking Forward&lt;/h2&gt;
&lt;p&gt;After a year of experimenting with this approach, I've come to believe that comprehensive testing isn't about reaching some magical coverage percentage - it's about building systems that give you confidence in your code's behavior. Copilot has been instrumental in making this transition from coverage-focused to behavior-focused testing.&lt;/p&gt;
&lt;p&gt;The techniques I've described here - behavioral auditing, API surface validation, automated test generation, flaky test management, and continuous quality monitoring - work together to create a testing strategy that's both comprehensive and maintainable. Each element addresses a different aspect of the testing challenge, from initial coverage gaps to long-term sustainability.&lt;/p&gt;
&lt;p&gt;What excites me most is that this is just the beginning. As AI tools become more sophisticated, I expect we'll see even more powerful approaches to test analysis and generation. The key is to remember that these tools are amplifiers of human insight, not replacements for it. The goal is to spend less time on mechanical test-writing and more time on the kinds of deep, thoughtful testing that actually prevents bugs.&lt;/p&gt;
&lt;p&gt;The future of testing isn't about perfect coverage - it's about perfect understanding of what your code actually does, and having the confidence that comes from knowing you've validated the behaviors that matter most.&lt;/p&gt;</content><category term="Software Development"/><category term="github-copilot"/><category term="test-coverage"/><category term="test-automation"/><category term="code-quality"/><category term="automated-testing"/><category term="integration-testing"/><category term="flaky-tests"/><category term="ci-cd"/><category term="quality-assurance"/><category term="behavior-driven-testing"/></entry><entry><title>Simple In-Memory Knowledge Graphs for Quick Graph Querying</title><link href="http://127.0.0.1:8000/simple-inmemory-knowledge-graphs-for-quick-graph-querying/" rel="alternate"/><published>2025-01-16T00:00:00+01:00</published><updated>2025-01-16T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2025-01-16:/simple-inmemory-knowledge-graphs-for-quick-graph-querying/</id><summary type="html">&lt;p&gt;As developers, we often reach for full-scale graph databases when simpler solutions would suffice. When your knowledge graph is modest in size, keeping it in memory can be both efficient and practical. Let's explore some powerful tools that make this approach work beautifully.&lt;/p&gt;</summary><content type="html">&lt;p&gt;up::[[knowledge_graphs]]&lt;/p&gt;
&lt;p&gt;Working with knowledge graphs doesn't always require &lt;a href="https://neo4j.com/"&gt;Neo4j&lt;/a&gt; or other heavyweight solutions. Sometimes you need a lightweight way to represent and query graph data right in memory. Let me share some approachable solutions I've found particularly useful.&lt;/p&gt;
&lt;h2 id="networkx-the-python-swiss-army-knife"&gt;NetworkX - The Python Swiss Army Knife&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/networkx/networkx"&gt;NetworkX&lt;/a&gt; has been my reliable companion for simple graph operations. It's incredibly intuitive and perfect for smaller knowledge graphs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;networkx&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nx&lt;/span&gt;

&lt;span class="c1"&gt;# Create a knowledge graph&lt;/span&gt;
&lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Add some knowledge&lt;/span&gt;
&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;knows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Bob&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Bob&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;works_at&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TechCorp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Simple queries&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_connections&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can test it with this example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Test NetworkX&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;find_connections&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Alice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;find_connections&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Bob&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;NetworkX&lt;/span&gt; &lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Bob&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relationship&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;knows&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})]&lt;/span&gt;
&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Alice&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relationship&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;knows&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TechCorp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relationship&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;works_at&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="rdflib-when-you-need-semantic-power"&gt;RDFLib - When You Need Semantic Power&lt;/h2&gt;
&lt;p&gt;If you're dealing with semantic data and need &lt;a href="https://en.wikipedia.org/wiki/SPARQL"&gt;SPARQL&lt;/a&gt;-like querying, &lt;a href="https://rdflib.readthedocs.io/en/stable/index.html"&gt;RDFLib&lt;/a&gt; provides a perfect middle ground:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;rdflib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Literal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RDF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;URIRef&lt;/span&gt;

&lt;span class="c1"&gt;# Create an in-memory graph&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Add triples&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;URIRef&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Alice&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;URIRef&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;knows&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;URIRef&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Bob&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="c1"&gt;# Query using SPARQL&lt;/span&gt;
&lt;span class="n"&gt;qres&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;SELECT ?s ?o&lt;/span&gt;
&lt;span class="sd"&gt;       WHERE {&lt;/span&gt;
&lt;span class="sd"&gt;          ?s knows ?o .&lt;/span&gt;
&lt;span class="sd"&gt;       }&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;qres&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
 &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; -&amp;gt; &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;RDFLib&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;Bob&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;TechCorp&lt;/span&gt;
&lt;span class="n"&gt;Alice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Bob&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="pygraphviz-visualization-with-query-capabilities"&gt;PyGraphviz - Visualization with Query Capabilities&lt;/h2&gt;
&lt;p&gt;When you need both visualization and querying use &lt;a href="https://github.com/pygraphviz/pygraphviz"&gt;pygraphviz&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pygraphviz&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pgv&lt;/span&gt;

&lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pgv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AGraph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_edge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Bob&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relationship&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;knows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_relationships&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: There might be an problem when installing pygraphviz in Google Colab, you can use matlplotlib + networkx instead&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="diy-solution-custom-graph-structure"&gt;DIY Solution - Custom Graph Structure&lt;/h2&gt;
&lt;p&gt;Sometimes, a custom solution fits best:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Any&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleKG&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predicate&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query_by_subject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]]:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_connected_nodes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;connected&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;objects&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;connected&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;objects&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;connected&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_paths&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]]:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

        &lt;span class="n"&gt;visited&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;queue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
        &lt;span class="n"&gt;paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;queue&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;queue&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;connected_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_connected_nodes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vertex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;next_node&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;connected_nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;next_node&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;next_node&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;next_node&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;visited&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;visited&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_node&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="n"&gt;queue&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;next_node&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;next_node&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;paths&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_by_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicates&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;predicate&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;predicates&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;obj&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;predicates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                    &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;subject&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obj&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_connected_through_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here is simple example how you can test it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Test the implementation&lt;/span&gt;
&lt;span class="n"&gt;kg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleKG&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;knows&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Bob&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Bob&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;works_at&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TechCorp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;TechCorp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;located_in&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;San Francisco&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Query by subject &amp;#39;Alice&amp;#39;:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query_by_subject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Find paths from Alice to TechCorp:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_paths&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TechCorp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Query by subject &amp;#39;Alice&amp;#39;: {&amp;#39;knows&amp;#39;: [&amp;#39;Bob&amp;#39;]}
Find paths from Alice to TechCorp: [[&amp;#39;Alice&amp;#39;, &amp;#39;Bob&amp;#39;, &amp;#39;TechCorp&amp;#39;]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;More advanced example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Create sample data&lt;/span&gt;
&lt;span class="n"&gt;kg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleKG&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Movies data&lt;/span&gt;
&lt;span class="n"&gt;movies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Inception&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;The Dark Knight&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Interstellar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dunkirk&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Memento&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;The Prestige&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Tenet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Fight Club&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Se7en&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;The Social Network&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Gone Girl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Panic Room&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;directors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;David Fincher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Martin Scorsese&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Quentin Tarantino&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Steven Spielberg&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;actors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Leonardo DiCaprio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christian Bale&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Matthew McConaughey&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Brad Pitt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Tom Hardy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Marion Cotillard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Michael Caine&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Anne Hathaway&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Cillian Murphy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Joseph Gordon-Levitt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Ellen Page&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Jesse Eisenberg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Ben Affleck&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Rosamund Pike&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Add relationships&lt;/span&gt;
&lt;span class="c1"&gt;# Directors directed movies&lt;/span&gt;
&lt;span class="n"&gt;movie_director&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Inception&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;The Dark Knight&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Interstellar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Dunkirk&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Memento&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;The Prestige&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Tenet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Fight Club&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;David Fincher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Se7en&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;David Fincher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;The Social Network&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;David Fincher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Gone Girl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;David Fincher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Panic Room&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;David Fincher&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;director&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;movie_director&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;directed_by&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;director&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;director&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;directed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Add actors to movies (random assignment for demonstration)&lt;/span&gt;
&lt;span class="n"&gt;movie_actors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Inception&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Leonardo DiCaprio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Tom Hardy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Marion Cotillard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Michael Caine&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Ellen Page&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Joseph Gordon-Levitt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;The Dark Knight&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Christian Bale&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Michael Caine&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Cillian Murphy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Interstellar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Matthew McConaughey&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Anne Hathaway&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Michael Caine&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Fight Club&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Brad Pitt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;The Social Network&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Jesse Eisenberg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Gone Girl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Ben Affleck&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Rosamund Pike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cast&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;movie_actors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;actor&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;stars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;acted_in&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Add some awards&lt;/span&gt;
&lt;span class="n"&gt;awards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Oscar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Golden Globe&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;BAFTA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;director&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;directors&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;award&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;awards&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;awards&lt;/span&gt;&lt;span class="p"&gt;))):&lt;/span&gt;
        &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;director&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;won&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;award&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;actor&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;actors&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;award&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;awards&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;awards&lt;/span&gt;&lt;span class="p"&gt;))):&lt;/span&gt;
        &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;won&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;award&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Example queries&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1. Find all movies directed by Christopher Nolan:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nolan_movies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_connected_through_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;directed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nolan_movies&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;2. Find actors who worked with Christopher Nolan (through any movie):&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nolan_actors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;movie&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nolan_movies&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;actors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_connected_through_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;stars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;nolan_actors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nolan_actors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;3. Find path between Leonardo DiCaprio and Christopher Nolan:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_paths&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Leonardo DiCaprio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Christopher Nolan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Found paths:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; -&amp;gt; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4. Find Oscar winners:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;oscar_winners&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_by_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;won&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;winner&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;winner&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;oscar_winners&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;winner&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Oscar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5. Find common movies between Michael Caine and Leonardo DiCaprio:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;caine_movies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_connected_through_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Michael Caine&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;acted_in&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;dicaprio_movies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_connected_through_predicate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Leonardo DiCaprio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;acted_in&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;caine_movies&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dicaprio_movies&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;movies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;directed&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Christopher&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nolan&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="err"&gt;[&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Inception&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dark&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Knight&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nb"&gt;Int&lt;/span&gt;&lt;span class="n"&gt;erstellar&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Dunkirk&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Memento&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Prestige&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Tenet&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;actors&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;who&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;worked&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Christopher&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nolan&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;through&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;movie&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="err"&gt;[&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Christian&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Bale&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Michael&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Caine&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Joseph&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kr"&gt;Go&lt;/span&gt;&lt;span class="n"&gt;rdon&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Levitt&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Cillian&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Murphy&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Anne&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Hathaway&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Matthew&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;McConaughey&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Marion&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cotillard&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Ellen&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Page&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="kr"&gt;To&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Hardy&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;between&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Christopher&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nolan&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="n"&gt;Found&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Inception&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Christopher&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nolan&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Inception&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Michael&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Caine&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dark&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Knight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Christopher&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nolan&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Inception&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Michael&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Caine&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;Int&lt;/span&gt;&lt;span class="n"&gt;erstellar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Christopher&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nolan&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Oscar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;winners&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="err"&gt;[&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="kr"&gt;To&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Hardy&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Marion&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cotillard&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Christian&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Bale&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Matthew&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;McConaughey&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Brad&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Pitt&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Martin&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Scorsese&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;common&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;movies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;between&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Michael&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Caine&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Leonardo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DiCaprio&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="err"&gt;[&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Inception&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="making-the-right-choice"&gt;Making the Right Choice&lt;/h2&gt;
&lt;p&gt;The best solution depends on your specific needs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use NetworkX for general graph operations and algorithms&lt;/li&gt;
&lt;li&gt;Choose RDFLib when working with semantic data and SPARQL&lt;/li&gt;
&lt;li&gt;Go with PyGraphviz when visualization is important&lt;/li&gt;
&lt;li&gt;Consider a custom solution for specialized query patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="performance-considerations"&gt;Performance Considerations&lt;/h2&gt;
&lt;p&gt;These solutions work well for graphs with thousands of nodes and edges. The key is keeping everything in memory and optimizing your query patterns. For NetworkX and RDFLib, using their built-in query methods is usually faster than writing custom traversal code.&lt;/p&gt;
&lt;h2 id="beyond-simple-solutions"&gt;Beyond Simple Solutions&lt;/h2&gt;
&lt;p&gt;When your knowledge graph grows beyond memory constraints or you need more complex querying capabilities, it might be time to consider solutions like &lt;a href="https://neo4j.com/"&gt;Neo4j&lt;/a&gt; or &lt;a href="https://aws.amazon.com/neptune/"&gt;Amazon Neptune&lt;/a&gt;. However, for many use cases, these in-memory solutions provide the perfect balance of simplicity and functionality.&lt;/p&gt;
&lt;h2 id="a-note-on-automated-graph-construction"&gt;A Note on Automated Graph Construction&lt;/h2&gt;
&lt;p&gt;Building knowledge graphs by hand, as shown in our examples, is straightforward. However, automatically constructing them from documents or unstructured data is a complex challenge worthy of its own article. Here are some key challenges you'll face:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Entity recognition&lt;/strong&gt; and &lt;strong&gt;disambiguation&lt;/strong&gt; is perhaps the trickiest part - determining whether "Apple" refers to the fruit or the company, or whether two mentions of "John Smith" refer to the same person. You'll need to handle coreference resolution (understanding that "he" refers to "John" mentioned earlier) and deal with variations in how entities are written ("NYC" vs "New York City").&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relationship extraction&lt;/strong&gt; comes with its own set of problems. Natural language is complex and often implicit - extracting clear, structured relationships from sentences like "After years at Microsoft, Sarah brought her expertise to the startup" requires sophisticated NLP techniques.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data quality&lt;/strong&gt; and &lt;strong&gt;consistency&lt;/strong&gt; are also major concerns. Sources might conflict with each other, contain outdated information, or present opinions as facts. You'll need strategies for handling uncertainty and conflicting information in your graph.&lt;/p&gt;
&lt;p&gt;If you're interested in automatic graph construction, I'd recommend starting with established NLP libraries and knowledge graph toolkits rather than building everything from scratch. But that's a topic for another deep dive!&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;Don't jump to complex graph databases when simpler solutions might suffice. These in-memory approaches can handle surprisingly complex tasks while keeping your codebase clean and maintainable. Plus, they're perfect for prototyping before committing to a full-scale graph database solution.&lt;/p&gt;
&lt;h2 id="further-reading-references"&gt;Further Reading, References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;paper &lt;a href="https://arxiv.org/abs/2305.14485"&gt;[2305.14485] Knowledge Graphs Querying&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;similarar attempt as in this article to build and query knowledge graph: &lt;a href="https://medium.com/analytics-vidhya/querying-using-simple-knowledge-graphs-abeb13d05e48"&gt;Querying using simple knowledge graphs | by Vishnu Nandakumar | Analytics Vidhya | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://memgraph.com/docs/ai-ecosystem/graph-rag"&gt;GraphRAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cognipy.org/"&gt;CogniPy for Pandas - In-memory Graph Database and Knowledge Graph with Natural Language Interface - CogniPy 1.0.0 documentation&lt;/a&gt; - In-memory Graph Database and Knowledge Graph with Natural Language Interface&lt;/li&gt;
&lt;li&gt;not necessarily small and simple solutions: &lt;a href="https://www.puppygraph.com/blog/best-graph-databases"&gt;Title Unavailable | Site Unreachable&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Howto"/><category term="graph"/><category term="knowledge-graph"/><category term="query"/><category term="networkx"/><category term="neo4j"/><category term="rdflib"/><category term="SPARQL"/></entry><entry><title>RankFlow plot for retriever visual evaluation</title><link href="http://127.0.0.1:8000/rankflow-plot-for-retriever-visual-evaluation/" rel="alternate"/><published>2024-07-08T00:00:00+02:00</published><updated>2024-07-08T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2024-07-08:/rankflow-plot-for-retriever-visual-evaluation/</id><summary type="html">&lt;p&gt;RAG systems depend on high-quality retrieval to surface relevant information. Analyzing how document rankings evolve through multiple re-ranking steps is complex. This article explores ways to collect ranking data and visualize rank changes to optimize retriever effectiveness.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#tldr"&gt;TLDR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#introduction-and-problem-statement"&gt;Introduction and problem statement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#inspiration---rank-flow-visualization"&gt;Inspiration - Rank flow visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tracking-the-rank-changes-in-your-rag"&gt;Tracking the rank changes in your RAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rank-tracking-using-the-structured-logs"&gt;Rank tracking using the structured logs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-track-with-struct-logs"&gt;How to track with struct logs?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rank-tracking-using-callbacks"&gt;Rank tracking using callbacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-track-retriever-data-with-callbacks"&gt;How to track retriever data with callbacks?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visualization"&gt;Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h2 id="tldr"&gt;TLDR&lt;/h2&gt;
&lt;p&gt;This article discusses the importance of tracking and visualizing rank changes in Retrieval-Augmented Generation (RAG) systems. It introduces the concept of rank flow visualization, which helps analyze how document rankings evolve through different stages of retrieval and re-ranking. The article outlines methods for collecting rank data using structured logging and callbacks, and presents a Python library called 'rankflow' for creating visual representations of these rank changes. This visualization technique enables AI professionals to quickly identify patterns and optimize their RAG systems, ultimately improving the quality of information retrieval and generation.
&lt;a id="introduction-and-problem-statement"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction-and-problem-statement"&gt;Introduction and problem statement&lt;/h2&gt;
&lt;p&gt;Retrieval-Augmented Generation (RAG) systems are composed of two primary components: the &lt;strong&gt;retriever&lt;/strong&gt; and the &lt;strong&gt;answer generator&lt;/strong&gt;. The overall efficacy of RAG is largely contingent on the quality of the retriever, making it a critical area for optimization and analysis.&lt;/p&gt;
&lt;p&gt;Advanced retrieval mechanisms often process numerous document fragments or nodes, which are initially ranked based on relevance. To enhance result quality, these nodes undergo &lt;strong&gt;multiple re-ranking phases&lt;/strong&gt;, each aimed at surfacing the most pertinent information.&lt;/p&gt;
&lt;p&gt;The re-ranking process can involve a variety of sophisticated techniques, including cross-encoders, bespoke re-ranking algorithms, and boosting systems. These methods serve to refine the rank of search results originating from diverse sources, such as traditional text search algorithms (e.g., BM25) and semantic search based on textual embeddings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tracking the evolving ranks of these document nodes&lt;/strong&gt; across multiple processing stages within the retriever can be a complex and time-consuming endeavor. However, leveraging the human brain's innate capacity for visual information processing offers a solution. By employing appropriate visualization techniques, we can significantly streamline the analysis of node rank fluctuations throughout the various retriever stages.&lt;/p&gt;
&lt;p&gt;This is where tools like rankflow chart (called also bump chart) come into play, offering a &lt;strong&gt;visual representation of rank changes&lt;/strong&gt; that allows for rapid comprehension and insights into the retriever's performance. Such visualizations enable AI professionals to efficiently identify patterns, anomalies, and opportunities for optimization in their RAG systems, ultimately leading to more effective and reliable information retrieval.&lt;/p&gt;
&lt;p&gt;In this article we focus on two aspects of visual analysis of rank changes: collecting data in retriever and generating graphical visualization like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="RankFlow Plot" src="/images/rankflow/rankflow_plot_full.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; RankFlow chart illustrating rank changes in four steps of re-ranking. You can track the given node's rank history visually. For example, Document 4, after hybrid search, initially had a rank of 4. Then, after the Cross-encoder surfaced, it was re-ranked to 1. The Graph re-ranker subsequently placed it at rank 6, and finally, the Booster changed its rank to 0.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="inspiration---rank-flow-visualization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="inspiration-rank-flow-visualization"&gt;Inspiration - Rank flow visualization&lt;/h2&gt;
&lt;p&gt;When we started searching information about the tool that could help us to visualize how the ranks are changing, we found &lt;a href="https://labs.polsys.net/tools/rankflow/"&gt;RankFlow&lt;/a&gt; that can do, more or less, the visualization type we were looking for.&lt;/p&gt;
&lt;p&gt;&lt;svg height="300" version="1.1" width="630" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="overflow: hidden; position: relative; top: -0.0546875px;"&gt;&lt;path fill="url('#4130-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M433.5,122Q401.5,122,369.74,206T305.5,290L305.5,239Q337.5,239,369.26,155T433.5,71L433.5,122" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4120-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M293.5,290Q261.5,290,229.5,290T165.5,290L165.5,239Q197.5,239,229.5,239T293.5,239L293.5,290" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4110-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M153.5,290Q121.5,290,89.5,290T25.5,290L25.5,239Q57.5,239,89.5,239T153.5,239L153.5,290" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4100-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M433.5,290Q401.5,290,369.18,178T305.5,66L305.5,15Q337.5,15,369.82,127T433.5,239L433.5,290" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4090-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M293.5,66Q261.5,66,229.5,66T165.5,66L165.5,15Q197.5,15,229.5,15T293.5,15L293.5,66" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4080-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M153.5,66Q121.5,66,89.74,150T25.5,234L25.5,183Q57.5,183,89.26,99T153.5,15L153.5,66" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4070-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M433.5,66Q401.5,66,369.74,150T305.5,234L305.5,183Q337.5,183,369.26,99T433.5,15L433.5,66" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4060-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M293.5,234Q261.5,234,229.5,234T165.5,234L165.5,183Q197.5,183,229.5,183T293.5,183L293.5,234" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4050-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M153.5,234Q121.5,234,89.42,206T25.5,178L25.5,127Q57.5,127,89.58,155T153.5,183L153.5,234" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4040-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M433.5,234Q401.5,234,369.42,206T305.5,178L305.5,127Q337.5,127,369.58,155T433.5,183L433.5,234" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4030-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M293.5,178Q261.5,178,229.5,178T165.5,178L165.5,127Q197.5,127,229.5,127T293.5,127L293.5,178" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4020-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M153.5,178Q121.5,178,89.42,150T25.5,122L25.5,71Q57.5,71,89.58,99T153.5,127L153.5,178" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4010-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M433.5,178Q401.5,178,369.42,150T305.5,122L305.5,71Q337.5,71,369.58,99T433.5,127L433.5,178" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#4000-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M293.5,122Q261.5,122,229.5,122T165.5,122L165.5,71Q197.5,71,229.5,71T293.5,71L293.5,122" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;path fill="url('#3990-rgb_200_0_55_-rgb_200_0_55_')" stroke="#ffffff" d="M153.5,122Q121.5,122,89.42,94T25.5,66L25.5,15Q57.5,15,89.58,43T153.5,71L153.5,122" stroke-width="1px" opacity="1" fill-opacity="1" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); opacity: 0.35; fill-opacity: 1;"&gt;&lt;/path&gt;&lt;desc style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;Created with Raphaël 2.1.2&lt;/desc&gt;&lt;defs style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;linearGradient id="3990-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4000-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4010-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4020-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4030-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4040-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4050-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4060-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4070-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4080-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4090-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4100-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4110-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4120-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;linearGradient id="4130-rgb_200_0_55_-rgb_200_0_55_" x1="0" y1="0" x2="1" y2="0" gradientTransform="matrix(1,0,0,1,0,0)" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;stop offset="0%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;stop offset="100%" stop-color="#c80037" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/stop&gt;&lt;/linearGradient&gt;&lt;/defs&gt;&lt;path fill="none" stroke="#000000" d="M8.5,300.5L8.5,0.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/path&gt;&lt;text x="3" y="291" text-anchor="end" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: end; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;0&lt;/tspan&gt;&lt;/text&gt;&lt;text x="3" y="20" text-anchor="end" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: end; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;5&lt;/tspan&gt;&lt;/text&gt;&lt;text x="13" y="5" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;Step_1&lt;/tspan&gt;&lt;/text&gt;&lt;rect x="13.5" y="15.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="13.5" y="71.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="13.5" y="127.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="13.5" y="183.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="13.5" y="239.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;text x="153" y="5" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;Step_2&lt;/tspan&gt;&lt;/text&gt;&lt;rect x="153.5" y="15.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="153.5" y="71.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="153.5" y="127.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="153.5" y="183.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="153.5" y="239.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;text x="293" y="5" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;Step_3&lt;/tspan&gt;&lt;/text&gt;&lt;rect x="293.5" y="15.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="293.5" y="71.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="293.5" y="127.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="293.5" y="183.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="293.5" y="239.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;text x="433" y="5" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;Step_4&lt;/tspan&gt;&lt;/text&gt;&lt;rect x="433.5" y="15.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="433.5" y="71.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="433.5" y="127.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="433.5" y="183.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;rect x="433.5" y="239.5" width="12" height="51" rx="0" ry="0" fill="#c80037" stroke="#000000" stroke-width="1px" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;&lt;/rect&gt;&lt;text x="27.5" y="41" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="167.5" y="97" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="307.5" y="97" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="447.5" y="153" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="27.5" y="97" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_12_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="167.5" y="153" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_12_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="307.5" y="153" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_12_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="447.5" y="209" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_12_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="27.5" y="153" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_2 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="167.5" y="209" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_2 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="307.5" y="209" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_2 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="447.5" y="41" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_2 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="27.5" y="209" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="167.5" y="41" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="307.5" y="41" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="447.5" y="265" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_1_3 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="27.5" y="265" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="167.5" y="265" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="307.5" y="265" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;text x="447.5" y="97" text-anchor="start" font-family="&amp;quot;Arial&amp;quot;" font-size="10px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: Arial; font-size: 10px;"&gt;&lt;tspan dy="3.44775390625" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"&gt;doc_2_1 (1)&lt;/tspan&gt;&lt;/text&gt;&lt;/svg&gt; |&lt;/p&gt;
&lt;p&gt;You can create the Excel table that reflect rank of the documents on each step. Note that the RankFlow tool supports also column with values that is responsible for the width of the "ribbon". You can use it to e.g. visualize importance of given node, or how many information from this node was finally used in the generated answer (if you have this type of information at hand).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Step_1&lt;/th&gt;
&lt;th&gt;val_1&lt;/th&gt;
&lt;th&gt;Step_2&lt;/th&gt;
&lt;th&gt;val_2&lt;/th&gt;
&lt;th&gt;Step_3&lt;/th&gt;
&lt;th&gt;val_3&lt;/th&gt;
&lt;th&gt;Step_4&lt;/th&gt;
&lt;th&gt;val_4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;doc_1_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_1_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_1_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_2_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;doc_12_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_1_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_1_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_2_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;doc_2_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_12_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_12_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_1_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;doc_1_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_2_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_2_2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_12_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;doc_2_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_2_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_2_1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;doc_1_3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This tool was an appetiser to have something similar implemented in Python. Before going to visualization, let's spent some time on how to collect data required for this visual analysis.&lt;/p&gt;
&lt;p&gt;&lt;a id="tracking-the-rank-changes-in-your-rag"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tracking-the-rank-changes-in-your-rag"&gt;Tracking the rank changes in your RAG&lt;/h2&gt;
&lt;p&gt;This is a separate topic - depending on the architecture of your retriever. I envision two main approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;using structured logging&lt;/li&gt;
&lt;li&gt;using callbacks&lt;/li&gt;
&lt;li&gt;using dedicated monitoring/[[2024-02-22-llm_observability_platforms|observability tools]]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we will discuss only first two since they are pretty while skipping the specific observability tools.&lt;/p&gt;
&lt;p&gt;&lt;a id="rank-tracking-using-the-structured-logs"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="rank-tracking-using-the-structured-logs"&gt;Rank tracking using the structured logs&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What are the struct logs?&lt;/strong&gt;
Structured logs are a standardized format for logging data where information is organized into consistent, machine-readable fields rather than free-form text. &lt;strong&gt;They typically use formats like JSON or key-value pairs&lt;/strong&gt;, making it easier to parse, search, and analyze log data programmatically. The benefits of using structured logs include improved log consistency, easier data extraction and analysis, better integration with log management tools, and enhanced ability to generate insights and troubleshoot issues in complex systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="how-to-track-with-struct-logs"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="how-to-track-with-struct-logs"&gt;How to track with struct logs?&lt;/h3&gt;
&lt;p&gt;&lt;a href=""&gt;Struct log&lt;/a&gt; have the advantage over non-structured log that you can easily, and with more confidentiality, extract data from it without using sophisticated log parsers. In Python, you can use loguru to drop the rank information to the separate log sink after each step that involves some form of reranking and trace e.g. node (chunk) id, new rank, and "label" for the reranking step. In this way you will get a jsonlines log file with all the data you need to create visualization. You can read more about how to use struct logs with loguru in &lt;a href=""&gt;loguru docs&lt;/a&gt; and &lt;a href=""&gt;this&lt;/a&gt; article.&lt;/p&gt;
&lt;p&gt;&lt;a id="rank-tracking-using-callbacks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="rank-tracking-using-callbacks"&gt;Rank tracking using callbacks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What are the callbacks?&lt;/strong&gt;
Callbacks in programming are functions passed as arguments to other functions, which are then executed at specific points during the execution of the containing function. In the context of machine learning and deep learning frameworks, callbacks are often used to track and log various metrics, execute custom actions, or modify behavior during training or inference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="how-to-track-retriever-data-with-callbacks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="how-to-track-retriever-data-with-callbacks"&gt;How to track retriever data with callbacks?&lt;/h3&gt;
&lt;p&gt;Here is an example how you can use callbacks to track re-ranking info in dummy implementation of the retriever with some re-ranking steps.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;id&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;RankingTracker&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_file&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rankings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;log_ranking&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="n"&gt;ranking&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rankings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;step&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ranking&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ranking&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;flush&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rankings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Retriever&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_relevant_documents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="c1"&gt;# Simulated retrieval&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Content 1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Content 2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Content 3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cross_encoder_rerank&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="c1"&gt;# Simulated cross-encoder re-ranking&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;  &lt;span class="c1"&gt;# Simulate score adjustment&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;custom_rerank&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="c1"&gt;# Simulated custom re-ranking&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt;  &lt;span class="c1"&gt;# Simulate score adjustment&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rag_retrieval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;retriever&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Retriever&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;rerankers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                  &lt;span class="n"&gt;tracker&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;RankingTracker&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="c1"&gt;# Initial retrieval&lt;/span&gt;
    &lt;span class="n"&gt;docs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;retriever&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_relevant_documents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;tracker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_ranking&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;initial_retrieval&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Apply each re-ranker&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reranker&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rerankers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;docs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reranker&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;query&amp;#39;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;reranker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__code__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;co_varnames&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;reranker&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tracker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log_ranking&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rerank_step_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;docs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this code above:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We define a simple &lt;code&gt;Document&lt;/code&gt; class to represent our documents.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;RankingTracker&lt;/code&gt; class is responsible for logging rankings at each step and writing them to a file.&lt;/li&gt;
&lt;li&gt;We have a basic &lt;code&gt;Retriever&lt;/code&gt; class that simulates initial document retrieval.&lt;/li&gt;
&lt;li&gt;Two re-ranking functions are defined: &lt;code&gt;cross_encoder_rerank&lt;/code&gt; and &lt;code&gt;custom_rerank&lt;/code&gt;. These simulate different re-ranking strategies.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;rag_retrieval&lt;/code&gt; function orchestrates the entire process:&lt;ul&gt;
&lt;li&gt;It first retrieves documents using the retriever.&lt;/li&gt;
&lt;li&gt;Then it applies each re-ranker in sequence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;After each step, it logs the current ranking using the tracker&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is the usage, with steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the tracker and retriever.&lt;/li&gt;
&lt;li&gt;Call &lt;code&gt;rag_retrieval&lt;/code&gt; with the query, retriever, list of re-rankers, and tracker.&lt;/li&gt;
&lt;li&gt;Print the final rankings.&lt;/li&gt;
&lt;li&gt;Flush the tracked rankings to a file.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Usage&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;tracker&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RankingTracker&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ranking_results.json&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;retriever&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Retriever&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;What is the capital of France?&amp;quot;&lt;/span&gt;

    &lt;span class="n"&gt;final_docs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rag_retrieval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;retriever&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cross_encoder_rerank&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;custom_rerank&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;tracker&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Print final rankings&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Final document rankings:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;final_docs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ID: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, Score: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Save all rankings to file&lt;/span&gt;
    &lt;span class="n"&gt;tracker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flush&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="visualization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="visualization"&gt;Visualization&lt;/h2&gt;
&lt;p&gt;For the visualization part you can use small Python library &lt;a href="https://pypi.org/project/rankflow/"&gt;rankflow&lt;/a&gt; (disclaimer: I'm the author) that is able to produce this type of visualization:
&lt;img alt="RankFlow Plot" src="/images/rankflow/rankflow_plot_full.jpg"&gt;&lt;/p&gt;
&lt;p&gt;First, install it with pip:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;rankflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It can accept various input data formats, one, convenient for data scientists is pandas data frame.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;rankflow&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RankFlow&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Doc 1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Doc 2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Doc 3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Step_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Step_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Step_3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Step_4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When the DataFrame is ready, then it is time to create RankFlow object and call &lt;code&gt;plot()&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RankFlow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# save the plot to png&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rankflow.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you found this library useful, please star the &lt;a href="https://github.com/izikeros/rankflow"&gt;repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With plotting options you have also some alternatives that you can use or take inspiration to create you own, customized rankflow plot/bump chart. See the references for alternatives.&lt;/p&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.vrogue.co/post/how-to-plot-bump-chart-in-r-finnstats"&gt;How To Plot Bump Chart In R Finnstats - vrogue.co&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kartikay-bagla/bump-plot-python"&gt;GitHub - kartikay-bagla/bump-plot-python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mplsoccer.readthedocs.io/en/latest/gallery/bumpy_charts/plot_bumpy.html#sphx-glr-gallery-bumpy-charts-plot-bumpy-py"&gt;Bumpy Charts - mplsoccer 1.2.4 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.org/gist/pascal-schetelat/8382651"&gt;Jupyter Notebook Viewer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"/><category term="retriever"/><category term="rag"/><category term="visuallization"/><category term="rag-evaluation"/><category term="logging"/><category term="struct-log"/><category term="rankflow"/><category term="observability"/><category term="callback"/></entry><entry><title>Open Source LLM Observability Tools and Platforms</title><link href="http://127.0.0.1:8000/open-source-llm-observability-tools-and-platforms/" rel="alternate"/><published>2024-02-22T00:00:00+01:00</published><updated>2024-06-26T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2024-02-22:/open-source-llm-observability-tools-and-platforms/</id><summary type="html">&lt;p&gt;Managing and monitoring the complex behavior of Large Language Models (LLMs) becomes increasingly crucial. LLMOps and LLM Observability provide essential tools for understanding and controlling these models, ensuring their safe and effective deployment. This article looks into the critical aspects of LLM Observability in the realm of generative AI.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a id="llm-observability-in-the-context-of-llmops-for-generative-ai"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="llm-observability-in-the-context-of-llmops-for-generative-ai"&gt;LLM Observability in the Context of LLMOps for Generative AI&lt;/h2&gt;
&lt;p&gt;AI is transforming the world, and one area where it has made significant strides is in generative models, particularly in the field of Large Language Models (LLMs) like GPT-3 and transformer models. However, as impressive as these models are, managing, monitoring, and understanding their behavior and output remains a challenge. Enter LLMOps, a new field focusing on the management and deployment of LLMs, and a key aspect of this is LLM Observability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#llm-observability-in-the-context-of-llmops-for-generative-ai"&gt;LLM Observability in the Context of LLMOps for Generative AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-is-llm-observability"&gt;What is LLM Observability?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#expected-functionalities-of-an-llm-observability-solution"&gt;Expected Functionalities of an LLM Observability Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#open-source-llm-observability-tools-and-platforms"&gt;Open Source LLM Observability Tools and Platforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#non-open-source"&gt;Non-open source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other---related"&gt;Other - related&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="what-is-llm-observability"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-is-llm-observability"&gt;What is LLM Observability?&lt;/h2&gt;
&lt;p&gt;LLM Observability is the ability to understand, monitor, and infer the internal state of an LLM from its external outputs. It encompasses several areas including model health monitoring, performance tracking, debugging, and evaluating model fairness and safety.&lt;/p&gt;
&lt;p&gt;In the context of LLMOps, LLM Observability is critical. LLMs are complex and can be unpredictable, producing outputs that range from harmless to potentially harmful or biased. It's therefore essential to have the right tools and methodologies for observing and understanding these models' behaviors in real-time, during training, testing, and after deployment.&lt;/p&gt;
&lt;p&gt;&lt;a id="expected-functionalities-of-an-llm-observability-solution"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="expected-functionalities-of-an-llm-observability-solution"&gt;Expected Functionalities of an LLM Observability Solution&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Performance Monitoring&lt;/strong&gt;: An observability solution should be able to track and monitor the performance of an LLM in real-time. This includes tracking metrics like accuracy, precision, recall, and F1 score, as well as more specific metrics like perplexity or token costs in the case of language models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Health Monitoring&lt;/strong&gt;: The solution should be capable of monitoring the overall health of the model, identifying and alerting on anomalies or potentially problematic patterns in the model's behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Debugging and Error Tracking&lt;/strong&gt;: If something does go wrong, the solution should provide debugging and error tracking functionalities, helping developers identify, trace, and fix issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fairness, Bias, and Safety Evaluation&lt;/strong&gt;: Given the potential for bias and ethical issues in AI, any observability solution should include features for evaluating fairness and safety, helping ensure that the model's outputs are unbiased and ethically sound.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interpretability&lt;/strong&gt;: LLMs can often be "black boxes", producing outputs without clear reasoning. A good observability solution should help make the model's decision-making process more transparent, providing insights into why a particular output was produced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integration with Existing LLMOps Tools&lt;/strong&gt;: Finally, the solution should be capable of integrating with existing LLMOps tools and workflows, from model development and training to deployment and maintenance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;LLM Observability is a crucial aspect of LLMOps for generative AI. It provides the &lt;strong&gt;visibility&lt;/strong&gt; and &lt;strong&gt;control&lt;/strong&gt; needed &lt;strong&gt;to effectively manage, deploy, and maintain Large Language Models&lt;/strong&gt;, ensuring they &lt;strong&gt;perform as expected, are free from bias, and are safe to use&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="open-source-llm-observability-tools-and-platforms"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="open-source-llm-observability-tools-and-platforms"&gt;Open Source LLM Observability Tools and Platforms&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/aavetis/azure-openai-logger"&gt;Azure OpenAI Logger&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/aavetis/azure-openai-logger.svg?logo=github"&gt; - "Batteries included" logging solution for your Azure OpenAI instance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Azure OpenAI Logger" src="https://github.com/aavetis/azure-openai-logger/raw/main/images/demo.gif"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/deepchecks/deepchecks"&gt;Deepchecks&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/deepchecks/deepchecks.svg?logo=github"&gt; - Tests for Continuous Validation of ML Models &amp;amp; Data. Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/evidentlyai/evidently"&gt;Evidently&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/evidentlyai/evidently.svg?logo=github"&gt; - Evaluate and monitor ML models from validation to production.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Giskard-AI/giskard"&gt;Giskard&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Giskard-AI/giskard.svg?logo=github"&gt; - Testing framework dedicated to ML models, from tabular to LLMs. Detect risks of biases, performance issues and errors in 4 lines of code.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/whylabs/whylogs"&gt;whylogs&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/whylabs/whylogs.svg?logo=github"&gt; - The open standard for data logging&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lunary-ai/lunary"&gt;lunary&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/lunary-ai/lunary.svg?logo=github"&gt; - The production toolkit for LLMs. observability, prompt management, and evaluations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/traceloop/openllmetry"&gt;openllmetry&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/traceloop/openllmetry.svg?logo=github"&gt; - Open-source observability for your LLM application, based on OpenTelemetry&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Arize-ai/phoenix"&gt;phoenix (Arize Ai)&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Arize-ai/phoenix.svg?logo=github"&gt; - AI Observability &amp;amp; Evaluation - Evaluate, troubleshoot, and fine-tune your LLM, CV, and NLP models in a notebook.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/langfuse/langfuse"&gt;langfuse&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/langfuse/langfuse.svg?logo=github"&gt; - Open source LLM engineering platform. observability, metrics, evals, prompt management  SDKs + integrations for Typescript, Python&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/whylabs/langkit"&gt;LangKit&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/whylabs/langkit.svg?logo=github"&gt; - An open-source toolkit for monitoring Large Language Models (LLMs).  Extracts signals from prompts &amp;amp; responses, ensuring safety &amp;amp; security. Features include text quality, relevance metrics, &amp;amp; sentiment analysis. Comprehensive tool for LLM observability.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AgentOps-AI/agentops"&gt;agentops&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/AgentOps-AI/agentops.svg?logo=github"&gt; - Python SDK for agent evals and observability&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pezzolabs/pezzo"&gt;pezzo&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/pezzolabs/pezzo.svg?logo=github"&gt; - Open-source, developer-first LLMOps platform designed to streamline prompt design, version management, instant delivery, collaboration, troubleshooting, observability and more.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fiddler-labs/fiddler-auditor"&gt;Fiddler AI&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/fiddler-labs/fiddler-auditor.svg?logo=github"&gt; - Evaluate, monitor, analyse, and improve machine learning and generative models from pre-production to production. Ship more ML and LLMs into production, and monitor ML and LLM metrics like hallucination, PII, and toxicity.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theodo-UK/OmniLog"&gt;OmniLog&lt;/a&gt; - &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Theodo-UK/OmniLog.svg?logo=github"&gt; - Observability tool for your LLM prompts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="non-open-source"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="non-open-source"&gt;Non-open source&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.rungalileo.io/galileo/galileo-gen-ai-studio/llm-studio"&gt;Generative AI Studio - Galileo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="other---related"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="other-related"&gt;Other - related&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/great-expectations/great_expectations"&gt;Great Expectations&lt;/a&gt; - Always know what to expect from your data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AgentOps-AI/tokencost"&gt;AgentOps-AI/tokencost&lt;/a&gt; - Easy token price estimates for LLMs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/YANG-DB/observability-prompots"&gt;observability prompts&lt;/a&gt; - LLM observability related prompts&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AstronomerAmber/LLM_Observability"&gt;LLM Observability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/BoundaryML/baml"&gt;baml&lt;/a&gt;  - A programming language to build strongly-typed LLM functions. Testing and observability included&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fluxninja/aperture"&gt;aperture&lt;/a&gt; - Rate limiting, caching, and request prioritization for modern workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/llm-monitoring-and-observability-c28121e75c2f"&gt;LLM Monitoring and Observability - A Summary of Techniques and Approaches for Responsible AI | by Josh Poduska | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aporia.com/learn/how-to-monitor-large-language-models/"&gt;Monitoring LLMs: Metrics, challenges, &amp;amp; hallucinations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mattcvincent/intro-llm-observability"&gt;mattcvincent/intro-&lt;em&gt;llm&lt;/em&gt;-&lt;em&gt;observability&lt;/em&gt;&lt;/a&gt; - Intro to LLM Observability&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.33rdsquare.com/what-is-perplexity-ai/"&gt;Demystifying Perplexity: An AI Expert‘s Comprehensive Guide - 33rd Square&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://huggingface.co/spaces/evaluate-metric/perplexity"&gt;Perplexity - a Hugging Face Space by evaluate-metric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drdroid.io/engineering-tools/list-of-top-llm-observability-tools"&gt;List of top LLM Observability Tools&lt;/a&gt; - good intro&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2024-12-19: Added reference to list of top observability tools&lt;/li&gt;
&lt;li&gt;2024-06-26: Added summary&lt;/li&gt;
&lt;/ul&gt;</content><category term="Generative AI"/><category term="observability"/><category term="mlops"/><category term="llmops"/><category term="llm"/><category term="monitoring"/><category term="model-monitoring"/><category term="prompt-management"/><category term="data-logging"/><category term="model-logging"/></entry><entry><title>Prompt Discovery</title><link href="http://127.0.0.1:8000/prompt-discovery/" rel="alternate"/><published>2023-11-04T00:00:00+01:00</published><updated>2023-11-04T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-11-04:/prompt-discovery/</id><summary type="html">&lt;p&gt;Learn prompt discovery to uncover the most effective prompts and combinations thereof to achieve specific tasks, while also considering factors like response quality, model performance, and computational efficiency&lt;/p&gt;</summary><content type="html">&lt;p&gt;Prompt discovery, in the context of large language models and prompt engineering, refers to the systematic process of identifying, optimizing, and fine-tuning prompts that elicit desired responses from the language model. It involves a blend of linguistic, computational, and experimental techniques to formulate prompts that yield accurate and contextually relevant outputs from the model.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goal of prompt discovery is to uncover the most effective prompts and combinations thereof to achieve specific tasks, while also considering factors like response quality, model performance, and computational efficiency.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In highly technical terms, prompt discovery encompasses several complex problems and activities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt Formulation&lt;/strong&gt;: This involves crafting prompts that are clear, unambiguous, and tailored to the desired task. Different phrasings and structures might lead to variations in model behavior, so prompt engineers need to experiment with syntax and semantics to achieve optimal results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt Permutations&lt;/strong&gt;: Researchers need to explore various permutations of prompts by altering wording, adding context, or using different query types. Systematically generating and testing different prompt variations is a crucial part of prompt discovery to identify which specific formulations generate the desired outputs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning Parameters&lt;/strong&gt;: Discovering the ideal fine-tuning parameters for each prompt and model combination is a complex optimization problem. Researchers must experiment with factors like learning rates, batch sizes, and optimization algorithms to fine-tune the model for specific prompts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Benchmarking and Comparison&lt;/strong&gt;: Comparing response quality across different prompt permutations, models, and settings is essential. This involves devising appropriate evaluation metrics to quantitatively assess the performance of the model in response to different prompts and making informed decisions based on these metrics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generalization and Transfer Learning&lt;/strong&gt;: Investigating the extent to which prompts can be generalized across tasks or domains is a challenging problem. Researchers need to explore how prompts can be adapted or transferred to different tasks without sacrificing performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploration of Novel Prompts&lt;/strong&gt;: As the field evolves, prompt engineers must continuously come up with innovative prompt formulations that push the boundaries of the model's capabilities. This might involve experimenting with new query structures, linguistic constructs, or contextual cues.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="process" src="/images/prompt_discovery/prompt_discovery_process.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Flowchart illustrating the steps in prompt discovery. Starting with prompt formulation, it progresses through prompt permutations, fine-tuning parameters, benchmarking and comparison, generalization and transfer learning, to the exploration of novel prompts.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For prompt discovery, a range of tools, both existing and potentially developed in the future, can be instrumental:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Automated Prompt Generation&lt;/strong&gt;: AI-assisted tools that automatically generate prompt variations based on input specifications could expedite the discovery process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt Optimization Algorithms&lt;/strong&gt;: Advanced optimization algorithms tailored for prompt discovery, including genetic algorithms or reinforcement learning approaches, could efficiently explore the prompt space.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interactive Prompt Testing Environments&lt;/strong&gt;: User-friendly interfaces that allow prompt engineers to interactively test and fine-tune prompts with real-time model feedback can facilitate rapid iteration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt Benchmarking Platforms&lt;/strong&gt;: Comprehensive platforms for benchmarking prompt performance across various tasks, models, and settings could aid in making informed prompt selection decisions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic Analysis Tools&lt;/strong&gt;: Tools that provide detailed semantic analysis of prompt-response pairs can help identify patterns and nuances in model behavior, guiding prompt formulation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Natural Language Understanding Frameworks&lt;/strong&gt;: Advanced NLU frameworks that provide insights into model comprehension and reasoning processes can inform prompt design for better results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transfer Learning Techniques&lt;/strong&gt;: Techniques that enable efficient transfer of knowledge from one prompt to another could support prompt generalization across tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Continuous Model Monitoring&lt;/strong&gt;: Real-time monitoring tools that track model performance in response to different prompts can aid in prompt discovery over time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="mindmap" src="/images/prompt_discovery/prompt_discovery_mindmap.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;*Figure 2:&lt;/strong&gt; Mindmap illustrating the key aspects of prompt discovery. It includes formulation, permutations, fine-tuning, benchmarking, generalization, novel prompts, and the different tools involved in the process.&lt;/p&gt;</content><category term="Generative AI"/><category term="machine-learning"/><category term="generative-ai"/><category term="prompt"/><category term="prompt-engineering"/><category term="prompt-discovery"/></entry><entry><title>Techniques to Boost RAG Performance in Production</title><link href="http://127.0.0.1:8000/techniques-to-boost-rag-performance-in-production/" rel="alternate"/><published>2023-11-01T00:00:00+01:00</published><updated>2023-11-04T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-11-01:/techniques-to-boost-rag-performance-in-production/</id><summary type="html">&lt;p&gt;This article discusses several advanced techniques that can be applied at different stages of the RAG pipeline to enhance its performance in a production setting.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Retrieval-Augmented Generation (RAG) is a powerful tool in the domain of machine learning, offering significant potential for improving the quality of text generation in various applications. However, optimizing its performance can be a challenging task. For the introductory text on RAG see my other &lt;a href="https://safjan.com/understanding-retrieval-augmented-generation-rag-empowering-llms/"&gt;article&lt;/a&gt;. This article discusses several advanced techniques that can be applied at different stages of the RAG pipeline to enhance its performance in a production setting.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#leveraging-hybrid-search"&gt;Leveraging Hybrid Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#utilizing-summaries-for-data-chunks"&gt;Utilizing Summaries for Data Chunks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#applying-query-transformations"&gt;Applying Query Transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#query-compression"&gt;Query Compression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimal-chunking-strategy"&gt;Optimal Chunking Strategy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fine-tuning-embedding-models"&gt;Fine-tuning Embedding Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enriching-metadata"&gt;Enriching Metadata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#employing-re-ranking"&gt;Employing Re-ranking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#addressing-the-lost-in-the-middle-problem"&gt;Addressing the 'Lost in the Middle' Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#meta-data-filtering"&gt;Meta-data Filtering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#query-routing"&gt;Query Routing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="leveraging-hybrid-search"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="leveraging-hybrid-search"&gt;Leveraging Hybrid Search&lt;/h2&gt;
&lt;p&gt;Hybrid search, a fusion of semantic search and keyword search, can be employed to retrieve pertinent data from a vector store. This method often yields superior results across a range of use cases. It essentially combines the strength of keyword search (precision) and semantic search (recall), providing a more comprehensive search solution.
[[dups/hybrid_search]]&lt;/p&gt;
&lt;p&gt;&lt;a id="utilizing-summaries-for-data-chunks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="utilizing-summaries-for-data-chunks"&gt;Utilizing Summaries for Data Chunks&lt;/h2&gt;
&lt;p&gt;An efficient way to enhance the quality of generation and reduce the number of tokens in the input is by summarizing the chunks of data and storing these summaries in the vector store. This technique is especially useful when dealing with data that includes numerous filler words. By summarizing the chunks, we can eliminate these superfluous elements, thereby refining the quality of the input data.
&lt;a id="query-compression"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="applying-query-transformations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="applying-query-transformations"&gt;Applying Query Transformations&lt;/h2&gt;
&lt;p&gt;Query transformations can significantly enhance the quality of responses. For instance, if a system does not find relevant context for a query, the LLM can rephrase the query and try again. See the [[rag_fusion|RAG Fusion]].&lt;/p&gt;
&lt;p&gt;Similarly, the &lt;a href="http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf"&gt;HyDE&lt;/a&gt; strategy generates a hypothetical response to a query and uses both for embedding lookup, which has been found to dramatically enhance performance.&lt;/p&gt;
&lt;p&gt;Another technique involves breaking down complex queries into sub-queries, a process that LLMs tend to handle better. This approach can be integrated into the RAG system to decompose a query into multiple simpler questions.&lt;/p&gt;
&lt;p&gt;&lt;a id="query-compression"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="query-compression"&gt;Query Compression&lt;/h2&gt;
&lt;p&gt;Query compression, (see a tool like &lt;a href="https://www.microsoft.com/en-us/research/project/llmlingua/longllmlingua/"&gt;LongLLMLingua&lt;/a&gt;) is a technique for improving RAG's performance in long context scenarios where large language models often face challenges such as increased computational and financial costs, longer latency, and inferior performance. By enhancing the density and optimizing the position of key information in the input prompt, LongLLMLingua improves LLMs' perception of key information, which in turn, reduces computational load, decreases latency, and improves performance. This strategy ensures that vital information is not lost or diluted in lengthy contexts, thereby enhancing the relevance and quality of the generated output.
&lt;a id="optimal-chunking-strategy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="optimal-chunking-strategy"&gt;Optimal Chunking Strategy&lt;/h2&gt;
&lt;p&gt;There are multiple strategies that can be applied to chunking see &lt;a href="https://safjan.com/from-fixed-size-to-nlp-chunking-a-deep-dive-into-text-chunking-techniques/#from-fixed-size-to-nlp-chunking-a-deep-dive-into-text-chunking-techniques"&gt;Chunking strategies&lt;/a&gt;. One of the aspects can be controlling the chunk overlap. Semantic retrieval may pose a challenge when a selected chunk has meaningful context in adjacent chunks that could be missed. To mitigate this, an overlap of chunks can be implemented, whereby neighboring chunks are also passed to the Language Model (LLM) for generation. This guarantees that the surrounding context is incorporated, thus enhancing the output's quality.&lt;/p&gt;
&lt;p&gt;&lt;a id="fine-tuning-embedding-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="fine-tuning-embedding-models"&gt;Fine-tuning Embedding Models&lt;/h2&gt;
&lt;p&gt;While off-the-shelf embedding models such as BERT and Ada may suffice for many use cases, they might not adequately represent specific domains in the vector space, leading to suboptimal retrieval quality. In such instances, it would be advantageous to fine-tune an embedding model using domain-specific data to significantly improve retrieval quality.&lt;/p&gt;
&lt;p&gt;&lt;a id="enriching-metadata"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="enriching-metadata"&gt;Enriching Metadata&lt;/h2&gt;
&lt;p&gt;The provision of metadata like source information about the chunks being processed can enhance the LLM's comprehension of the context, leading to a better output generation. This additional layer of information can provide the LLM with a more holistic understanding of the data, enabling it to generate more accurate and relevant responses.&lt;/p&gt;
&lt;p&gt;&lt;a id="employing-re-ranking"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="employing-re-ranking"&gt;Employing Re-ranking&lt;/h2&gt;
&lt;p&gt;Semantic search may yield top-k results that are too similar to each other. To ensure a wider array of snippets, it is beneficial to &lt;a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html"&gt;re-rank&lt;/a&gt; the results based on other factors such as metadata and keyword matches. This diversification of snippets can lead to a more nuanced and comprehensive context for the LLM to generate responses. Re-ranker can be based on a cross-encoder.&lt;/p&gt;
&lt;p&gt;&lt;a id="addressing-the-lost-in-the-middle-problem"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="addressing-the-lost-in-the-middle-problem"&gt;Addressing the 'Lost in the Middle' Problem&lt;/h2&gt;
&lt;p&gt;LLMs tend not to assign equal weight to all tokens in the input, often overlooking tokens located in the middle. This phenomenon, known as the &lt;a href="https://arxiv.org/abs/2307.03172"&gt;'lost in the middle' problem&lt;/a&gt;, can be addressed by reordering the context snippets to place the most vital snippets at the beginning and end of the input, with less important snippets situated in the middle.&lt;/p&gt;
&lt;p&gt;&lt;a id="meta-data-filtering"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="meta-data-filtering"&gt;Meta-data Filtering&lt;/h2&gt;
&lt;p&gt;Meta-data, such as date tags, can be added to your chunks to improve retrieval. For example, filtering by recency can be beneficial when querying email history. Recent emails may not necessarily be the most similar from an embedding standpoint, but they are more likely to be relevant.&lt;/p&gt;
&lt;p&gt;&lt;a id="query-routing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="query-routing"&gt;Query Routing&lt;/h2&gt;
&lt;p&gt;Having multiple indexes and routing queries to the appropriate index can be beneficial. For instance, different indexes could handle summarization questions, pointed questions, and date-sensitive questions. Trying to optimize one index for all these behaviors may compromise its effectiveness.&lt;/p&gt;
&lt;p&gt;The performance of RAG in production can be significantly improved by applying a range of techniques, including hybrid search, chunk summarization, overlapping chunks, fine-tuned embedding models, metadata enhancement, re-ranking, addressing the 'lost in the middle' problem, query transformations, meta-data filtering, and query routing. These strategies will help to optimize the RAG pipeline, ensuring higher quality output and improved overall performance.&lt;/p&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://llmstack.ai/blog/retrieval-augmented-generation"&gt;Retrieval Augmented Generation (RAG): What, Why and How? | LLMStack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2307.03172"&gt;[2307.03172] Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c"&gt;10 Ways to Improve the Performance of Retrieval Augmented Generation Systems | by Matt Ambrogi | Sep, 2023 | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hypothetical Document Embeddings (HyDE) - &lt;a href="http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf"&gt;Precise Zero-Shot Dense Retrieval without Relevance Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html"&gt;Retrieve &amp;amp; Re-Rank - Sentence-Transformers documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.llamaindex.ai/improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"&gt;Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT) | by Emanuel Ferreira | Oct, 2023 | LlamaIndex Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/towards-generative-ai/improving-rag-retrieval-augmented-generation-answer-quality-with-re-ranker-55a19931325"&gt;Improving RAG (Retrieval Augmented Generation) Answer Quality with Re-ranker | by Shivam Solanki | Towards Generative AI | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SingleStore (db), finetuning embeddings model, CacheGPT, Nemo-Guardrails, &lt;a href="https://madhukarkumar.medium.com/secrets-to-optimizing-rag-llm-apps-for-better-accuracy-performance-and-lower-cost-da1014127c0a"&gt;Secrets to Optimizing RAG LLM Apps for Better Performance, Accuracy and Lower Costs! | by Madhukar Kumar | madhukarkumar | Sep, 2023 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/run-llama/finetune-embedding"&gt;run-llama/finetune-embedding: Fine-Tuning Embedding for RAG with Synthetic Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zilliztech/GPTCache"&gt;zilliztech/GPTCache: Semantic cache for LLMs. Fully integrated with LangChain and llama_index.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Guardrails"&gt;NVIDIA/NeMo-Guardrails: NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;library to evaluate the context retrieved from your enterprise corpus of data (how do you know if the context being retrieved is accurate) &lt;a href="https://github.com/explodinggradients/ragas"&gt;GitHub - explodinggradients/ragas: Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LangSmith, introduced by LangChain - a highly effective tool for monitoring and examining the responses between the app and the LLM.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2310.15123"&gt;[2310.15123] Branch-Solve-Merge Improves Large Language Model Evaluation and Generation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;X::[[RAG_question_answering_deciding_on_the_strategies_Architecture]]
up::[[MOC_RAG]]&lt;/p&gt;</content><category term="Generative AI"/><category term="machine-learning"/><category term="python"/><category term="rag"/><category term="llm"/><category term="retrieval-augmented-generation"/><category term="re-ranking"/><category term="lost-in-the-middle"/></entry><entry><title>Understanding the Differences in Language Models - Transformers vs. Markov Models</title><link href="http://127.0.0.1:8000/understanding-differences-gpt-transformers-markov-models/" rel="alternate"/><published>2023-10-07T00:00:00+02:00</published><updated>2023-10-07T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-10-07:/understanding-differences-gpt-transformers-markov-models/</id><summary type="html">&lt;p&gt;This article explores distinguishing details of Markov Models and Transformer-based models like GPT, focusing on how they predict the next character in a sequence. It explores the fundamental differences between these models, with a particular emphasis on how the self-attention mechanism in Transformer models makes a difference compared to the fixed context length in Markov models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In Machine Learning (ML) and natural language processing (NLP), different models have been developed to understand and generate human language. Two such models that have gained significant attention are the Markov Models and the Transformer-based models like GPT (&lt;a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer"&gt;Generative Pretrained Transformer&lt;/a&gt;). While both types of models can predict the next character in a sequence, they differ significantly in their underlying mechanisms and capabilities. This article aims to look into the details of these models, with a particular focus on how the self-attention mechanism in Transformer models makes a difference compared to the fixed context length in Markov models.&lt;/p&gt;
&lt;h2 id="markov-models-a-brief-overview"&gt;Markov Models: A Brief Overview&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Markov_model"&gt;Markov Models&lt;/a&gt;, named after the Russian mathematician &lt;a href="https://en.wikipedia.org/wiki/Andrey_Markov"&gt;Andrey Markov&lt;/a&gt;, are a class of models that predict future states based solely on the current state, disregarding all past states. This property is known as the Markov Property, and it is the fundamental assumption that underlies all Markov models.&lt;/p&gt;
&lt;p&gt;In the context of language modeling, a Markov Model might predict the next word or character in a sentence based on the current word or character. For instance, given the word "The", a Markov Model might predict that the next word is "cat" based on the probability distribution of words that follow "The" in its training data.&lt;/p&gt;
&lt;p&gt;The main limitation of Markov Models is their lack of memory. Since they only consider the current state, they are unable to capture long-term dependencies in a sequence. For example, in the sentence "I grew up in France... I speak fluent ...", a Markov Model might struggle to fill in the blank correctly because the relevant context ("France") is several words back.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Markov Chain text generation" src="/images/transformers_vs_markov/markov_model_text_generation.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; &lt;em&gt;Markov Model might predict  the next word based on the probability distribution of words in its training data. Image Source: &lt;a href="https://jaroslawwiosna.github.io/markov-chain-text/"&gt;markov-chain-text | Modern C++ Markov chain text generator&lt;/a&gt; by Jarosław Wiosna&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="transformer-models-an-introduction"&gt;Transformer Models: An Introduction&lt;/h2&gt;
&lt;p&gt;Transformer models, on the other hand, were introduced in the seminal paper &lt;a href="https://arxiv.org/abs/1706.03762"&gt;"Attention is All You Need"&lt;/a&gt; by Vaswani et al. (2017). They represent a significant departure from previous sequence-to-sequence models, eschewing recurrent and convolutional layers in favor of self-attention mechanisms.&lt;/p&gt;
&lt;p&gt;GPT, developed by OpenAI, is a prominent example of a Transformer model. It is a generative model that can generate human-like text by predicting the next word in a sequence. Unlike Markov Models, GPT considers the entire context of a sequence when making predictions, allowing it to capture long-term dependencies.&lt;/p&gt;
&lt;h2 id="the-power-of-self-attention"&gt;The Power of Self-Attention&lt;/h2&gt;
&lt;p&gt;The key innovation of Transformer models is the self-attention mechanism. This mechanism allows the model to weigh the importance of different words in the context when predicting the next word. For instance, in the sentence "The cat, which was black and white, jumped over the ___", the model might assign more importance to "cat" and "jumped" when predicting the next word.&lt;/p&gt;
&lt;p&gt;Self-attention is calculated using the dot product of the query and key vectors, which are learned representations of the input. The resulting attention scores are then used to weight the value vectors, which are also learned representations of the input. This weighted sum forms the output of the self-attention layer.&lt;/p&gt;
&lt;p&gt;The self-attention mechanism allows Transformer models to consider the entire context of a sequence, rather than just the current state. This is a significant advantage over Markov Models, which are limited by their fixed context length.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transformer model - Context and Attention" src="/images/transformers_vs_markov/transformers_context_and_atention.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; &lt;em&gt;The self-attention mechanism allows Transformer models to consider the entire context of a sequence, rather than just the current state. Image Source:  &lt;a href="https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the"&gt;A Deep Dive Into the Transformer Architecture – The Development of Transformer Models&lt;/a&gt; by Kevin Hooke&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="fixed-context-length-vs-variable-context-length"&gt;Fixed Context Length vs. Variable Context Length&lt;/h2&gt;
&lt;p&gt;Markov Models, due to their inherent design, have a fixed context length. They only consider the current state when making predictions, which limits their ability to capture long-term dependencies. This can lead to less accurate predictions, especially in complex sequences where the relevant context might be several states back.&lt;/p&gt;
&lt;p&gt;Transformer models, on the other hand, have a variable context length. Thanks to the self-attention mechanism, they can consider the entire context of a sequence when making predictions. This allows them to capture long-term dependencies and make more accurate predictions.&lt;/p&gt;
&lt;p&gt;Moreover, the self-attention mechanism allows Transformer models to dynamically adjust the context length based on the input. For instance, in a sentence with many irrelevant words, the model might focus on a few key words, effectively reducing the context length. This dynamic context length is another advantage of Transformer models over Markov Models.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While both Markov Models and Transformer models like GPT can predict the next character in a sequence, they differ significantly in their underlying mechanisms and capabilities. Markov Models, with their fixed context length, are limited in their ability to capture long-term dependencies. Transformer models, with their self-attention mechanism, can consider the entire context of a sequence, allowing them to capture long-term dependencies and make more accurate predictions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp;amp; Polosukhin, I. (2017). &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention is all you need&lt;/a&gt;. In Advances in neural information processing systems (pp. 5998-6008).&lt;/li&gt;
&lt;li&gt;Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp;amp; Sutskever, I. (2019). &lt;a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;. OpenAI Blog.&lt;/li&gt;
&lt;li&gt;Bishop, C. M. (2006). &lt;a href="https://www.springer.com/gp/book/9780387310732"&gt;Pattern Recognition and Machine Learning&lt;/a&gt;. Springer.&lt;/li&gt;
&lt;li&gt;Ruder, S. (2019). &lt;a href="http://jalammar.github.io/illustrated-transformer/"&gt;The Illustrated Transformer&lt;/a&gt;. Jay Alammar's Blog.&lt;/li&gt;
&lt;li&gt;Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... &amp;amp; Amodei, D. (2020). &lt;a href="https://arxiv.org/abs/2005.14165"&gt;Language Models are Few-Shot Learners&lt;/a&gt;. In Advances in Neural Information Processing Systems.&lt;/li&gt;
&lt;li&gt;Chollet, F. (2018). &lt;a href="https://www.manning.com/books/deep-learning-with-python"&gt;Deep Learning with Python&lt;/a&gt;. Manning Publications Co.&lt;/li&gt;
&lt;li&gt;Jurafsky, D., &amp;amp; Martin, J. H. (2019). &lt;a href="https://web.stanford.edu/~jurafsky/slp3/"&gt;Speech and Language Processing&lt;/a&gt;. Stanford University.&lt;/li&gt;
&lt;li&gt;Al-Rfou, R., Choe, D., Constant, N., Guo, M., &amp;amp; Jones, L. (2019). &lt;a href="https://arxiv.org/abs/1808.04444"&gt;Character-Level Language Modeling with Deeper Self-Attention&lt;/a&gt;. In Proceedings of the AAAI Conference on Artificial Intelligence.&lt;/li&gt;
&lt;li&gt;Goodfellow, I., Bengio, Y., &amp;amp; Courville, A. (2016). &lt;a href="http://www.deeplearningbook.org/"&gt;Deep Learning&lt;/a&gt;. MIT press.&lt;/li&gt;
&lt;li&gt;Manning, C. D., &amp;amp; Schütze, H. (1999). &lt;a href="https://mitpress.mit.edu/books/foundations-statistical-natural-language-processing"&gt;Foundations of Statistical Natural Language Processing&lt;/a&gt;. MIT Press.&lt;/li&gt;
&lt;li&gt;Jurafsky, D., &amp;amp; Martin, J. H. (2009). &lt;a href="https://www.pearson.com/us/higher-education/program/Jurafsky-Speech-and-Language-Processing-An-Introduction-to-Natural-Language-Processing-Computational-Linguistics-and-Speech-Recognition-2nd-Edition/PGM319216.html"&gt;Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition&lt;/a&gt;. Prentice Hall.&lt;/li&gt;
&lt;li&gt;Jelinek, F. (1997). &lt;a href="https://mitpress.mit.edu/books/statistical-methods-speech-recognition"&gt;Statistical Methods for Speech Recognition&lt;/a&gt;. MIT Press.&lt;/li&gt;
&lt;li&gt;Russell, S., &amp;amp; Norvig, P. (2016). &lt;a href="http://aima.cs.berkeley.edu/"&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt;. Pearson.&lt;/li&gt;
&lt;li&gt;Charniak, E. (1993). &lt;a href="https://mitpress.mit.edu/books/statistical-language-learning"&gt;Statistical Language Learning&lt;/a&gt;. MIT Press.&lt;/li&gt;
&lt;li&gt;Lin, T. (2015). &lt;a href="https://towardsdatascience.com/markov-chains-and-text-generation-162fd4ec8f26"&gt;Markov Chains and Text Generation&lt;/a&gt;. Towards Data Science Blog.&lt;/li&gt;
&lt;li&gt;Goodman, J. (2001). &lt;a href="https://www.microsoft.com/en-us/research/publication/a-bit-of-progress-in-language-modeling/"&gt;A bit of progress in language modeling&lt;/a&gt;. Microsoft Research.&lt;/li&gt;
&lt;li&gt;Rosenfeld, R. (2000). &lt;a href="https://www.cs.cmu.edu/~roni/papers/SLM-hlt01.pdf"&gt;Two Decades of Statistical Language Modeling: Where Do We Go From Here?&lt;/a&gt;. Proceedings of the IEEE.&lt;/li&gt;
&lt;li&gt;Nazarko, K. (2021). &lt;a href="https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e"&gt;Word-level text generation using GPT-2, LSTM and Markov Chain&lt;/a&gt;. Towards Data Science Blog.&lt;/li&gt;
&lt;li&gt;Adyatama, A. (2020). &lt;a href="https://algotech.netlify.app/blog/text-generating-with-markov-chains/"&gt;Text Generation with Markov Chains&lt;/a&gt; Algoritma Technical Blog.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;X::[[transformers_vs_markov_models_illustrations]]&lt;/p&gt;</content><category term="Generative AI"/><category term="machine-learning"/><category term="transformers"/><category term="markov-models"/><category term="attention"/><category term="self-attention"/><category term="natural-language-processing"/><category term="nlp"/><category term="AI"/><category term="deep-learning"/><category term="language-models"/><category term="GPT"/></entry><entry><title>How Agile Can Kill Creativity in Data Science team?</title><link href="http://127.0.0.1:8000/how-agile-can-kill-creativity-in-data-science-team/" rel="alternate"/><published>2023-09-29T00:00:00+02:00</published><updated>2023-09-29T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-09-29:/how-agile-can-kill-creativity-in-data-science-team/</id><summary type="html">&lt;p&gt;Discover the delicate balance between Agile methodologies and imagination in the domain of data science and analytics. Uncover the impact of Agile approaches on creativity within data science teams. Explore how these practices shape the innovative landscape of data science and analytics.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Agile methodologies can provide numerous benefits to data science and analytics teams, such as quicker delivery, enhanced collaboration, and increased customer satisfaction. However, if not implemented effectively, Agile may unintentionally impede creativity in these teams. Here are a few ways Agile can potentially hinder creativity in data science/analytics.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#potential-problems"&gt;Potential problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tight-deadlines-and-sprints"&gt;Tight deadlines and sprints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#focus-on-deliverables"&gt;Focus on deliverables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lack-of-autonomy"&gt;Lack of autonomy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#constant-and-sudden-changes"&gt;Constant and sudden changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#overemphasis-on-standardized-processes"&gt;Overemphasis on standardized processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mitigation"&gt;Mitigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#complementary-practices"&gt;Complementary practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#frameworks-tailored-for-data-science-projects"&gt;Frameworks tailored for data science projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="potential-problems"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="potential-problems"&gt;Potential problems&lt;/h2&gt;
&lt;p&gt;&lt;a id="tight-deadlines-and-sprints"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="tight-deadlines-and-sprints"&gt;Tight deadlines and sprints&lt;/h3&gt;
&lt;p&gt;Agile typically operates on tight timelines with fixed sprints. This can limit the time available for exploration, experimentation, and creative thinking. The emphasis on adhering to strict schedules may discourage innovative approaches that require more time to develop.&lt;/p&gt;
&lt;p&gt;&lt;a id="focus-on-deliverables"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="focus-on-deliverables"&gt;Focus on deliverables&lt;/h3&gt;
&lt;p&gt;Agile methodologies often prioritize delivering functioning solutions over long-term exploration. This focus on short-term goals can discourage team members from taking the time to explore complex problems creatively, resulting in a more practical, rather than innovative, approach.&lt;/p&gt;
&lt;p&gt;&lt;a id="lack-of-autonomy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="lack-of-autonomy"&gt;Lack of autonomy&lt;/h3&gt;
&lt;p&gt;In some Agile implementations, teams may be closely supervised or required to adhere to preset workflows. This kind of micromanagement limits individual creativity, as team members may not have the freedom to experiment, propose alternative solutions, or take calculated risks.&lt;/p&gt;
&lt;p&gt;&lt;a id="constant-and-sudden-changes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="constant-and-sudden-changes"&gt;Constant and sudden changes&lt;/h3&gt;
&lt;p&gt;Agile projects often involve iterative development with frequent changes in priorities and requirements. While this adaptability is beneficial in many cases, it can disrupt the creative process and impede the ability to think deeply about problems. Constantly switching gears may hinder the exploration of unconventional solutions.&lt;/p&gt;
&lt;p&gt;&lt;a id="overemphasis-on-standardized-processes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="overemphasis-on-standardized-processes"&gt;Overemphasis on standardized processes&lt;/h3&gt;
&lt;p&gt;Agile frameworks provide standardized processes and practices that ensure consistency and predictability. While these are essential for efficient project management, a strict adherence to these processes can stifle creativity as it may discourage deviation from the prescribed methods.&lt;/p&gt;
&lt;p&gt;&lt;a id="mitigation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mitigation"&gt;Mitigation&lt;/h2&gt;
&lt;p&gt;&lt;a id="complementary-practices"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="complementary-practices"&gt;Complementary practices&lt;/h3&gt;
&lt;p&gt;To prevent the potential negative impact on creativity, Agile methodologies should be complemented with the following practices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow dedicated time for &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;learning&lt;/strong&gt; outside of fixed sprints.&lt;/li&gt;
&lt;li&gt;Encourage &lt;strong&gt;cross-functional collaboration&lt;/strong&gt; and knowledge sharing to foster creativity.&lt;/li&gt;
&lt;li&gt;Provide opportunities for &lt;strong&gt;innovation-driven initiatives&lt;/strong&gt; alongside project-driven ones.&lt;/li&gt;
&lt;li&gt;Support a &lt;strong&gt;psychologically safe environment&lt;/strong&gt; that allows for experimentation and failure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recognize&lt;/strong&gt; and reward &lt;strong&gt;creative thinking&lt;/strong&gt; and experimentation within the team.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="frameworks-tailored-for-data-science-projects"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="frameworks-tailored-for-data-science-projects"&gt;Frameworks tailored for data science projects&lt;/h3&gt;
&lt;p&gt;The data science teams need to adapt Agile practices to suit their specific needs and contexts, and to balance the trade-offs between speed, flexibility, and quality. Data science teams can adopt or modify a framework that is tailored for data science projects, such as the &lt;strong&gt;&lt;a href="https://learn.microsoft.com/en-us/azure/architecture/data-science-process/overview"&gt;Team Data Science Process&lt;/a&gt;&lt;/strong&gt; (TDSP) or the &lt;strong&gt;&lt;a href="https://www.datascience-pm.com/agile-data-science/"&gt;Agile Data Science Process&lt;/a&gt;&lt;/strong&gt; These frameworks provide guidance on how to structure, execute, and manage data science projects using Agile principles and practices.&lt;/p&gt;
&lt;p&gt;By adjusting Agile practices to accommodate these considerations, data science/analytics teams can create a balance between efficient project management and fostering creativity and innovation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/azure/architecture/data-science-process/overview"&gt;What is the Team Data Science Process? - Azure Architecture Center | Microsoft Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datascience-pm.com/agile-data-science/"&gt;Agile Data Science - Data Science Process Alliance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://eugeneyan.com/writing/data-science-and-agile-what-works-and-what-doesnt/"&gt;Data Science and Agile (What Works, and What Doesn't)&lt;/a&gt; (read about poor resource planning)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.geeksforgeeks.org/agile-methodology-advantages-and-disadvantages/"&gt;Agile Methodology Advantages and Disadvantages - GeeksforGeeks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Machine Learning"/><category term="data-science"/><category term="agile"/><category term="scrum"/><category term="SAFe"/><category term="sprint"/><category term="deliverables"/><category term="process"/><category term="exploration"/><category term="collaboration"/><category term="creativity"/><category term="creative-thinking"/><category term="agile-data-science-process"/><category term="team-data-science-process"/></entry><entry><title>From Fixed-Size to NLP Chunking - A Deep Dive into Text Chunking Techniques</title><link href="http://127.0.0.1:8000/from-fixed-size-to-nlp-chunking-a-deep-dive-into-text-chunking-techniques/" rel="alternate"/><published>2023-09-11T00:00:00+02:00</published><updated>2023-11-06T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-09-11:/from-fixed-size-to-nlp-chunking-a-deep-dive-into-text-chunking-techniques/</id><summary type="html">&lt;p&gt;Discover text chunking - the secret sauce behind accurate search results and smarter language models! By understanding how to effectively chunk text, we can improve the way we index documents, handle user queries, and utilize search results. Ready to uncover the secrets of text chunking?&lt;/p&gt;</summary><content type="html">&lt;h2 id="understanding-chunking"&gt;Understanding Chunking&lt;/h2&gt;
&lt;p&gt;Chunking is a process that aims to embed a piece of content with as little noise as possible while maintaining semantic relevance[^2]. This process is particularly useful in semantic search, where we index a corpus of documents, each containing valuable information on a specific topic.&lt;/p&gt;
&lt;p&gt;An effective chunking strategy ensures that search results accurately capture the essence of a user's query. If our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to surface relevant content. As a &lt;strong&gt;rule of thumb&lt;/strong&gt;, if the &lt;strong&gt;chunk of text makes sense without the surrounding context to a human&lt;/strong&gt;, it will likely make sense to the language model as well[^2]. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#factors-influencing-chunking-strategy"&gt;Factors Influencing Chunking Strategy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#size-of-the-texts-to-be-indexed"&gt;Size of the Texts to be Indexed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#length-and-complexity-of-user-queries"&gt;Length and Complexity of User Queries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#utilization-of-the-retrieved-results-in-the-application"&gt;Utilization of the Retrieved Results in the Application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#chunking-methods"&gt;Chunking Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fixed-size-in-characters-overlapping-sliding-window"&gt;Fixed-size (in characters) Overlapping Sliding Window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fixed-size-in-tokens-overlapping-sliding-window"&gt;Fixed-size (in tokens) Overlapping Sliding Window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recursive-structure-aware-splitting"&gt;Recursive Structure Aware Splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#structure-aware-splitting-by-sentence-paragraph-section-chapter"&gt;Structure Aware Splitting (by Sentence, Paragraph, Section, Chapter)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nlp-chunking-tracking-topic-changes"&gt;NLP Chunking: Tracking Topic Changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#content-aware-splitting-markdown-latex-html"&gt;Content-Aware Splitting (Markdown, LaTeX, HTML)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adding-extra-context-to-the-chunk-metadata-summaries"&gt;Adding Extra Context to the Chunk (metadata, summaries)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#further-reading"&gt;Further Reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="factors-influencing-chunking-strategy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="factors-influencing-chunking-strategy"&gt;Factors Influencing Chunking Strategy&lt;/h2&gt;
&lt;p&gt;There are three main factors to consider when determining a chunking strategy for a specific use case and application:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The size of the texts to be indexed and chunked&lt;/li&gt;
&lt;li&gt;The length and complexity of user queries&lt;/li&gt;
&lt;li&gt;The utilization of the retrieved results in the application&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="size-of-the-texts-to-be-indexed"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="size-of-the-texts-to-be-indexed"&gt;Size of the Texts to be Indexed&lt;/h3&gt;
&lt;p&gt;The chunking unit and size should be adjusted according to the nature of the text. The chunk should be long enough to contain the relevant semantic load. For instance, individual words may not convey a specific message or piece of information, while putting an entire encyclopedia in one chunk may result in a chunk that is "about everything."&lt;/p&gt;
&lt;p&gt;&lt;a id="length-and-complexity-of-user-queries"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="length-and-complexity-of-user-queries"&gt;Length and Complexity of User Queries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Longer queries&lt;/strong&gt; or those with greater complexity typically &lt;strong&gt;benefit from a smaller chunk length&lt;/strong&gt;. This helps to narrow down the search space and improve the precision of the search results. Smaller chunks allow more focused matching against embeddings, reducing the impact of irrelevant parts within the query.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shorter and simpler queries&lt;/strong&gt; might not require chunking at all, as they can be processed as a single unit. Chunking may introduce unnecessary overhead in these cases, potentially hampering search performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="utilization-of-the-retrieved-results-in-the-application"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="utilization-of-the-retrieved-results-in-the-application"&gt;Utilization of the Retrieved Results in the Application&lt;/h3&gt;
&lt;p&gt;In cases where search results are only an intermediate step in the whole chain in the app, the size of the chunk might have significant importance for the seamless operation of the application. For example, if results from multiple search queries are the input context for the prompt to the LLM, having small chunks might ease fitting all inputs in the maximum allowed context size for a given LLM. Conversely, if the search result is presented to the user, larger chunks may be more appropriate.&lt;/p&gt;
&lt;p&gt;&lt;a id="chunking-methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="chunking-methods"&gt;Chunking Methods&lt;/h2&gt;
&lt;p&gt;There are several methods for chunking text, each with its own advantages and disadvantages. The choice of method depends on the specific requirements of the use case and application.&lt;/p&gt;
&lt;p&gt;&lt;a id="fixed-size-in-characters-overlapping-sliding-window"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="fixed-size-in-characters-overlapping-sliding-window"&gt;Fixed-size (in characters) Overlapping Sliding Window&lt;/h3&gt;
&lt;p&gt;The Fixed-size overlapping sliding window method is a naive approach to text chunking, dividing the text into fixed-size pieces regarded as chunks. In this method, the text is divided based on the count of characters, making it straightforward to implement. The use of overlap in this method aids in preserving the integrity of sentences or thoughts, ensuring they are not cut in the middle. If one window truncates a thought, another window might contain the complete thought.&lt;/p&gt;
&lt;p&gt;However, this method presents certain limitations. One significant drawback is the lack of precise control over the context size. Most language models operate on the basis of tokens rather than characters or words, making this method less efficient. The strict and fixed-size nature of the window might also result in severing words, sentences, or paragraphs in the middle, which could impede comprehension and disrupt the flow of information.&lt;/p&gt;
&lt;p&gt;Furthermore, this method does not take semantics into account, providing no guarantee that the semantic unit of the text capturing a given idea or thought will be accurately encapsulated within a chunk. Consequently, one chunk may not be semantically distinct from another.&lt;/p&gt;
&lt;h4 id="use-cases"&gt;Use Cases&lt;/h4&gt;
&lt;p&gt;The Fixed-size overlapping sliding window method can be beneficial in certain scenarios. It is especially useful in preliminary exploratory data analysis, where the goal is to obtain a general understanding of the text structure rather than a deep semantic analysis. Additionally, it could be employed in scenarios where the text data does not have a strong semantic structure, such as in certain types of raw data or logs.&lt;/p&gt;
&lt;p&gt;However, for tasks that require semantic understanding and precise context, such as sentiment analysis, question-answering systems, or text summarization, more sophisticated text chunking methods would be more appropriate.&lt;/p&gt;
&lt;h4 id="summary"&gt;Summary&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Counting characters makes implementation easy&lt;/li&gt;
&lt;li&gt;Using overlap helps to avoid having sentences or thoughts cut in the middle - if one window is cutting the thought, perhaps another will have it in one piece.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not precise control of the context size - models work and size the text in tokens not in characters or words&lt;/li&gt;
&lt;li&gt;Having a strict, fixed-size window might lead to cutting words, sentences, or paragraphs in the middle.&lt;/li&gt;
&lt;li&gt;Doesn't take semantics into account, no guarantee that the semantic unit of text capturing the given idea, thought will be accurately captured in the chunk and another chunk will be dedicated to another idea&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Use cases:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preliminary exploratory data analysis where a general understanding of the text is required&lt;/li&gt;
&lt;li&gt;Scenarios where the text does not have a strong semantic structure, such as certain types of raw data or logs&lt;/li&gt;
&lt;li&gt;Not recommended for tasks requiring semantic understanding and precise contexts like sentiment analysis, question-answering systems, or text summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="fixed-size-in-tokens-overlapping-sliding-window"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="fixed-size-in-tokens-overlapping-sliding-window"&gt;Fixed-size (in tokens) Overlapping Sliding Window&lt;/h3&gt;
&lt;p&gt;The Fixed-size sliding window method in tokens is another approach to text chunking. Unlike the character-based method, this approach divides the text into chunks based on the count of tokens that came out from the tokenizer, making it more aligned with the way language models operate.&lt;/p&gt;
&lt;p&gt;In this method, the size of the context is more precisely controlled as it works on tokens rather than characters. A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This can make avoiding cutting words in the middle a little better than when counting characters, but the problem still persists. It can still sever sentences or thoughts in the middle, which could disrupt the flow of information. Moreover, similar to the character-based method, this approach does not take semantics into account. There's no guarantee that a chunk accurately captures a unique thought or idea, making the chunks potentially semantically inconsistent.&lt;/p&gt;
&lt;h4 id="where-to-use-it"&gt;Where to Use It&lt;/h4&gt;
&lt;p&gt;The use cases are similar to the fixed size window based on characters count with one difference - when the count is based on tokens it works better for the tasks where we are limited by the LLM context size.&lt;/p&gt;
&lt;h4 id="summary_1"&gt;Summary&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More precise control over LLM context size as it operates on tokens, not characters.&lt;/li&gt;
&lt;li&gt;Still, relatively easy to implement&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can still sever sentences or thoughts in the middle&lt;/li&gt;
&lt;li&gt;Does not take semantics into account, hence no guarantee that a chunk accurately captures a unique thought or idea&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Use cases:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For exploratory, initial work with LLMs&lt;/li&gt;
&lt;li&gt;Not recommended for tasks requiring a deep understanding of the semantics and context of the text, like sentiment analysis or text summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="recursive-structure-aware-splitting"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="recursive-structure-aware-splitting"&gt;Recursive Structure Aware Splitting&lt;/h3&gt;
&lt;p&gt;Recursive Structure-aware Aware Splitting is a hybrid approach to text chunking, combining elements of the fixed-size sliding window method and the structure-aware splitting method. This method attempts to create chunks of approximately fixed sizes, either in characters or tokens, while also trying to preserve the original units of text such as words, sentences, or paragraphs.&lt;/p&gt;
&lt;p&gt;In this method, the text is recursively split using various separators such as paragraph breaks ("\n\n"), new lines ("\n"), or spaces (" "), moving to the next level of granularity only when necessary. This allows the method to balance the need for a fixed chunk size with the desire to respect the natural linguistic boundaries of the text.&lt;/p&gt;
&lt;p&gt;The major advantage of this method is its flexibility. It provides more precise control over context size compared to fixed-size methods, while also ensuring that semantic units of text are not unnecessarily severed.&lt;/p&gt;
&lt;p&gt;However, this method also has its drawbacks. The complexity of implementation is higher due to the recursive nature of the splitting. There's also the risk of ending up with chunks of highly variable sizes, especially with texts of varying structural complexity.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: &lt;a href="https://www.langchain.com/"&gt;LangChain&lt;/a&gt; has an implementation of &lt;a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"&gt;Recursively split&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="where-to-use-it_1"&gt;Where to Use It&lt;/h4&gt;
&lt;p&gt;Recursive Structure Aware Splitting is particularly useful in tasks where both the granularity of tokens and the preservation of semantic integrity are crucial. This includes tasks such as text summarization, sentiment analysis, and document classification.&lt;/p&gt;
&lt;p&gt;However, due to its complexity, it might not be the best fit for tasks that require quick and simple text chunking, or for tasks involving texts with inconsistent or unclear structural divisions.&lt;/p&gt;
&lt;h4 id="summary_2"&gt;Summary&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balances the need for fixed chunk sizes with the preservation of natural linguistic boundaries&lt;/li&gt;
&lt;li&gt;Provides more precise control over the context size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Higher complexity of implementation due to the recursive nature of the splitting&lt;/li&gt;
&lt;li&gt;Risk of ending up with chunks of highly variable sizes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Use cases:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Useful in tasks where both the granularity of tokens and the preservation of semantic integrity are crucial, such as text summarization, sentiment analysis, and document classification&lt;/li&gt;
&lt;li&gt;Not recommended for tasks requiring quick and simple text chunking, or tasks involving texts with inconsistent or unclear structural divisions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="structure-aware-splitting-by-sentence-paragraph-section-chapter"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="structure-aware-splitting-by-sentence-paragraph-section-chapter"&gt;Structure Aware Splitting (by Sentence, Paragraph, Section, Chapter)&lt;/h3&gt;
&lt;p&gt;Structure Aware Splitting is an advanced approach to text chunking, which takes into account the inherent structure of the text. Instead of using a fixed-size window, this method divides the text into chunks based on its natural divisions such as sentences, paragraphs, sections, or chapters.&lt;/p&gt;
&lt;p&gt;This method is particularly beneficial as it respects the natural linguistic boundaries of the text, ensuring that words, sentences, and thoughts are not cut in the middle. This aids in preserving the semantic integrity of the information within each chunk.&lt;/p&gt;
&lt;p&gt;However, this method does have certain limitations. Handling text of varying structural complexity might be challenging. For instance, some texts might not have clearly defined sections or chapters, e.g. text extracted from the OCR output, unformatted speech-to-text outputs, text extracted from tables. Also, while it's more semantically aware than the fixed-size methods, it still doesn't guarantee perfect semantic consistency within chunks, especially for larger structural units like sections or chapters.&lt;/p&gt;
&lt;h4 id="where-to-use-it_2"&gt;Where to Use It&lt;/h4&gt;
&lt;p&gt;Structure Aware Splitting is highly effective for tasks that require a good understanding of the context and semantics of the text. It is particularly useful for text summarization, sentiment analysis, and document classification tasks.&lt;/p&gt;
&lt;p&gt;However, it might not be the best fit for tasks involving texts that lack defined structural divisions, or for tasks that require a finer granularity, such as word-level Named Entity Recognition (NER).&lt;/p&gt;
&lt;h4 id="summary_3"&gt;Summary&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Respects natural linguistic boundaries, avoiding severing words, sentences, or thoughts&lt;/li&gt;
&lt;li&gt;Preserves the semantic integrity of information within each chunk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Challenging to handle text with varying structural complexity&lt;/li&gt;
&lt;li&gt;Does not guarantee perfect semantic consistency within chunks, especially for larger structural units&lt;/li&gt;
&lt;li&gt;We don't have control over chunk size. Chunks from given document might significantly vary in the size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Use cases:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Effective for tasks requiring good understanding of context and semantics, such as text summarization, sentiment analysis, and document classification&lt;/li&gt;
&lt;li&gt;Not recommended for tasks involving texts that lack defined structural divisions, or tasks needing finer granularity, like word-level NER&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="nlp-chunking-tracking-topic-changes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="nlp-chunking-tracking-topic-changes"&gt;NLP Chunking: Tracking Topic Changes&lt;/h3&gt;
&lt;p&gt;NLP Chunking with Topic Tracking is a sophisticated approach to text chunking. This method divides the text into chunks based on semantic understanding, specifically by detecting significant shifts in the topics of sentences. If the topic of a sentence significantly differs from the topic of the previous chunk, this sentence is considered the beginning of a new chunk.&lt;/p&gt;
&lt;p&gt;This method has the distinct advantage of maintaining semantic consistency within each chunk. By tracking the changes in topics, this method ensures that each chunk is semantically distinct from the others, thereby capturing the inherent structure and meaning of the text.&lt;/p&gt;
&lt;p&gt;However, this method is not without its challenges. It requires advanced NLP techniques to accurately detect topic shifts, which adds to the complexity of implementation. Additionally, the accuracy of chunking heavily depends on the effectiveness of the topic modeling and detection techniques used.&lt;/p&gt;
&lt;h4 id="where-to-use-it_3"&gt;Where to Use It&lt;/h4&gt;
&lt;p&gt;NLP Chunking with Topic Tracking is highly effective for tasks that require an understanding of the semantic context and topic continuity. It is particularly useful for text summarization, sentiment analysis, and document classification tasks.&lt;/p&gt;
&lt;p&gt;This method might not be the best fit for tasks involving texts that have a high degree of topic overlap or for tasks that require simple text chunking without the need for deep semantic understanding.&lt;/p&gt;
&lt;h4 id="summary_4"&gt;Summary&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Maintains semantic consistency within each chunk&lt;/li&gt;
&lt;li&gt;Captures the inherent structure and meaning of the text by tracking topic changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires advanced NLP techniques, increasing the complexity of implementation&lt;/li&gt;
&lt;li&gt;The accuracy of chunking heavily depends on the effectiveness of the topic modeling and detection techniques used&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Use cases:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Highly effective for tasks requiring semantic context and topic continuity, such as text summarization, sentiment analysis, and document classification&lt;/li&gt;
&lt;li&gt;Not recommended for tasks involving texts with high degrees of topic overlap or tasks requiring simple text chunking without the need for deep semantic understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="content-aware-splitting-markdown-latex-html"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="content-aware-splitting-markdown-latex-html"&gt;Content-Aware Splitting (Markdown, LaTeX, HTML)&lt;/h3&gt;
&lt;p&gt;Content-Aware Splitting is a method of text chunking that focuses on the type and structure of the content, particularly in structured documents like those written in Markdown, LaTeX, or HTML. This method identifies and respects the inherent structure and divisions of the content, such as headings, code blocks, and tables, to create distinct chunks.&lt;/p&gt;
&lt;p&gt;The primary advantage of this method is that it ensures different types of content are not mixed within a single chunk. For instance, a chunk containing a code block will not also contain a part of a table. This helps maintain the integrity and context of the content within each chunk.&lt;/p&gt;
&lt;p&gt;However, this method also presents certain challenges. It requires understanding and parsing the specific syntax of the structured document format, which can increase the complexity of implementation. Moreover, it might not be suitable for documents that lack clear structural divisions or those written in plain text without any specific format.&lt;/p&gt;
&lt;h4 id="where-to-use-it_4"&gt;Where to Use It&lt;/h4&gt;
&lt;p&gt;Content Aware Splitting is especially useful when dealing with structured documents or content with clear formatting, such as technical documentation, academic papers, or web pages. It helps ensure that the chunks created are meaningful and contextually consistent.&lt;/p&gt;
&lt;p&gt;However, this method might not be the best fit for unstructured or plain text documents, or for tasks that do not require a deep understanding of the content structure.&lt;/p&gt;
&lt;h4 id="summary_5"&gt;Summary&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensures different types of content are not mixed within a single chunk&lt;/li&gt;
&lt;li&gt;Respects and maintains the integrity and context of the content within each chunk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires understanding and parsing the specific syntax of the structured document format&lt;/li&gt;
&lt;li&gt;Might not be suitable for unstructured or plain text documents&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Where to Use It:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Particularly useful for structured documents or content with clear formatting, such as technical documentation, academic papers, or web pages&lt;/li&gt;
&lt;li&gt;Not recommended for unstructured or plain text documents, or tasks that do not require a deep understanding of the content structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="adding-extra-context-to-the-chunk-metadata-summaries"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="adding-extra-context-to-the-chunk-metadata-summaries"&gt;Adding Extra Context to the Chunk (metadata, summaries)&lt;/h3&gt;
&lt;p&gt;Adding extra context to the chunks in the form of metadata or summaries can significantly enhance the value of each chunk and improve the overall understanding of the text[^3]. Here are two strategies:&lt;/p&gt;
&lt;h4 id="adding-metadata-to-each-chunk"&gt;Adding Metadata to Each Chunk&lt;/h4&gt;
&lt;p&gt;This strategy involves adding relevant metadata to each chunk. Metadata could include information such as the source of the text, the author, the date of publication, or even data about the content of the chunk itself, like its topic or keywords. This extra context can provide valuable insights and make the chunks more meaningful and easier to analyze.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: In the case of the chunks that are vectorized using text embeddings,  be aware, that vector databases typically allow storage of metadata alongside the embedding vectors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides additional information about each chunk&lt;/li&gt;
&lt;li&gt;Enhances the value of each chunk, making them more meaningful and easier to analyze&lt;/li&gt;
&lt;li&gt;Can help to produce more effective embeddings by fixing the broader context for the chunk.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires additional processing to generate and attach the metadata&lt;/li&gt;
&lt;li&gt;The usefulness of the metadata depends on its relevance and accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Where to Use It:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Especially useful in tasks that involve analyzing the origin, authorship, or content of the chunks, such as text classification, document clustering, or information retrieval&lt;/li&gt;
&lt;li&gt;Can be used to filter the sources used to provide context to LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can get intuition what is possible by reading llama_index documentation on metadata extraction and usage: &lt;a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor.html"&gt;Metadata Extraction Usage Pattern - LlamaIndex 🦙 0.9.30&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="passing-on-chunk-summaries"&gt;Passing on Chunk Summaries&lt;/h4&gt;
&lt;p&gt;In this strategy, each chunk is summarized, and that summary is passed on to the next chunk. This method provides a 'running context' that can enhance the understanding of the text and maintain the continuity of information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enhances the understanding of the text by maintaining a running context&lt;/li&gt;
&lt;li&gt;Helps to ensure the continuity of information across chunks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires advanced NLP techniques to generate accurate and meaningful summaries&lt;/li&gt;
&lt;li&gt;The effectiveness of this method depends on the quality of the summaries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Where to Use It:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Particularly useful in tasks where understanding the continuity and context of the text is crucial, such as text summarization or reading comprehension tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="other-experimental-strategies-for-adding-context-to-the-chunks"&gt;Other Experimental Strategies for Adding Context to the Chunks&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Keyword Tagging:&lt;/strong&gt; This method involves identifying and tagging the most important keywords or phrases in each chunk. These tags then serve as a quick reference or summary of the chunk's content. Advanced NLP techniques can be used to identify these keywords based on their relevance and frequency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sentiment Analysis:&lt;/strong&gt; For text that contains opinions or reviews, performing sentiment analysis on each chunk and attaching the sentiment score (positive, negative, neutral) as metadata can provide valuable context. This can be particularly useful in tasks such as customer feedback analysis or social media monitoring.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Entity Recognition:&lt;/strong&gt; Applying Named Entity Recognition (NER) techniques to each chunk can identify and label entities such as names of people, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. This entity information can be added to each chunk, providing valuable context, especially in tasks like information extraction or knowledge graph construction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Topic Classification:&lt;/strong&gt; Each chunk can be classified into one or more topics using machine learning or NLP techniques. This topic label can provide a quick understanding of what each chunk is about, adding valuable context, especially for tasks like document classification or recommendation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chunk Linking:&lt;/strong&gt; This method involves creating links between related chunks based on shared keywords, entities, or topics. These links can provide a 'map' of the content, showing how different chunks relate to each other. This can be particularly useful in tasks involving large and complex texts, where understanding the overall structure and relations between different parts is important.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="conclusions"&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;In the field of Natural Language Processing, text chunking emerges as a powerful technique that significantly enhances the performance of semantic search and language models. By breaking down text into manageable, contextually relevant chunks, we can ensure more accurate and meaningful search results.&lt;/p&gt;
&lt;p&gt;The choice of chunking method, whether it's fixed-size, structure-aware, or NLP chunking, depends on the specific requirements of the use case and application. Each method has its own strengths and limitations, and understanding these is crucial to implementing an effective chunking strategy.&lt;/p&gt;
&lt;p&gt;Moreover, adding extra context to the chunks, such as metadata or summaries, can further enhance the value of each chunk and improve the overall understanding of the text. Experimental strategies like keyword tagging, sentiment analysis, entity recognition, topic classification, and chunk linking offer promising avenues for further exploration.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;
&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[^1] &lt;a href="https://blog.abacus.ai/blog/2023/08/10/create-your-custom-chatgpt-pick-the-best-llm-that-works-for-you/"&gt;Create a CustomGPT And Supercharge your Company with AI  -  Pick the Best LLM - The Abacus.AI Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[^2] &lt;a href="https://www.pinecone.io/learn/chunking-strategies/"&gt;Chunking Strategies for LLM Applications | Pinecone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[^3] &lt;a href="https://actalyst.medium.com/optimize-llm-enterprise-applications-through-embeddings-and-chunking-strategy-1bbdb03bedae"&gt;Optimize LLM Enterprise Applications through Embeddings and Chunking Strategy. | by Actalyst | Aug, 2023 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[^4] &lt;a href="https://vectara.com/grounded-generation-done-right-chunking/"&gt;Retrieval Augmented Generation (RAG) Done Right: Chunking - Vectara&lt;/a&gt; (NLP chunking, compare chunking strategies) + &lt;a href="https://github.com/vectara/example-notebooks/blob/main/notebooks/chunking-demo.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="further-reading"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="further-reading"&gt;Further Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/aimonks/simple-guide-to-text-chunking-for-your-llm-applications-bddfe8ad7892"&gt;Simple guide to Text Chunking for Your LLM Applications | by NoCode AI | 𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2307.03172"&gt;[2307.03172] Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://community.openai.com/t/the-length-of-the-embedding-contents/111471"&gt;The length of the embedding contents - API - OpenAI Developer Forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1"&gt;Building RAG-based LLM Applications for Production (Part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;expanding context, hierarchical search, ...: &lt;a href="https://reframe.is/wiki/Effects-of-Chunk-Sizes-on-Retrieval-Augmented-Generation-RAG-Applications-8b728c36d005434dba39ad19be9b82cc/"&gt;Effects of Chunk Sizes on Retrieval Augmented Generation (RAG) Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dl.acm.org/doi/10.1007/s10579-013-9250-3"&gt;A novel method for performance evaluation of text chunking | Language Resources and Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mattambrogi.com/posts/chunk-size-matters/"&gt;Matt Ambrogi&lt;/a&gt; "Chunk Size Matters"&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"&gt;Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex | by Ravi Theja | Oct, 2023 | LlamaIndex Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;short (4min 25s) overview of chunking methods from Weaviate: &lt;a href="https://www.youtube.com/watch?v=h5id4erwD4s"&gt;Chunking Methods to use Custom Data with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=8OJC21T2SL4"&gt;The 5 Levels Of Text Splitting For Retrieval - YouTube&lt;/a&gt; (Fixed Size Chunking,  Recursive Chunking, Document Based Chunking, &lt;strong&gt;Semantic Chunking&lt;/strong&gt;, Agentic Chunking - chunking strategy that explore the possibility to use LLM to determine how much and what text should be included in a chunk based on the context.) + &lt;a href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visualization of chunking - &lt;a href="https://chunkviz.up.railway.app/"&gt;ChunkViz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.blog/2024/06/06/breaking-up-is-hard-to-do-chunking-in-rag-applications/"&gt;Breaking up is hard to do: Chunking in RAG applications - Stack Overflow&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;size matters: “You are going to compare that with an embedding of your content. If the size of the content that you're embedding is wildly different from the size of the user's query, you're going to have a higher chance of getting a lower similarity score.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2023-11-06: Added reference: Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex&lt;/li&gt;
&lt;li&gt;2023-11-13: Added video from Weaviate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;X::[[RAG_question_answering_deciding_on_the_strategies_Architecture]]
X::[[context_generation_strategy]]
X::[[2023-10-13-chunking_strategy]]
X::[[2023-10-13-embedding_strategy]]
X::[[RAG_evaluation]]
X::[[RAG_chunking]]&lt;/p&gt;</content><category term="Machine Learning"/><category term="chunking"/><category term="text-processing"/><category term="nlp"/><category term="semantic-search"/><category term="language-models"/><category term="fixed-size-chunking"/><category term="recursive-structure-aware-splitting"/><category term="structure-aware-splitting"/><category term="nlp-chunking"/><category term="content-aware-splitting"/><category term="metadata"/><category term="summaries"/><category term="keyword-tagging"/><category term="sentiment-analysis"/><category term="entity-recognition"/><category term="topic-classification"/><category term="chunk-linking"/><category term="llm"/><category term="rag"/></entry><entry><title>Problems with Langchain and how to minimize their impact</title><link href="http://127.0.0.1:8000/problems-with-Langchain-and-how-to-minimize-their-impact/" rel="alternate"/><published>2023-09-01T00:00:00+02:00</published><updated>2023-10-19T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-09-01:/problems-with-Langchain-and-how-to-minimize-their-impact/</id><summary type="html">&lt;p&gt;Beyond the Hype - LangChain's Hidden Flaws and How to Master AI Development.&lt;/p&gt;</summary><content type="html">&lt;p&gt;X::[[langchain_alternatives]]&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.langchain.com/docs/"&gt;LangChain&lt;/a&gt;, a popular framework for building applications with &lt;a href="https://en.wikipedia.org/wiki/Large_language_model"&gt;large language models&lt;/a&gt; (LLMs), has been touted as a game-changer in the world of AI-driven development. However, as more users dive into the library and its capabilities, some have found that it falls short of expectations. In this section, we'll discuss ten issues with LangChain that have left users underwhelmed and questioning its value proposition.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#problems"&gt;Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-overly-complex-and-unnecessary-abstractions"&gt;1. Overly complex and unnecessary abstractions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-easy-breakable-and-unreliable"&gt;2. Easy breakable and unreliable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-poor-documentation"&gt;3. Poor documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-a-high-level-of-abstraction-hinders-customization"&gt;4. A high level of abstraction hinders customization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-inefficient-token-usage"&gt;5. Inefficient token usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-difficult-integration-with-existing-tools"&gt;6. Difficult integration with existing tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-limited-value-proposition"&gt;7. Limited value proposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-inconsistent-behavior-and-hidden-details"&gt;8. Inconsistent behavior and hidden details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-better-alternatives-available"&gt;9. Better alternatives available&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#10-primarily-optimized-for-demos"&gt;10. Primarily optimized for demos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#takeaways---how-to-use-the-langchain-right-way"&gt;Takeaways - How to Use the LangChain Right Way?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="problems"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="problems"&gt;Problems&lt;/h2&gt;
&lt;p&gt;&lt;a id="1-overly-complex-and-unnecessary-abstractions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-overly-complex-and-unnecessary-abstractions"&gt;1. Overly complex and unnecessary abstractions&lt;/h3&gt;
&lt;p&gt;LangChain has been criticized for having too many layers of abstraction, making it difficult to understand and modify the underlying code. These layers can lead to confusion, especially for those who are new to LLMs or LangChain itself. The complexity can also make it challenging to adapt the library to specific use cases or integrate it with existing tools and scripts. In some cases, users have found that they can achieve their goals more easily by using simpler, more straightforward code.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-easy-breakable-and-unreliable"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-easy-breakable-and-unreliable"&gt;2. Easy breakable and unreliable&lt;/h3&gt;
&lt;p&gt;Some users have found LangChain to be unreliable and difficult to fix due to its complex structure. The framework's fragility can lead to unexpected issues in production systems, making it challenging to maintain and scale applications built with LangChain. Users have reported that the deeper and more complex their application becomes, the more LangChain seems to become a risk to its maintainability.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-poor-documentation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-poor-documentation"&gt;3. Poor documentation&lt;/h3&gt;
&lt;p&gt;LangChain's documentation has been described as confusing and lacking in key details, making it challenging for users to fully understand the library's capabilities and limitations. The documentation often omits explanations of default parameters and important details, leaving users to piece together information from various sources. This lack of clarity can hinder users' ability to effectively leverage LangChain in their projects.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-a-high-level-of-abstraction-hinders-customization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-a-high-level-of-abstraction-hinders-customization"&gt;4. A high level of abstraction hinders customization&lt;/h3&gt;
&lt;p&gt;Users have reported that LangChain's high level of abstraction makes it difficult to modify and adapt the library for specific use cases. This can be particularly problematic when users want to make small changes to the default behavior of LangChain or integrate it with other tools and scripts. In these cases, users may find it easier to bypass LangChain altogether and build their own solutions from scratch.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-inefficient-token-usage"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-inefficient-token-usage"&gt;5. Inefficient token usage&lt;/h3&gt;
&lt;p&gt;LangChain has been criticized for inefficient token usage in its API calls, which can result in higher costs. This can be particularly problematic for users who are trying to minimize their expenses while working with LLMs. Some users have found that they can achieve better results with fewer tokens by using custom Python code or other alternative libraries.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-difficult-integration-with-existing-tools"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-difficult-integration-with-existing-tools"&gt;6. Difficult integration with existing tools&lt;/h3&gt;
&lt;p&gt;Users have reported difficulties integrating LangChain with their existing Python tools and scripts. This can be especially challenging for those who have complex analytics or other advanced functionality built into their applications. The high level of abstraction in LangChain can make it difficult to interface with these existing tools, forcing users to build workarounds or abandon LangChain in favor of more compatible solutions.&lt;/p&gt;
&lt;p&gt;&lt;a id="7-limited-value-proposition"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="7-limited-value-proposition"&gt;7. Limited value proposition&lt;/h3&gt;
&lt;p&gt;Some users feel that LangChain does not provide enough value compared to the effort required to implement and maintain it. They argue that the library's primary use case is to quickly create demos or prototypes, rather than building production-ready applications. In these cases, users may find it more efficient to build their own solutions or explore alternative libraries that offer a better balance of ease of use and functionality.&lt;/p&gt;
&lt;p&gt;&lt;a id="8-inconsistent-behavior-and-hidden-details"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="8-inconsistent-behavior-and-hidden-details"&gt;8. Inconsistent behavior and hidden details&lt;/h3&gt;
&lt;p&gt;LangChain has been criticized for hiding important details and having inconsistent behavior, which can lead to unexpected issues in production systems. Users have reported that LangChain's default settings and behaviors are often undocumented or poorly explained, making it difficult to predict how the library will behave in different scenarios. This lack of transparency can lead to frustration and wasted time troubleshooting issues that could have been avoided with better documentation.&lt;/p&gt;
&lt;p&gt;&lt;a id="9-better-alternatives-available"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="9-better-alternatives-available"&gt;9. Better alternatives available&lt;/h3&gt;
&lt;p&gt;Users have mentioned other libraries, such as &lt;a href="https://github.com/microsoft/semantic-kernel"&gt;Semantic Kernel&lt;/a&gt;, &lt;a href="https://github.com/jerryjliu/llama_index"&gt;LlamaIndex&lt;/a&gt;, &lt;a href="https://haystack.deepset.ai/"&gt;Deepset Haystack&lt;/a&gt; , or &lt;a href="https://github.com/TransformerOptimus/SuperAGI"&gt;SuperAGI&lt;/a&gt;, as more suitable alternatives to LangChain. These alternatives often provide clearer documentation, more flexible customization options, and better integration with existing tools and scripts. In some cases, users have found that they can achieve their goals more easily and efficiently by using these alternative libraries instead of LangChain. See &lt;a href="https://github.com/kyrolabs/awesome-langchain#other-llm-frameworks"&gt;awesome-langchain&lt;/a&gt; for a list of LLM frameworks.&lt;/p&gt;
&lt;p&gt;&lt;a id="10-primarily-optimized-for-demos"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="10-primarily-optimized-for-demos"&gt;10. Primarily optimized for demos&lt;/h3&gt;
&lt;p&gt;LangChain has been described as being primarily optimized for quickly creating demos, rather than for building production-ready applications. &lt;a href="https://blog.streamlit.io/langchain-streamlit/"&gt;Partnership&lt;/a&gt; with &lt;a href="https://streamlit.io/generative-ai?ref=blog.streamlit.io"&gt;Streamlit&lt;/a&gt; should ease demo creation even more. While this can be useful for those who want to quickly experiment with LLMs or showcase their ideas, it can be limiting for users who want to build more robust, scalable applications. In these cases, users may find that LangChain's focus on demos and prototypes hinders their ability to build high-quality, production-ready applications.&lt;/p&gt;
&lt;p&gt;&lt;a id="takeaways---how-to-use-the-langchain-right-way"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="takeaways-how-to-use-the-langchain-right-way"&gt;Takeaways - How to Use the LangChain Right Way?&lt;/h2&gt;
&lt;p&gt;Based on the community comments and experiences shared, here are some pieces of advice on how to create apps using LangChain that will be reliable, easy to maintain and debug:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use LangChain for prototyping and experimentation&lt;/strong&gt;: LangChain can be useful for quickly creating prototypes and validating ideas. However, for more complex and production-level applications, you might want to consider implementing the functionality you need yourself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Understand the underlying concepts&lt;/strong&gt;: Before using LangChain, make sure to understand the core concepts of LLMs, prompts, and how the different components of the framework interact. This will help you make informed decisions about which parts of LangChain to use and which to replace with custom implementations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Focus on the value of the ecosystem&lt;/strong&gt;: LangChain provides integrations with various tools, indexes, and prompt templates. Leverage these resources to build your application, but be aware of the limitations and potential issues that might arise from using the default settings and abstractions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Be prepared to write custom code&lt;/strong&gt;: LangChain might not cover all use cases or provide the level of control and customization you need for your application. Be prepared to write custom code to better suit your specific requirements and use case.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Keep an eye on alternative tools and libraries&lt;/strong&gt;: As the field of LLMs is rapidly evolving, new tools and libraries are being developed that might better suit your needs. Stay informed about the latest developments and consider using alternative libraries like &lt;a href="https://haystack.deepset.ai/"&gt;Deepset Haystack&lt;/a&gt;, &lt;a href="https://github.com/stanfordnlp/dspy"&gt;DSPy&lt;/a&gt; , or Microsoft tools like &lt;a href="https://learn.microsoft.com/en-us/semantic-kernel/overview/"&gt;semantic-kernel&lt;/a&gt; and &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; if they better align with your project requirements. The &lt;a href="https://github.com/kyrolabs/awesome-langchain#other-llm-frameworks"&gt;list&lt;/a&gt; is huge and growing!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learn from LangChain's source code&lt;/strong&gt;: If you find that LangChain's abstractions and documentation are not sufficient for your needs, you can learn from the source code itself. Use the provided prompts and implementation details as inspiration and adapt them to your own project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consider local LLM models&lt;/strong&gt;: While LangChain primarily focuses on using OpenAI's models, you might want to explore using local LLM models like Llama, Galpaca, Vicuna, or Koala. These models can offer benefits in terms of cost, privacy, and offline capabilities. However, be aware that they might not be as powerful or accurate as GPT-3.5 Turbo.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrate with existing tools and scripts&lt;/strong&gt;: If you need to interface with existing Python tools or scripts, make sure to understand how LangChain interacts with them and how you can best integrate them into your application.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Test and measure the performance of your application&lt;/strong&gt;: When using LangChain, ensure that you thoroughly test your application and measure its performance against different prompts and configurations. This will help you identify potential issues and areas for improvement.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Keep an eye on the costs&lt;/strong&gt;: Be mindful of the API costs associated with using LangChain and consider optimizing your application to reduce the number of API calls and tokens used.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My favourite choice from this list would be #6 - to learn from the LangChain implemented tools and techniques by looking into the code.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In considering LangChain, it's vital to acknowledge its limitations and challenges before embracing it enthusiastically. Although LangChain has garnered significant attention and investment, users have pinpointed various drawbacks that could impede its effectiveness in more intricate, production-ready applications. To make well-informed decisions about LangChain's suitability for their projects, developers should gain an understanding of these issues.
In the ever-evolving landscape of LLM-driven development, assessing the available tools and libraries is crucial to determining which aligns best with your specific needs and requirements. It's worth noting that the ideal solution might not yet exist, necessitating adaptation or customization of existing tools or even the creation of your own to realize your vision for AI-driven applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2023-10-19: Added AutoGen and semantic-kernel, removed GPTi,&lt;/li&gt;
&lt;li&gt;2023-10-19: Added link to list of alternative frameworks&lt;/li&gt;
&lt;/ul&gt;</content><category term="Generative AI"/><category term="machine-learning"/><category term="python"/><category term="langchain"/><category term="prompt-engineering"/><category term="tokens"/><category term="llm"/><category term="gpt"/><category term="openai"/></entry><entry><title>Understanding Retrieval-Augmented Generation (RAG) empowering LLMs</title><link href="http://127.0.0.1:8000/understanding-retrieval-augmented-generation-rag-empowering-llms/" rel="alternate"/><published>2023-08-24T00:00:00+02:00</published><updated>2023-10-23T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-08-24:/understanding-retrieval-augmented-generation-rag-empowering-llms/</id><summary type="html">&lt;p&gt;Understand innovative artificial intelligence framework that empower large language models (LLMs) by anchoring them to external knowledge sources with accurate, current information.&lt;/p&gt;</summary><content type="html">&lt;h2 id="tldr"&gt;TLDR&lt;/h2&gt;
&lt;p&gt;Retrieval augmented generation refers to the method of enhancing a user's input to a large language model (LLM) such as ChatGPT by incorporating extra information obtained from an external source. This additional data can then be utilized by the LLM to enrich the response it produces.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction-understanding-retrieval-augmented-generation-rag"&gt;Introduction: Understanding Retrieval-Augmented Generation (RAG)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-need-for-rag-in-large-language-models"&gt;The Need for RAG in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-open-book-approach-of-rag"&gt;The 'Open Book' Approach of RAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#personalized-and-verifiable-responses-with-rag"&gt;Personalized and Verifiable Responses with RAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#challenges-and-future-directions"&gt;Challenges and Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction-understanding-retrieval-augmented-generation-rag"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction-understanding-retrieval-augmented-generation-rag"&gt;Introduction: Understanding Retrieval-Augmented Generation (RAG)&lt;/h2&gt;
&lt;p&gt;Retrieval-Augmented Generation, commonly referred to as RAG, and sometimes called Grounded Generation (GG), represents an ingenious integration of pretrained dense retrieval (DPR) and &lt;a href="https://en.wikipedia.org/wiki/Seq2seq"&gt;sequence-to-sequence&lt;/a&gt; models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Transformer architecture (used in GPT models) is a member of sequence-to-sequence (Seq2Seq) architectures. Seq2Seq models are designed to handle tasks that involve transforming an input sequence into an output sequence, such as machine translation, text summarization, and dialogue generation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The process involves retrieving documents using DPR and subsequently transmitting them to a seq2seq model. Through a process of marginalization, these models then produce desired outputs. The retriever and seq2seq modules commence their operations as pretrained models, and through a joint fine-tuning process, they adapt collaboratively, thus enhancing both retrieval and generation for specific downstream tasks. &lt;strong&gt;This innovative artificial intelligence framework serves as a means to empower large language models (LLMs) by anchoring them to external knowledge sources.&lt;/strong&gt; Consequently, this strategic approach ensures the availability of accurate, current information, thereby granting users valuable insights into the generative mechanisms of these models. For a comprehensive understanding of the RAG technique, we offer an in-depth exploration, commencing with a simplified overview and progressively delving into more intricate technical facets.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Data processing in RAG" src="https://learn.microsoft.com/en-us/azure/machine-learning/media/concept-retrieval-augmented-generation/retrieval-augmented-generation-walkthrough.png?view=azureml-api-2#lightbox"&gt;&lt;/p&gt;
&lt;p&gt;Figure 1. Data processing, storage and referencing in RAG method. Source: &lt;a href="https://learn.microsoft.com/en-us/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2"&gt;Microsoft&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="the-need-for-rag-in-large-language-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-need-for-rag-in-large-language-models"&gt;The Need for RAG in Large Language Models&lt;/h2&gt;
&lt;p&gt;Large language models, while powerful, can sometimes be inconsistent in their responses. They may provide accurate answers to certain questions but struggle with others, often regurgitating random facts from their training data. This inconsistency stems from the fact that LLMs understand the statistical relationships between words but not their actual meanings.&lt;/p&gt;
&lt;p&gt;To address this issue, researchers have developed the RAG &lt;strong&gt;framework, which improves the quality of LLM-generated responses by grounding the model in external sources of knowledge.&lt;/strong&gt; This approach not only ensures access to the most current and reliable facts but also allows users to verify the model's claims for accuracy and trustworthiness.&lt;/p&gt;
&lt;p&gt;&lt;a id="the-open-book-approach-of-rag"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-open-book-approach-of-rag"&gt;The 'Open Book' Approach of RAG&lt;/h2&gt;
&lt;p&gt;RAG operates in &lt;strong&gt;two main phases: retrieval and content generation&lt;/strong&gt;. During the retrieval phase, algorithms search for and retrieve relevant snippets of information based on the user's prompt or question. These facts can come from various sources, such as indexed documents on the internet or a closed-domain enterprise setting for added security and reliability.&lt;/p&gt;
&lt;p&gt;In the generative phase, the LLM uses the retrieved information and its internal representation of training data to synthesize a tailored answer for the user.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This approach is akin to an "open book" exam, where the model can browse through content in a book rather than relying solely on its memory.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="RAG Operation" src="/images/retrieval_augmented_generation/RAG.png"&gt;
Figure 2. RAG operation. Information preparation and storage. Augmenting prompt with external information.&lt;/p&gt;
&lt;p&gt;&lt;a id="personalized-and-verifiable-responses-with-rag"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="personalized-and-verifiable-responses-with-rag"&gt;Personalized and Verifiable Responses with RAG&lt;/h2&gt;
&lt;p&gt;RAG allows LLM-powered chatbots to provide more personalized answers without the need for human-written scripts. By reducing the need to continuously train the model on new data, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting.&lt;/p&gt;
&lt;p&gt;Moreover, RAG enables LLMs to generate more specific, diverse, and factual language compared to traditional parametric-only seq2seq models. This feature is particularly useful for businesses that require up-to-date information and verifiable responses.&lt;/p&gt;
&lt;p&gt;&lt;a id="challenges-and-future-directions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="challenges-and-future-directions"&gt;Challenges and Future Directions&lt;/h2&gt;
&lt;p&gt;Despite its advantages, RAG is not without its challenges. For instance, &lt;strong&gt;LLMs may struggle to recognize when they don't know the answer&lt;/strong&gt; to a question, leading to incorrect or misleading information. To address this issue, researchers are working on fine-tuning LLMs to recognize unanswerable questions and probe for more detail until they can provide a definitive answer.&lt;/p&gt;
&lt;p&gt;Furthermore, there is ongoing research to improve both the retrieval and generation aspects of RAG. This includes &lt;strong&gt;finding and fetching the most relevant information possible and structuring that information&lt;/strong&gt; to elicit the richest responses from the LLM.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Retrieval-Augmented Generation offers a promising solution to the limitations of large language models by grounding them in external knowledge sources. By adopting RAG, businesses can achieve customized solutions, maintain data relevance, and optimize costs while harnessing the reasoning capabilities of LLMs. As research continues to advance in this area, we can expect even more powerful and efficient language models in the future.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Original paper &lt;a href="https://arxiv.org/abs/2005.11401"&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks&lt;/a&gt; by Patrick Lewis et al. (available as &lt;a href="https://paperswithcode.com/method/rag"&gt;paper with code&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Exemplary notebooks on amazon Sagemaker:&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_jumpstart_knn.html"&gt;Retrieval-Augmented Generation: Question Answering based on Custom Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/jumpstart-foundation-models/question_answering_retrieval_augmented_generation/question_answering_langchain_jumpstart.html"&gt;Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced LangChain Library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python library with RAG implementation: &lt;a href="https://github.com/llmware-ai/llmware"&gt;GitHub - llmware-ai/llmware: Providing enterprise-grade LLM-based development framework, tools and fine-tuned models.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Analytics: &lt;a href="https://www.vectorview.ai/"&gt;Vectorview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep-dive into specific use-case of RAG with scaling in mind: &lt;a href="https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1"&gt;Building RAG-based LLM Applications for Production (Part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Good section on possible improvements to RAG: &lt;a href="https://llmstack.ai/blog/retrieval-augmented-generation"&gt;Retrieval Augmented Generation (RAG): What, Why and How? | LLMStack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;General intro to RAG: &lt;a href="https://scriv.ai/guides/retrieval-augmented-generation-overview/"&gt;How do domain-specific chatbots work? An Overview of Retrieval Augmented Generation (RAG) | Scriv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optimization, async, using summaries: &lt;a href="https://madhukarkumar.medium.com/secrets-to-optimizing-rag-llm-apps-for-better-accuracy-performance-and-lower-cost-da1014127c0a"&gt;Secrets to Optimizing RAG LLM Apps for Better Performance, Accuracy and Lower Costs! | by Madhukar Kumar | madhukarkumar | Sep, 2023 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Check the GitHub for the RAG-related projects: &lt;a href="https://github.com/topics/retrieval-augmented-generation?l=python"&gt;retrieval-augmented-generation · GitHub Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/16cbimi/yet_another_rag_system_implementation_details_and/"&gt;Yet another RAG system - implementation details and lessons learned : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/"&gt;Building and Evaluating Advanced RAG Applications - DeepLearning.AI&lt;/a&gt; - recent course from deeplearning.ai (Andrew Ng). Instructors: Jerry Liu and Anupam Datta.&lt;/li&gt;
&lt;li&gt;In this course, we’ll explore:&lt;ul&gt;
&lt;li&gt;Two advanced retrieval methods: Sentence-window retrieval and auto-merging retrieval that perform better compared to the baseline RAG pipeline.&lt;/li&gt;
&lt;li&gt;Evaluation and experiment tracking: A way evaluate and iteratively improve your RAG pipeline’s performance.&lt;/li&gt;
&lt;li&gt;The RAG triad: Context Relevance, Groundedness, and Answer Relevance, which are methods to evaluate the relevance and truthfulness of your LLM’s response.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;X::[[2023-11-01-boosting_RAG]]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2023-10-23: Added link to LLMStack&lt;/li&gt;
&lt;li&gt;2023-11-06: Added TLDR section&lt;/li&gt;
&lt;li&gt;2023-11-06: Added ToC&lt;/li&gt;
&lt;/ul&gt;</content><category term="Generative AI"/><category term="llm"/><category term="nlp"/><category term="question-answering"/><category term="rag"/><category term="re-ranking"/><category term="embeddings"/><category term="Transformers"/><category term="seq2seq"/><category term="prompt"/><category term="pretrained-dense-retrieval"/></entry><entry><title>Exploring Python Packages for Loading and Processing YAML Front Matter in Markdown Documents</title><link href="http://127.0.0.1:8000/python-packages-yaml-front-matter-markdown/" rel="alternate"/><published>2023-07-11T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-07-11:/python-packages-yaml-front-matter-markdown/</id><summary type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pyyaml"&gt;PyYAML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#frontmatter"&gt;Frontmatter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#yaml-front-matter"&gt;YAML Front Matter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python-markdown"&gt;Python Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mistune"&gt;mistune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#commonmark"&gt;Commonmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#which-one-to-use-in-my-case"&gt;Which one to use in my case?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#simple-front-matter-extraction"&gt;Simple Front Matter Extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#advanced-front-matter-manipulation"&gt;Advanced Front Matter Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#seamless-integration-with-markdown-parsing"&gt;Seamless Integration with Markdown Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#performance-and-speed"&gt;Performance and Speed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#commonmark-compliance"&gt;CommonMark Compliance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#minimalistic-approach"&gt;Minimalistic Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Markdown has gained …&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pyyaml"&gt;PyYAML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#frontmatter"&gt;Frontmatter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#yaml-front-matter"&gt;YAML Front Matter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python-markdown"&gt;Python Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mistune"&gt;mistune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#commonmark"&gt;Commonmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#which-one-to-use-in-my-case"&gt;Which one to use in my case?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#simple-front-matter-extraction"&gt;Simple Front Matter Extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#advanced-front-matter-manipulation"&gt;Advanced Front Matter Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#seamless-integration-with-markdown-parsing"&gt;Seamless Integration with Markdown Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#performance-and-speed"&gt;Performance and Speed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#commonmark-compliance"&gt;CommonMark Compliance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#minimalistic-approach"&gt;Minimalistic Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Markdown has gained popularity as a lightweight markup language for creating structured documents. It is widely used in various domains, including blogging, documentation, and note-taking. Markdown documents often include front matter, which is a metadata section at the beginning of the document. This front matter typically contains YAML (YAML Ain't Markup Language) formatted data that provides additional information about the document. In this blog post, we will explore several Python packages that can help you load and process YAML front matter in Markdown documents, providing you with the necessary tools to extract and work with this valuable metadata.&lt;/p&gt;
&lt;p&gt;&lt;a id="pyyaml"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="pyyaml"&gt;PyYAML&lt;/h3&gt;
&lt;p&gt;PyYAML is a powerful YAML parser and emitter for Python. It allows you to easily read and write YAML files, making it a suitable choice for extracting YAML front matter from Markdown documents.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/PyYAML/"&gt;PyYAML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href="https://github.com/yaml/pyyaml"&gt;PyYAML on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on how to load, modify and save front matter to markdown document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yaml&lt;/span&gt;

&lt;span class="c1"&gt;# Read front matter from a Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;front_matter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;---&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yaml&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;safe_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;front_matter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Modify front matter&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Modified&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2023-07-12&amp;#39;&lt;/span&gt;

&lt;span class="c1"&gt;# Write front matter back to the Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yaml&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default_flow_style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="frontmatter"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="python-frontmatter"&gt;python-frontmatter&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://jekyllrb.com/"&gt;Jekyll&lt;/a&gt;-style YAML front matter offers a useful way to add arbitrary, structured metadata to text documents, regardless of type.
This is a small package to load and parse files (or just text) with YAML (or JSON, TOML or other) front matter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/python-frontmatter/"&gt;python-frontmatter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href="https://github.com/eyeseast/python-frontmatter"&gt;python-frontmatter on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on how to load, modify and save front matter to markdown document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;frontmatter&lt;/span&gt;

&lt;span class="c1"&gt;# Read front matter from a Markdown file&lt;/span&gt;
&lt;span class="n"&gt;post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;frontmatter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Modify front matter&lt;/span&gt;
&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;modified&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2023-07-12&amp;#39;&lt;/span&gt;

&lt;span class="c1"&gt;# Write front matter back to the Markdown file&lt;/span&gt;
&lt;span class="n"&gt;frontmatter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="python-markdown"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="python-markdown"&gt;Python Markdown&lt;/h3&gt;
&lt;p&gt;Python Markdown is a popular package for parsing and rendering Markdown documents. While its primary focus is on converting Markdown to HTML, it also provides support for custom extensions, including front matter parsing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/Markdown/"&gt;Python Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href="https://github.com/Python-Markdown/markdown"&gt;Python Markdown on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on how to load, modify and save front matter to markdown document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;markdown.extensions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt;

&lt;span class="c1"&gt;# Read front matter from a Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;md&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MetaExtension&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Modify front matter&lt;/span&gt;
&lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Meta&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Modified&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2023-07-12&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Write front matter back to the Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Meta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pformat&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="mistune"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="mistune"&gt;mistune&lt;/h3&gt;
&lt;p&gt;Description: mistune is a fast and extensible Markdown parser implemented in pure Python. It aims to be compatible with the Markdown specification while offering various customization options, including support for front matter parsing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/mistune/"&gt;mistune&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href="https://github.com/lepture/mistune"&gt;mistune on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on how to load, modify and save front matter to markdown document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;mistune&lt;/span&gt;

&lt;span class="c1"&gt;# Read front matter from a Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;md&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mistune&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Markdown&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;renderer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mistune&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AstRenderer&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# Modify front matter&lt;/span&gt;
&lt;span class="n"&gt;front_matter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;renderer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;front_matter&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;front_matter&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Modified&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2023-07-12&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# Write front matter back to the Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;renderer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="commonmark"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="commonmark"&gt;Commonmark&lt;/h3&gt;
&lt;p&gt;Commonmark is a comprehensive Markdown parsing and rendering library for Python. It adheres to the CommonMark specification and offers a wide range of features, including support for parsing YAML front matter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/commonmark/"&gt;Commonmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href="https://github.com/commonmark/commonmark-python"&gt;Commonmark on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on how to load, modify and save front matter to markdown document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;commonmark&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;

&lt;span class="c1"&gt;# Read front matter from a Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Extract front matter&lt;/span&gt;
&lt;span class="n"&gt;front_matter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;^---\n(.*?)\n---\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DOTALL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yaml&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;safe_load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;front_matter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Modify front matter&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Modified&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2023-07-12&amp;#39;&lt;/span&gt;

&lt;span class="c1"&gt;# Write front matter back to the Markdown file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;article.md&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yaml&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default_flow_style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;front_matter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="which-one-to-use-in-my-case"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="which-one-to-use-in-my-case"&gt;Which one to use in my case?&lt;/h2&gt;
&lt;p&gt;Here are distinct use cases related to loading and processing YAML front matter in Markdown documents, along with recommended libraries for each case and the justifications for the recommendations:&lt;/p&gt;
&lt;p&gt;&lt;a id="simple-front-matter-extraction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="simple-front-matter-extraction"&gt;Simple Front Matter Extraction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recommended Library: &lt;strong&gt;Frontmatter&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Frontmatter is a dedicated Python package designed specifically for working with front matter in Markdown documents. It provides a simple and intuitive API for extracting front matter data, making it a suitable choice for straightforward front matter extraction needs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="advanced-front-matter-manipulation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="advanced-front-matter-manipulation"&gt;Advanced Front Matter Manipulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recommended Library: &lt;strong&gt;PyYAML&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;PyYAML is a powerful YAML parser and emitter for Python. If you require advanced manipulation and processing of YAML front matter, PyYAML offers extensive functionality and flexibility. It allows you to read and write YAML files, making it a robust choice for complex front matter handling.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="seamless-integration-with-markdown-parsing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="seamless-integration-with-markdown-parsing"&gt;Seamless Integration with Markdown Parsing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recommended Library: &lt;strong&gt;Python Markdown&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If your focus is on seamless integration with Markdown parsing, Python Markdown is a widely-used and feature-rich package. It supports custom extensions, including front matter parsing, allowing you to extract front matter while parsing the Markdown content. This integration can simplify your workflow when working with Markdown documents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="performance-and-speed"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="performance-and-speed"&gt;Performance and Speed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recommended Library: &lt;strong&gt;mistune&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;mistune is a fast and extensible Markdown parser implemented in pure Python. If performance and speed are crucial factors in your use case, mistune's efficient parsing capabilities make it an ideal choice. It provides customization options, including support for front matter parsing, while maintaining high performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="commonmark-compliance"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="commonmark-compliance"&gt;CommonMark Compliance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recommended Library: &lt;strong&gt;Commonmark&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If adhering to the CommonMark specification is essential, Commonmark is a comprehensive Markdown parsing and rendering library that aligns with the specification. It supports front matter parsing while ensuring compliance with the CommonMark standard, providing a reliable solution for standardized Markdown processing.&lt;/p&gt;
&lt;p&gt;&lt;a id="minimalistic-approach"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="minimalistic-approach"&gt;Minimalistic Approach&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recommended Library: &lt;strong&gt;YAML Front Matter&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;YAML Front Matter is a minimalistic package that focuses on simplicity and ease of use. If you prefer a lightweight solution for extracting YAML front matter from Markdown files without additional complexity, YAML Front Matter provides a straightforward and efficient approach.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we explored several Python packages that can load and process YAML front matter in Markdown documents. These packages provide convenient and efficient methods for extracting metadata from the front matter section, enabling you to access and manipulate this valuable information.&lt;/p&gt;</content><category term="Note"/><category term="python"/><category term="markdown"/><category term="front"/><category term="matter"/><category term="YAML"/><category term="packages"/></entry><entry><title>Easy Text Vectorization With VectorHub and Sentence Transformers</title><link href="http://127.0.0.1:8000/text-vectorization-with-vectorhub-and-sentence-transformers/" rel="alternate"/><published>2023-07-04T00:00:00+02:00</published><updated>2023-07-04T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-07-04:/text-vectorization-with-vectorhub-and-sentence-transformers/</id><summary type="html">&lt;p&gt;Learn how to use Sentence Transformers for text vectorization with different models using consistent API.&lt;/p&gt;</summary><content type="html">&lt;p&gt;up::[[embeddings]]
X::[[vector_embeddings_search]]
X::[[dont_use_openai_embeddings]]&lt;/p&gt;
&lt;p&gt;Text is heavily inspired by part of the e-book: &lt;a href="https://learn.getvectorai.com/vector-ai-documentation/semantic-nlp-search-with-faiss-and-vectorhub"&gt;Semantic NLP search with FAISS and VectorHub - Guide To Vectors (getvectorai.com)&lt;/a&gt; - which was using VectorHub as an interface to the models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: VectorHub is deprecated and no longer maintained. The authors of VectorHub recommend using &lt;a href="https://www.sbert.net/"&gt;Sentence Transformers&lt;/a&gt;, TFHub, and Huggingface directly for text vectorization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This article demonstrates a similar process as the original article but uses a sentence transformers package.&lt;/p&gt;
&lt;h3 id="encoding-data-using-sentence-transformers"&gt;Encoding Data Using Sentence Transformers&lt;/h3&gt;
&lt;p&gt;To encode models easily, we will utilize the &lt;a href="https://www.sbert.net/"&gt;Sentence Transformers&lt;/a&gt; library. SentenceTransformers is a Python framework for state-of-the-art sentence, text, and image embeddings. It provides a variety of pre-trained models that can convert sentences into meaningful numerical representations.&lt;/p&gt;
&lt;p&gt;First, we need to install the &lt;code&gt;sentence-transformers&lt;/code&gt; package, which includes the necessary dependencies for using Sentence Transformers. This library offers a wide range of pre-trained models, such as &lt;a href="https://en.wikipedia.org/wiki/BERT_(Language_model)"&gt;BERT&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/transformers/model_doc/roberta"&gt;RoBERTa&lt;/a&gt;, and &lt;a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"&gt;MiniLM&lt;/a&gt;, that can be used for text encoding. More information about Sentence Transformers can be found &lt;a href="https://www.sbert.net/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;sentence-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we will instantiate our model and start the encoding process. In this example, we will use the "all-MiniLM-L6-v2" model, which is a variant of the MiniLM model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sentence_transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SentenceTransformer&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SentenceTransformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;all-MiniLM-L6-v2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Sentences to be encoded&lt;/span&gt;
&lt;span class="n"&gt;sentences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;This framework generates embeddings for each input sentence&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;Sentences are passed as a list of strings.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;The quick brown fox jumps over the lazy dog.&amp;#39;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Encode sentences using the Sentence Transformers model&lt;/span&gt;
&lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Print the embeddings&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Sentence:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Embedding:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the code snippet above, we begin by installing the &lt;code&gt;sentence-transformers&lt;/code&gt; package, which provides the necessary tools for working with Sentence Transformers. This library offers various pre-trained models that can convert sentences into meaningful vector representations.&lt;/p&gt;
&lt;p&gt;After the installation, we import the &lt;code&gt;SentenceTransformer&lt;/code&gt; class from the &lt;code&gt;sentence_transformers&lt;/code&gt; module. We instantiate the model using the &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt; variant, which will be used for encoding our sentences.&lt;/p&gt;
&lt;p&gt;We define a list of sentences that we want to encode using the Sentence Transformers model. In this case, we have three exemplary sentences: "This framework generates embeddings for each input sentence," "Sentences are passed as a list of strings," and "The quick brown fox jumps over the lazy dog."&lt;/p&gt;
&lt;p&gt;To perform the encoding, we use the &lt;code&gt;encode&lt;/code&gt; method of the &lt;code&gt;model&lt;/code&gt; object, passing in the &lt;code&gt;sentences&lt;/code&gt; list. This method returns the corresponding embeddings for each sentence, which we store in the &lt;code&gt;embeddings&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;Finally, we iterate over the &lt;code&gt;sentences&lt;/code&gt; and &lt;code&gt;embeddings&lt;/code&gt; lists together using &lt;code&gt;zip&lt;/code&gt;. For each sentence and its corresponding embedding, we print them out to visualize the results.&lt;/p&gt;
&lt;p&gt;Please note that the code snippet above uses the "all-MiniLM-L6-v2" model as an example. You can explore and choose from a wide range of models provided by Sentence Transformers according to your specific requirements.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/RelevanceAI/vectorhub"&gt;GitHub - RelevanceAI/vectorhub: Vector Hub - Library for easy discovery, and consumption of State-of-the-art models to turn data into vectors. (text2vec, image2vec, video2vec, graph2vec, bert, inception, etc)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.getvectorai.com/"&gt;Introduction - Guide To Vectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Generative AI"/><category term="embeddings"/><category term="nlp"/><category term="vectorhub"/><category term="bert"/><category term="sentence-transformers"/></entry><entry><title>Harnessing the Power of Dependency Injection for Improved Testability in Python</title><link href="http://127.0.0.1:8000/python-dependency-injection-for-the-testability/" rel="alternate"/><published>2023-06-21T00:00:00+02:00</published><updated>2023-06-21T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-06-21:/python-dependency-injection-for-the-testability/</id><summary type="html">&lt;p&gt;Learn how to use dependency injection to decouple dependencies from our functions, methods, or classes, making it easier to test and maintain our code.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In software development, testability is a crucial aspect that helps ensure the reliability and maintainability of our code. One effective technique for enhancing testability is dependency injection (DI). Dependency injection allows us to decouple dependencies from our functions, methods, or classes, making it easier to test and maintain our code. In this blog post, we will explore various techniques, use cases, and lesser-known tricks for using dependency injection in Python.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-dependency-injection"&gt;What is Dependency Injection?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benefits-of-dependency-injection"&gt;Benefits of Dependency Injection:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#techniques-for-dependency-injection"&gt;Techniques for Dependency Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#constructor-injection"&gt;Constructor Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#setter-injection"&gt;Setter Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interface-injection"&gt;Interface Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-cases-for-dependency-injection"&gt;Use Cases for Dependency Injection:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing-legacy-code"&gt;Testing Legacy Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mocking-dependencies"&gt;Mocking Dependencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#improving-code-reusability"&gt;Improving Code Reusability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#parameter-injection"&gt;Parameter Injection:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-managers-and-dependency-injection"&gt;Context Managers and Dependency Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dependency-injection-containers"&gt;Dependency Injection Containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="what-is-dependency-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-is-dependency-injection"&gt;What is Dependency Injection?&lt;/h2&gt;
&lt;p&gt;Dependency injection is a design pattern that allows us to inject dependencies into a class or function from external sources rather than creating them internally. By doing so, we reduce the coupling between components and make them more flexible, reusable, and testable.&lt;/p&gt;
&lt;p&gt;&lt;a id="benefits-of-dependency-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="benefits-of-dependency-injection"&gt;Benefits of Dependency Injection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Improved testability&lt;/strong&gt;: By injecting dependencies, we can easily replace them with mocks or stubs during testing, making our tests more isolated and focused.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoupled code&lt;/strong&gt;: Dependency injection reduces the tight coupling between components, promoting better separation of concerns and modular design.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code reusability&lt;/strong&gt;: With dependency injection, components become more reusable as they rely on abstractions rather than concrete implementations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easier maintenance&lt;/strong&gt;: By externalizing dependencies, we can modify or extend their behavior without affecting the code that uses them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="techniques-for-dependency-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="techniques-for-dependency-injection"&gt;Techniques for Dependency Injection&lt;/h2&gt;
&lt;p&gt;&lt;a id="constructor-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="constructor-injection"&gt;Constructor Injection&lt;/h3&gt;
&lt;p&gt;Constructor injection involves passing dependencies through a class's constructor. It ensures that the required dependencies are available before an object is created.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;UserService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_repository&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user_repository&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;user_repository&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user_repository&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="setter-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="setter-injection"&gt;Setter Injection&lt;/h3&gt;
&lt;p&gt;Setter injection involves setting the dependencies using setter methods. This technique allows for more flexibility, as dependencies can be changed or updated after object initialization.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;NotificationService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;set_email_service&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;email_service&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;email_service&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;email_service&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;send_notification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;email_service&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;email&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;New notification!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="interface-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="interface-injection"&gt;Interface Injection&lt;/h3&gt;
&lt;p&gt;Interface injection is a technique where a dependency is injected by providing an interface or an abstract base class. This allows for the injection of different implementations of the same interface, providing flexibility and extensibility.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;abc&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;abstractmethod&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Database&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MySQLDatabase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Perform MySQL query&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;PostgresDatabase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Perform Postgres query&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="use-cases-for-dependency-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="use-cases-for-dependency-injection"&gt;Use Cases for Dependency Injection&lt;/h2&gt;
&lt;p&gt;&lt;a id="testing-legacy-code"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="testing-legacy-code"&gt;Testing Legacy Code&lt;/h3&gt;
&lt;p&gt;When working with legacy code that has tightly coupled dependencies, dependency injection can be used to introduce testability by replacing or mocking those dependencies during testing.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;legacy_function&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="n"&gt;db_connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MySQLDatabase&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# Tightly coupled dependency&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;

&lt;span class="c1"&gt;# Using dependency injection to test legacy_function&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_legacy_function&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;mock_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MockMySQLDatabase&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;legacy_function&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inject_dependencies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db_connection&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mock_db&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Test the function&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="mocking-dependencies"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="mocking-dependencies"&gt;Mocking Dependencies&lt;/h3&gt;
&lt;p&gt;In unit testing, dependency injection allows us to replace real dependencies with mock objects, enabling us to focus on testing the behavior of the unit under test in isolation.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;UserService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_repository&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user_repository&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;user_repository&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user_repository&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Testing UserService with a mock user repository&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_get_user&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;mock_repository&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MockUserRepository&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;service&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;UserService&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_repository&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mock_repository&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Test the method using the mock repository&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="improving-code-reusability"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="improving-code-reusability"&gt;Improving Code Reusability&lt;/h3&gt;
&lt;p&gt;Dependency injection promotes code reusability by relying on abstractions rather than concrete implementations. This allows different implementations to be injected based on specific requirements.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;PaymentGateway&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_payment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;amount&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;PayPalGateway&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PaymentGateway&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_payment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;amount&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Process payment via PayPal&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;StripeGateway&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PaymentGateway&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_payment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;amount&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Process payment via Stripe&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Lesser-Known Techniques and Tricks:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="parameter-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="parameter-injection"&gt;Parameter Injection&lt;/h3&gt;
&lt;p&gt;In addition to constructor, setter, and interface injection, parameter injection is a technique where dependencies are passed directly as parameters to functions or methods. This can be useful in situations where direct injection is preferred over using class instances.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Processing data...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Process the data&lt;/span&gt;

&lt;span class="c1"&gt;# Calling the function with injected dependencies&lt;/span&gt;
&lt;span class="n"&gt;logger&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Logger&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;process_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="context-managers-and-dependency-injection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="context-managers-and-dependency-injection"&gt;Context Managers and Dependency Injection&lt;/h3&gt;
&lt;p&gt;Context managers can be combined with dependency injection to provide resources or dependencies within a specific scope, ensuring their proper initialization and cleanup.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;contextlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;contextmanager&lt;/span&gt;

&lt;span class="nd"&gt;@contextmanager&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;db_connection&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MySQLDatabase&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# Dependency initialization&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;
    &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# Cleanup&lt;/span&gt;

&lt;span class="c1"&gt;# Using the context manager with dependency injection&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;db_connection&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Use the database connection within the context&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="dependency-injection-containers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="dependency-injection-containers"&gt;Dependency Injection Containers&lt;/h3&gt;
&lt;p&gt;Dependency injection containers or frameworks provide a centralized way to manage dependencies, their configurations, and their lifetime. Popular Python DI frameworks include &lt;code&gt;injector&lt;/code&gt;, &lt;code&gt;DInjector&lt;/code&gt;, and &lt;code&gt;inject&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;injector&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;inject&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Injector&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;UserService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nd"&gt;@inject&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_repository&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user_repository&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;user_repository&lt;/span&gt;

&lt;span class="c1"&gt;# Creating and using an injector&lt;/span&gt;
&lt;span class="n"&gt;injector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Injector&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;user_service&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;injector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UserService&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Dependency injection is a powerful technique for improving testability, code modularity, and reusability in Python. By applying various injection techniques and exploring different use cases, you can design more robust and maintainable code. Additionally, the lesser-known tricks and techniques covered in this blog post can further enhance your understanding and application of dependency injection in various scenarios.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="python"/><category term="testing"/><category term="dependency-injection"/><category term="software-development"/></entry><entry><title>The Role and Responsibilities of a Forward Deployed Engineer - Bridging the Gap Between Software Products and Customer Needs</title><link href="http://127.0.0.1:8000/the-role-and-responsibilities-of-a-forward-deployed-engineer/" rel="alternate"/><published>2023-06-09T00:00:00+02:00</published><updated>2023-06-09T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-06-09:/the-role-and-responsibilities-of-a-forward-deployed-engineer/</id><summary type="html">&lt;p&gt;Bridging the gap between software products and customer needs, Forward Deployed Engineers are the game-changers of enterprise software. Discover their unique role in driving success and why it's in high demand. Don't miss out!&lt;/p&gt;</summary><content type="html">&lt;h2 id="tldr"&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;A Forward Deployed Engineer (FDE) is a versatile software engineer who works closely with customers to bridge the gap between enterprise software products and their specific implementation needs. FDEs collaborate with engineering teams, provide technical support, partner with product teams, assist in revenue growth activities, and lead customer success efforts. With a mix of technical skills, an entrepreneurial mindset, and product intuition, FDEs play a crucial role in ensuring successful product deployment and customer satisfaction.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#tldr"&gt;TL;DR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#understanding-the-role-of-fdes"&gt;Understanding the Role of FDEs&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#collaboration-with-engineering"&gt;Collaboration with Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#partnership-with-product-teams"&gt;Partnership with Product Teams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support-for-revenue-growth"&gt;Support for Revenue Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#leadership-in-customer-success"&gt;Leadership in Customer Success&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#why-forward-deployed-engineers-are-in-high-demand"&gt;Why Forward Deployed Engineers are in High Demand?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#technical-expertise-and-customer-focus"&gt;Technical Expertise and Customer Focus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#agile-problem-solvers"&gt;Agile Problem Solvers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#product-intuition"&gt;Product Intuition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In enterprise software, there is an increasing demand for versatile engineers who can seamlessly integrate complex products into customers' specific implementation needs. This demand has given rise to the role of Forward Deployed Engineer (FDE). FDEs play a crucial role in ensuring successful technical integration and ongoing product deployment, acting as a bridge between the product suite and the unique requirements of each customer. This blog post will look into the responsibilities of FDEs and shed light on why this role is in high demand.&lt;/p&gt;
&lt;p&gt;&lt;a id="understanding-the-role-of-fdes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="understanding-the-role-of-fdes"&gt;Understanding the Role of FDEs&lt;/h2&gt;
&lt;p&gt;Forward Deployed Engineers are software engineers with broad skill sets that enable them to work closely with customers and iterate on enterprise software products. They possess technical expertise while being customer-facing, making them a valuable asset in various areas of an enterprise software organization.&lt;/p&gt;
&lt;p&gt;&lt;a id="collaboration-with-engineering"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="collaboration-with-engineering"&gt;Collaboration with Engineering&lt;/h3&gt;
&lt;p&gt;Forward Deployed Engineers (FDEs) play a crucial role in fostering collaboration between engineering teams and external stakeholders. By actively contributing to internal codebases and working closely with core engineering teams, FDEs ensure that customer feedback and implementation needs are effectively communicated and addressed.&lt;/p&gt;
&lt;p&gt;FDEs act as the bridge between the technical complexities of the product and the understanding of external stakeholders. They have a deep understanding of the product's architecture, functionalities, and underlying technologies. This expertise allows them to effectively communicate technical topics to non-technical stakeholders, such as customers or business executives.&lt;/p&gt;
&lt;p&gt;When customers encounter challenges or require customizations to the product suite, FDEs work closely with the engineering team to find viable solutions. They provide valuable insights on the implementation needs and collaborate with engineers to identify the best approach. FDEs act as advocates for customers, ensuring that their requirements are properly understood and addressed within the product's capabilities.&lt;/p&gt;
&lt;p&gt;Through this collaboration, FDEs contribute to the improvement of internal codebases. They provide feedback to engineering teams regarding areas that require enhancements or optimizations based on real-world customer experiences. This feedback loop helps create a continuous improvement process for the product, making it more robust and aligned with customer needs.&lt;/p&gt;
&lt;p&gt;Furthermore, FDEs actively participate in cross-functional meetings, bringing the perspective of external stakeholders to the engineering team. This collaboration helps align engineering efforts with customer requirements and provides valuable context for decision-making.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Collaboration with engineering is a critical aspect of the FDE role. By effectively communicating technical topics to external stakeholders and working closely with the engineering team, FDEs ensure that customer feedback is accurately relayed, implementation needs are addressed, and the product continues to evolve to meet customer expectations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="partnership-with-product-teams"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="partnership-with-product-teams"&gt;Partnership with Product Teams&lt;/h3&gt;
&lt;p&gt;Forward Deployed Engineers (FDEs) play a pivotal role in establishing a strong partnership between external stakeholders and the product teams. By leveraging their customer-facing experience and technical expertise, FDEs bring valuable insights to the table, shaping the product roadmap and driving its evolution.&lt;/p&gt;
&lt;p&gt;FDEs act as the voice of the customer within the organization. They gather feedback, requirements, and feature requests directly from customers and effectively communicate these insights to the product teams. By understanding the customers' pain points, desired features, and use cases, FDEs provide invaluable information that helps shape the product's direction.&lt;/p&gt;
&lt;p&gt;Throughout the engineering lifecycle, FDEs collaborate closely with the product teams to iterate on existing features and deliver new use cases. They work in tandem with product managers, developers, and designers to ensure that the product roadmap aligns with the specific needs of customers. FDEs provide real-world context and technical expertise, enabling product teams to make informed decisions regarding prioritization, feature enhancements, and trade-offs.&lt;/p&gt;
&lt;p&gt;FDEs also act as a bridge between product teams and customers during the implementation phase. They facilitate ongoing communication, ensuring that the product is implemented effectively and meets customers' expectations. FDEs provide guidance on technical integration, address any gaps between the product suite and customer requirements, and offer insights on best practices for successful deployment.&lt;/p&gt;
&lt;p&gt;Additionally, FDEs actively participate in testing and validation processes, providing feedback on new features and enhancements from the customer's perspective. They collaborate with product teams to conduct user acceptance testing, gather feedback, and ensure that the product meets the desired outcomes.&lt;/p&gt;
&lt;p&gt;By establishing a strong partnership with product teams, FDEs contribute to the overall success of the product. Their unique position allows them to bridge the gap between customer needs and product development, ensuring that the product remains relevant, competitive, and aligned with the evolving market landscape.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The partnership between FDEs and product teams is essential for driving innovation, customer satisfaction, and product evolution. FDEs bring customer insights, technical expertise, and a deep understanding of implementation needs to collaborate closely with product teams, influencing the product roadmap, and delivering value-driven solutions to customers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="support-for-revenue-growth"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="support-for-revenue-growth"&gt;Support for Revenue Growth&lt;/h3&gt;
&lt;p&gt;Forward Deployed Engineers (FDEs) contribute significantly to revenue growth by providing technical expertise and support in various revenue-related activities. Their role extends beyond engineering and involves actively participating in sales meetings, leading technical discussions, and completing Requests for Proposal (RFPs).&lt;/p&gt;
&lt;p&gt;As technical advisors, FDEs join sales meetings with non-technical external stakeholders, such as executives or business leaders. In this capacity, they provide valuable insights into the product's capabilities, technical requirements, and implementation process. By bridging the gap between the product suite and the customers' specific needs, FDEs help potential clients understand the value proposition and make informed purchasing decisions.&lt;/p&gt;
&lt;p&gt;Moreover, FDEs take the lead in technical sales calls and meetings with external technical stakeholders. They are responsible for communicating the technical aspects of the product, answering complex inquiries, and addressing any technical concerns potential customers may have. FDEs play a crucial role in building trust and confidence in the product's ability to meet the customers' requirements.&lt;/p&gt;
&lt;p&gt;FDEs also contribute to revenue growth by completing RFPs. These documents are often requested by potential customers to evaluate software solutions for their specific needs. FDEs leverage their technical knowledge and customer insights to provide comprehensive and accurate responses to these RFPs. By effectively showcasing the product's capabilities and aligning them with customer requirements, FDEs play a key role in unlocking new revenue opportunities.&lt;/p&gt;
&lt;p&gt;Additionally, FDEs collaborate with the sales and marketing teams to develop technical collateral, such as case studies, technical whitepapers, and solution guides. These resources help articulate the product's value proposition, highlight successful customer implementations, and provide technical details to support the sales process. FDEs actively contribute to these materials, ensuring they are accurate, relevant, and impactful.&lt;/p&gt;
&lt;p&gt;By supporting revenue growth initiatives, FDEs contribute to the overall success of the organization. Their technical expertise, customer-centric mindset, and ability to effectively communicate the value of the product position them as trusted advisors and advocates for both the customers and the sales teams. FDEs help drive new business opportunities, enhance customer satisfaction, and ultimately contribute to the financial growth of the company.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FDEs play a crucial role in supporting revenue growth by providing technical support, leading sales discussions, completing RFPs, and developing collateral. Their ability to bridge the gap between technical complexities and customer needs helps build trust, accelerate sales cycles, and unlock new revenue streams. FDEs are instrumental in driving the financial success of the organization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="leadership-in-customer-success"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="leadership-in-customer-success"&gt;Leadership in Customer Success&lt;/h3&gt;
&lt;p&gt;Forward Deployed Engineers (FDEs) take on a leadership role in ensuring customer success throughout the implementation and deployment of the product. They act as technical leads and provide critical support to customers, facilitating onboarding, and driving the adoption of new features into customers' production environments.&lt;/p&gt;
&lt;p&gt;FDEs serve as the primary point of contact for customers during the implementation phase. They work closely with customer success teams to understand the customers' specific requirements and develop tailored implementation plans. FDEs leverage their technical expertise to guide customers through the integration process, ensuring a smooth and successful onboarding experience.&lt;/p&gt;
&lt;p&gt;As technical leads, FDEs provide ongoing support to customers, addressing any technical issues or challenges they may encounter. They troubleshoot and resolve complex technical problems, acting as a bridge between the customers and the engineering team. FDEs leverage their deep understanding of the product to provide timely and effective solutions, ensuring that customers can fully leverage the capabilities of the software.&lt;/p&gt;
&lt;p&gt;In addition to technical support, FDEs play a critical role in driving the adoption of new features and enhancements. They collaborate with customers to understand their specific use cases and provide guidance on how to best utilize the product's functionality to achieve their desired outcomes. FDEs conduct training sessions, create documentation, and offer best practices to ensure that customers can maximize the value they derive from the product.&lt;/p&gt;
&lt;p&gt;FDEs also act as advocates for customers within the organization. They actively collect feedback, feature requests, and insights from customers and communicate them to the product teams. By representing the customers' voice, FDEs contribute to the continuous improvement of the product, ensuring that it evolves to meet their changing needs.&lt;/p&gt;
&lt;p&gt;Building strong relationships with customers is a key aspect of the FDE role. FDEs engage in regular communication, conduct business reviews, and seek opportunities to deepen customer engagement. By understanding the customers' goals, challenges, and aspirations, FDEs can provide personalized recommendations and strategic guidance, ultimately fostering long-term customer satisfaction and loyalty.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FDEs assume a leadership role in customer success by providing technical guidance, support, and advocacy throughout the implementation and deployment process. Their deep technical expertise, customer-centric approach, and ability to build strong relationships position them as trusted partners for customers. FDEs play a crucial role in driving customer success, ensuring that customers achieve their desired outcomes and maximizing the value they derive from the product.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="why-forward-deployed-engineers-are-in-high-demand"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="why-forward-deployed-engineers-are-in-high-demand"&gt;Why Forward Deployed Engineers are in High Demand?&lt;/h2&gt;
&lt;p&gt;The increasing complexity of enterprise software products and the variability in customer requirements have created a significant demand for FDEs. Here are some reasons why this role is sought after:&lt;/p&gt;
&lt;p&gt;&lt;a id="technical-expertise-and-customer-focus"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="technical-expertise-and-customer-focus"&gt;Technical Expertise and Customer Focus&lt;/h3&gt;
&lt;p&gt;FDEs possess a unique mix of technical skills and customer-centricity. They understand the details of the product and can effectively communicate its value to both technical and non-technical stakeholders. Their ability to bridge the gap between engineering and customer needs is invaluable in ensuring successful deployments.&lt;/p&gt;
&lt;p&gt;&lt;a id="agile-problem-solvers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="agile-problem-solvers"&gt;Agile Problem Solvers&lt;/h3&gt;
&lt;p&gt;FDEs exhibit an entrepreneurial mindset, allowing them to adapt quickly to evolving customer requirements. They are adept at identifying challenges, proposing solutions, and iterating on product features. This agility is essential in a rapidly changing technological landscape, where customers' needs evolve at a fast pace.&lt;/p&gt;
&lt;p&gt;&lt;a id="product-intuition"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="product-intuition"&gt;Product Intuition&lt;/h3&gt;
&lt;p&gt;By working closely with customers, FDEs develop a deep understanding of their pain points and aspirations. This product intuition enables them to provide valuable insights to product teams, helping shape the product roadmap and prioritize features that align with customer needs. FDEs contribute to the development of customer-centric software solutions.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Forward Deployed Engineers play a vital role in enterprise software organizations, acting as the bridge between products and customer implementations. Their broad skill set, technical expertise, entrepreneurial mindset, and product intuition make them invaluable assets in driving customer success, revenue growth, and product evolution. As enterprise software continues to evolve, the demand for FDEs will likely increase, providing software engineers with a customer-facing path that allows them to thrive in both technical and business domains&lt;/p&gt;
&lt;p&gt;X::[[forward_deployed_engineer]]&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="machine-learning"/><category term="python"/><category term="career"/><category term="roles"/><category term="software-development"/></entry><entry><title>The Best Vector Databases for Storing Embeddings</title><link href="http://127.0.0.1:8000/the-best-vector-databases-for-storing-embeddings/" rel="alternate"/><published>2023-06-05T00:00:00+02:00</published><updated>2023-06-05T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-06-05:/the-best-vector-databases-for-storing-embeddings/</id><summary type="html">&lt;p&gt;Look into the World of Vector Databases Fueling NLP's Transformative Journey.&lt;/p&gt;</summary><content type="html">&lt;p&gt;X::[[embeddings]]&lt;/p&gt;
&lt;h2 id="best-vector-databases-for-storing-embeddings-in-nlp"&gt;Best Vector Databases for Storing Embeddings in NLP&lt;/h2&gt;
&lt;p&gt;As natural language processing (NLP) continues to advance, the need for efficient storage and retrieval of vector representations, or embeddings, has become paramount.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Vector databases are purpose-built databases that excel in storing and querying high-dimensional vector data, such as word embeddings or document representations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This article explores the best vector databases available, their unique features, and the crucial parameters that differentiate them.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#best-vector-databases-for-storing-embeddings-in-nlp"&gt;Best Vector Databases for Storing Embeddings in NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tldr"&gt;TLDR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-vector-databases-are-and-why-there-is-demand-for-them"&gt;What vector databases are, and why there is demand for them?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#understanding-tradeoffs-and-identifying-the-specific-requirements-to-choose-the-best-tool"&gt;Understanding tradeoffs and identifying the specific requirements to choose the best tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vector-databases"&gt;Vector databases&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#chroma"&gt;Chroma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#haystack-by-deepsetai"&gt;Haystack by DeepsetAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#faiss-by-facebook"&gt;Faiss by Facebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#milvus"&gt;Milvus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pgvector"&gt;pgvector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pinecone"&gt;Pinecone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#supabase"&gt;Supabase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#qdrant"&gt;Qdrant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vespa"&gt;Vespa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#weaviate"&gt;Weaviate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deeplake"&gt;DeepLake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vectorstore-from-langchain"&gt;VectorStore from LangChain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-relevant-vector-databases"&gt;Other Relevant Vector Databases&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#annoy"&gt;Annoy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#elasticsearch"&gt;Elasticsearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hnswlib"&gt;Hnswlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nmslib"&gt;NMSLIB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#tabular-summary-of-the-features"&gt;Tabular summary of the features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommendations"&gt;Recommendations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#easy-start-and-user-friendliness---good-for-poc"&gt;Easy Start and User-Friendliness - good for PoC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#advanced-capabilities-and-performance"&gt;Advanced Capabilities and Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#customization-and-advanced-use-cases"&gt;Customization and Advanced Use Cases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#related-reading"&gt;Related reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="tldr"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tldr"&gt;TLDR&lt;/h2&gt;
&lt;p&gt;If you don't want to spent time on reading about each solution, you might want to head directly for the &lt;a href="#recommendations"&gt;recommendations&lt;/a&gt; section where solutions for various use cases are proposed.
&lt;a id="what-vector-databases-are-and-why-there-is-demand-for-them"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-vector-databases-are-and-why-there-is-demand-for-them"&gt;What vector databases are, and why there is demand for them?&lt;/h2&gt;
&lt;p&gt;Vector databases are specialized databases designed for efficient storage, retrieval, and manipulation of vector representations, particularly in the context of Natural Language Processing (NLP) and machine learning applications. They are optimized for handling high-dimensional embeddings that represent textual or numerical data in a vectorized format.&lt;/p&gt;
&lt;p&gt;While traditional databases like PostgreSQL are versatile and battle-tested, they are not specifically optimized for vector operations. Vector databases, on the other hand, provide a set of features and optimizations tailored to the unique requirements of working with vector embeddings. Here are some reasons why vector databases are in demand despite the existence of other types of databases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Vector databases are built to handle large-scale datasets and can scale horizontally to accommodate growing data volumes. They distribute the storage and processing of vectors across multiple machines, enabling efficient handling of massive amounts of embedding data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Query Speed&lt;/strong&gt;: Vector databases employ advanced indexing structures and search algorithms, such as approximate nearest neighbor (ANN) search, to achieve fast and accurate similarity searches. These optimizations enable rapid retrieval of vectors based on their similarity to a given query vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accuracy of Search Results&lt;/strong&gt;: Vector databases focus on preserving the accuracy of similarity search results. They leverage techniques like space partitioning, dimensionality reduction, and quantization to ensure that similar vectors are efficiently identified, even in high-dimensional spaces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: Vector databases offer flexibility in terms of supported vector operations and indexing methods. They often provide a range of indexing algorithms, allowing users to choose the one that best suits their specific use case. Additionally, vector databases may support additional functionality like filtering, ranking, and semantic search.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Persistence and Durability&lt;/strong&gt;: Vector databases prioritize data persistence and durability, ensuring that vector embeddings are reliably stored and protected against data loss. They often integrate with existing storage solutions or provide mechanisms for backup and replication.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Storage Location&lt;/strong&gt;: Vector databases can be deployed either on-premises or in the cloud, providing flexibility in terms of infrastructure choices. Cloud-based vector databases offer the advantage of managed services, offloading the operational overhead of maintaining and scaling the database infrastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Direct Library vs. Abstraction&lt;/strong&gt;: Vector databases come in two main forms: those that offer a direct library interface for integration into existing systems and those that provide a higher-level abstraction, such as RESTful APIs or query languages. This flexibility allows developers to choose the level of control and integration that best fits their requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While traditional databases like PostgreSQL can handle various data types, including vectors, they may lack the specialized optimizations and features provided by vector databases. Vector databases excel in efficiently storing and querying high-dimensional embeddings, enabling faster similarity search and supporting specific vector-related operations. By leveraging these optimizations, vector databases streamline the development and deployment of NLP and machine learning applications.&lt;/p&gt;
&lt;p&gt;&lt;a id="understanding-tradeoffs-and-identifying-the-specific-requirements-to-choose-the-best-tool"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="understanding-tradeoffs-and-identifying-the-specific-requirements-to-choose-the-best-tool"&gt;Understanding tradeoffs and identifying the specific requirements to choose the best tool&lt;/h2&gt;
&lt;p&gt;When choosing a vector database, there are several tradeoffs and potentially contradicting requirements that developers need to consider. Here are some typical tradeoffs and contradictions related to selecting a vector database:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability vs. Query Speed&lt;/strong&gt;: Achieving high scalability often requires distributing data across multiple nodes, which can impact query speed due to network communication. Balancing the need for scalability with the requirement for fast query response times can be a tradeoff when selecting a vector database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search Accuracy vs. Query Speed&lt;/strong&gt;: Algorithms that provide high search accuracy, such as exact nearest neighbor search, can be computationally expensive and impact query speed. Approximate algorithms, while faster, might sacrifice some accuracy. The tradeoff lies in finding the right balance between search accuracy and query speed based on the specific use case.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flexibility vs. Performance&lt;/strong&gt;: Some vector databases offer extensive customization options, allowing users to tailor the system to their specific requirements. However, the more flexibility provided, the more overhead might be introduced, potentially impacting overall performance. Balancing the need for flexibility with performance considerations is crucial.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Persistence and Durability vs. Query Performance&lt;/strong&gt;: Ensuring data persistence and durability typically involves additional disk I/O operations, which can impact query performance. The tradeoff here is finding the right level of data persistence and durability while maintaining satisfactory query performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Storage Location vs. Data Security&lt;/strong&gt;: Storing vector embeddings locally provides faster access, but it may introduce data security risks. Cloud-based storage solutions offer scalability and redundancy but may raise concerns about data privacy and compliance. The choice between local and cloud storage involves weighing the benefits of each option against data security requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Direct Library vs. Abstraction&lt;/strong&gt;: Some vector databases offer direct library interfaces for seamless integration into existing systems, while others provide higher-level abstractions like APIs or query languages for ease of use. The tradeoff here is between the level of control and integration required versus the simplicity of implementation and maintenance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ease of Use vs. Advanced Features&lt;/strong&gt;: Vector databases that prioritize ease of use often sacrifice some advanced features and optimization techniques. Developers must consider the complexity of their use case and weigh the need for advanced features against the simplicity of the database.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these tradeoffs and identifying the specific requirements of a project is crucial in selecting a vector database that best aligns with the desired tradeoff priorities. It requires carefully evaluating the tradeoffs and making informed decisions based on the unique needs of the application or system being developed.&lt;/p&gt;
&lt;p&gt;&lt;a id="vector-databases"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="vector-databases"&gt;Vector databases&lt;/h2&gt;
&lt;p&gt;&lt;a id="chroma"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="chroma"&gt;Chroma&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/chroma-core/chroma?logo=github"&gt;
&lt;img alt="chroma logo" src="https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png"&gt;
&lt;a href="https://www.trychroma.com/"&gt;Chroma&lt;/a&gt; is an open-source vector database developed by Chroma.ai. It focuses on scalability, providing robust support for storing and querying large-scale embedding datasets efficiently. Chroma offers a distributed architecture with horizontal scalability, enabling it to handle massive volumes of vector data. It leverages Apache Cassandra for high availability and fault tolerance, ensuring data persistence and durability.&lt;/p&gt;
&lt;p&gt;One unique aspect of Chroma is its &lt;strong&gt;flexible indexing system&lt;/strong&gt;. It supports &lt;strong&gt;multiple indexing strategies&lt;/strong&gt;, such as &lt;a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods"&gt;approximate nearest neighbors&lt;/a&gt; (ANN) algorithms like &lt;a href="https://arxiv.org/abs/1603.09320"&gt;HNSW&lt;/a&gt; and &lt;a href="https://towardsdatascience.com/similarity-search-with-ivfpq-9c6348fd4db3"&gt;IVFPQ&lt;/a&gt;, enabling fast and accurate similarity searches. Chroma also provides comprehensive &lt;strong&gt;Python and RESTful APIs&lt;/strong&gt;, making it &lt;strong&gt;easily integratable&lt;/strong&gt; into NLP pipelines. With its emphasis on &lt;strong&gt;scalability&lt;/strong&gt; and &lt;strong&gt;speed&lt;/strong&gt;, Chroma is an excellent choice for applications that require high-performance vector storage and retrieval.&lt;/p&gt;
&lt;p&gt;They have &lt;a href="https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing"&gt;Colab&lt;/a&gt; notebook with the demo.&lt;/p&gt;
&lt;p&gt;The core API commands (from the product page)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;chromadb&lt;/span&gt;

&lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chromadb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# add embeddings and documents&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get back similar ones&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note: there are plugins for LangChain, LlamaIndex, OpenAI and others.
&lt;a id="haystack-by-deepsetai"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="haystack-by-deepsetai"&gt;Haystack by DeepsetAI&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/deepset-ai/haystack?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="haystack logo" src="images/vectordb/haystack.png"&gt;
DeepsetAI's &lt;a href="https://haystack.deepset.ai/"&gt;Haystack&lt;/a&gt; is another popular vector database designed specifically for NLP applications. It offers a range of features tailored to support end-to-end development of search systems using embeddings. Haystack integrates well with popular transformer models like BERT, allowing users to extract embeddings directly from pre-trained models. It leverages &lt;a href="https://www.elastic.co/what-is/elasticsearch"&gt;Elasticsearch&lt;/a&gt; as its underlying storage engine, providing powerful indexing and querying capabilities.&lt;/p&gt;
&lt;p&gt;Haystack stands out with its &lt;strong&gt;intuitive query language&lt;/strong&gt;, which supports complex &lt;strong&gt;semantic searches&lt;/strong&gt; and &lt;strong&gt;filtering&lt;/strong&gt; based on various parameters. Additionally, it offers a &lt;strong&gt;modular pipeline&lt;/strong&gt; architecture for preprocessing, &lt;strong&gt;embedding extraction&lt;/strong&gt;, and querying, making it &lt;strong&gt;highly customizable and adaptable&lt;/strong&gt; to different NLP use cases. With its &lt;strong&gt;user-friendly interface&lt;/strong&gt; and comprehensive functionality, DeepsetAI's Haystack is an excellent choice for developers seeking a flexible and feature-rich vector database for NLP.&lt;/p&gt;
&lt;p&gt;&lt;a id="faiss-by-facebook"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="faiss-by-facebook"&gt;Faiss by Facebook&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/facebookresearch/faiss?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://faiss.ai/"&gt;Faiss logo&lt;/a&gt;, developed by Facebook AI Research, is a widely-used vector database renowned for its high-performance similarity search capabilities. It provides a range of indexing methods optimized for efficient retrieval of nearest neighbors, including IVF (Inverted File) and HNSW (Hierarchical Navigable Small World). Faiss also supports GPU acceleration, enabling fast computation on large-scale embeddings.&lt;/p&gt;
&lt;p&gt;One of Faiss' notable features is its support for &lt;strong&gt;multi-index search&lt;/strong&gt;, which combines different indexing methods to improve search accuracy and speed. Additionally, Faiss offers a &lt;strong&gt;Python interface&lt;/strong&gt;, making it easy to integrate with existing NLP pipelines and frameworks. With its focus on &lt;strong&gt;search performance and versatility&lt;/strong&gt;, Faiss is a go-to choice for projects demanding fast and accurate similarity &lt;strong&gt;search over vast embedding collections&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="milvus"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="milvus"&gt;Milvus&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/milvus-io/milvus?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Milvus logo" src="/images/vectordb/milvus.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://milvus.io/"&gt;Milvus&lt;/a&gt; is an open-source vector database developed by Zilliz, designed for efficient storage and retrieval of large-scale embeddings. It provides high scalability and supports distributed deployment across multiple machines, making it suitable for handling massive NLP datasets. Milvus integrates with popular ANN libraries like Faiss, Annoy, and NMSLIB, offering flexible indexing options to achieve high search accuracy.&lt;/p&gt;
&lt;p&gt;One key feature of Milvus is its &lt;strong&gt;GPU support&lt;/strong&gt;, leveraging NVIDIA GPUs for accelerated computation. This makes Milvus an excellent choice &lt;strong&gt;for deep learning applications&lt;/strong&gt; that require fast vector search and similarity calculations. Furthermore, Milvus provides a user-friendly &lt;strong&gt;WebUI&lt;/strong&gt; and supports &lt;strong&gt;multiple programming languages&lt;/strong&gt;, simplifying development and deployment processes. With its focus on scalability and GPU acceleration, Milvus is an ideal vector database for large-scale NLP projects.&lt;/p&gt;
&lt;p&gt;&lt;a id="pgvector"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="pgvector"&gt;pgvector&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/ankane/pgvector?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;Open-source vector similarity search for Postgres. Pgvector helps to built vector database on top of PostgreSQL, a popular open-source relational database. It leverages the powerful indexing capabilities of PostgreSQL's extension system to provide efficient storage and retrieval of vector embeddings. pgvector supports both CPU and GPU inference, enabling high-performance vector operations.&lt;/p&gt;
&lt;p&gt;One key advantage of pgvector is its seamless &lt;strong&gt;integration with the broader PostgreSQL&lt;/strong&gt; ecosystem. Users can leverage the rich functionality of PostgreSQL, such as ACID compliance and support for complex queries, while benefiting from vector-specific operations. pgvector provides a PostgreSQL extension that extends the SQL syntax to handle vector operations and offers a Python library for easy integration. With its compatibility with PostgreSQL and efficient vector storage, pgvector is a reliable choice for NLP applications that require a seamless SQL integration.&lt;/p&gt;
&lt;p&gt;&lt;a id="pinecone"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="pinecone"&gt;Pinecone&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Pinecone logo" src="/images/vectordb/pinecone.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt; is a managed vector database built for handling large-scale embeddings in real-time applications. It focuses on low-latency search and high-throughput indexing, making it suitable for latency-sensitive NLP use cases. Pinecone's cloud-native infrastructure handles indexing, storage, and query serving, allowing developers to focus on building their applications.&lt;/p&gt;
&lt;p&gt;Pinecone offers a RESTful &lt;strong&gt;API&lt;/strong&gt; and client libraries &lt;strong&gt;for various programming languages&lt;/strong&gt;, simplifying integration with different NLP frameworks. It supports &lt;strong&gt;dynamic indexing&lt;/strong&gt;, allowing incremental updates to embeddings without rebuilding the entire index. Pinecone also provides advanced features like &lt;strong&gt;vector similarity search&lt;/strong&gt;, &lt;strong&gt;filtering&lt;/strong&gt;, and result ranking. With its &lt;strong&gt;emphasis on real-time performance&lt;/strong&gt; and ease of use, Pinecone is an excellent choice for developers seeking a fully managed vector database for NLP applications.&lt;/p&gt;
&lt;p&gt;&lt;a id="supabase"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="supabase"&gt;Supabase&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/supabase/supabase?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Supabase logo" src="images/vectordb/supabase.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt;, known for its open-source data platform, offers a scalable vector storage solution designed for fast and efficient retrieval of embeddings. Supabase leverages PostgreSQL as its underlying storage engine, ensuring data durability and compatibility with standard SQL queries. It provides a range of features such as indexing, querying, and filtering, optimized for vector data.&lt;/p&gt;
&lt;p&gt;One distinctive aspect of Supabase is its &lt;strong&gt;real-time capabilities&lt;/strong&gt;, enabled by its integration with PostgREST and PostgreSQL's logical decoding feature. This allows developers to build real-time applications that can react to changes in vector data. Supabase also provides a user-friendly &lt;strong&gt;interface&lt;/strong&gt; and &lt;strong&gt;client libraries&lt;/strong&gt; for &lt;strong&gt;various programming languages&lt;/strong&gt;, making it accessible to developers with different skill sets. With its combination of vector storage and real-time capabilities, Supabase is an excellent choice for NLP projects that require both scalability and real-time updates.&lt;/p&gt;
&lt;p&gt;&lt;a id="qdrant"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="qdrant"&gt;Qdrant&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/qdrant/qdrant?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Qdrant logo" src="/images/vectordb/qdrant.png"&gt;&lt;/p&gt;
&lt;p&gt;Qdrant is an open-source vector database designed for similarity search and efficient storage of high-dimensional embeddings. It leverages an approximate nearest neighbor (ANN) algorithm based on Hierarchical Navigable Small World (HNSW) graphs, enabling fast and accurate similarity searches. Qdrant supports both CPU and GPU inference, allowing users to leverage hardware acceleration for faster computations.&lt;/p&gt;
&lt;p&gt;One notable feature of Qdrant is its &lt;strong&gt;RESTful API&lt;/strong&gt;, which provides a user-friendly &lt;strong&gt;interface for indexing, searching, and managing vector data&lt;/strong&gt;. Qdrant also offers &lt;strong&gt;flexible query options&lt;/strong&gt;, allowing users to specify search parameters and control the trade-off between accuracy and speed. With its focus on efficient similarity search and user-friendly API, Qdrant is a powerful vector database for various NLP applications.&lt;/p&gt;
&lt;p&gt;&lt;a id="vespa"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="vespa"&gt;Vespa&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/vespa-engine/vespa?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="vespa logo" src="https://vespa.ai/assets/vespa-logo.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://vespa.ai/"&gt;Vespa&lt;/a&gt; is an open-source big data processing and serving engine developed by Verizon Media. It provides a distributed, scalable, and high-performance infrastructure for storing and querying vector embeddings. Vespa utilizes an inverted index structure combined with approximate nearest neighbor (ANN) search algorithms for efficient and accurate similarity searches.&lt;/p&gt;
&lt;p&gt;One of Vespa's key features is its &lt;strong&gt;built-in ranking framework&lt;/strong&gt;, allowing developers to define custom ranking models and apply &lt;strong&gt;complex ranking algorithms to search results&lt;/strong&gt;. Vespa also supports &lt;strong&gt;real-time updates&lt;/strong&gt;, making it suitable for &lt;strong&gt;dynamic embedding datasets&lt;/strong&gt;. Additionally, Vespa provides a &lt;strong&gt;query language&lt;/strong&gt; and a user-friendly &lt;strong&gt;WebUI&lt;/strong&gt; for managing and monitoring the vector database. With its focus on &lt;strong&gt;distributed processing&lt;/strong&gt; and advanced ranking capabilities, Vespa is a powerful tool for NLP applications that require complex ranking models and real-time updates.&lt;/p&gt;
&lt;p&gt;&lt;a id="weaviate"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="weaviate"&gt;Weaviate&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/semi-technologies/weaviate?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Weaviate logo" src="/images/vectordb/weaviate.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://weaviate.io/"&gt;Weaviate&lt;/a&gt; is an open-source knowledge graph and vector search engine that excels in handling high-dimensional embeddings. It combines the power of graph databases and vector search to provide efficient storage, retrieval, and exploration of vector data. Weaviate offers powerful indexing methods, including approximate nearest neighbor (ANN) algorithms like HNSW, for fast and accurate similarity searches.&lt;/p&gt;
&lt;p&gt;One unique aspect of Weaviate is its &lt;strong&gt;focus on semantics and contextual relationships&lt;/strong&gt;. It allows users to define &lt;strong&gt;custom schema and relationships between entities&lt;/strong&gt;, enabling &lt;strong&gt;complex queries that go beyond simple vector similarity&lt;/strong&gt;. Weaviate also provides a &lt;strong&gt;RESTful API&lt;/strong&gt;, client libraries, and a user-friendly &lt;strong&gt;WebUI&lt;/strong&gt; for easy integration and management. With its combination of &lt;strong&gt;graph database features&lt;/strong&gt; and vector search capabilities, Weaviate is an excellent choice &lt;strong&gt;for NLP applications that require semantic understanding and exploration of embeddings&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="deeplake"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="deeplake"&gt;DeepLake&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/activeloopai/deeplake?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="DeepLake logo" src="https://camo.githubusercontent.com/d0c805affb06f5ea9ba767de06b77a04de54a7ef433fad08b2729d5e6b11112c/68747470733a2f2f692e706f7374696d672e63632f72736a63576333532f646565706c616b652d6c6f676f2e706e67"&gt;
&lt;a href="https://www.activeloop.ai/"&gt;DeepLake&lt;/a&gt; is an open-source vector database designed for efficient storage and retrieval of embeddings. It focuses on scalability and speed, making it suitable for handling large-scale NLP datasets. DeepLake provides a distributed architecture with built-in support for horizontal scalability, allowing users to handle massive volumes of vector data.&lt;/p&gt;
&lt;p&gt;One unique feature of DeepLake is its support for &lt;strong&gt;distributed vector indexing and querying&lt;/strong&gt;. It leverages an &lt;strong&gt;ANN&lt;/strong&gt; algorithm based on the Product Quantization (PQ) method, enabling fast and accurate similarity searches. DeepLake also provides a &lt;strong&gt;RESTful API&lt;/strong&gt; for easy integration with NLP pipelines and frameworks. With its emphasis on &lt;strong&gt;scalability and distributed processing&lt;/strong&gt;, DeepLake is a robust vector database for demanding NLP applications.&lt;/p&gt;
&lt;p&gt;&lt;a id="vectorstore-from-langchain"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="vectorstore-from-langchain"&gt;VectorStore from LangChain&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/hwchase17/langchain?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;LangChain &lt;a href="https://docs.langchain.com/docs/components/indexing/vectorstore"&gt;VectorStore&lt;/a&gt; is an open-source vector database optimized for multilingual NLP applications. It focuses on efficient storage and retrieval of embeddings across multiple languages. VectorStore supports various indexing methods, including approximate nearest neighbor (ANN) algorithms like HNSW and Annoy, for fast similarity searches.&lt;/p&gt;
&lt;p&gt;One distinguishing feature of VectorStore is its &lt;strong&gt;language-specific indexing&lt;/strong&gt; and &lt;strong&gt;retrieval capabilities&lt;/strong&gt;. It provides &lt;strong&gt;language-specific tokenization&lt;/strong&gt; and &lt;strong&gt;indexing strategies&lt;/strong&gt; to &lt;strong&gt;optimize search accuracy for different languages&lt;/strong&gt;. VectorStore also offers a &lt;strong&gt;RESTful API&lt;/strong&gt; and client libraries for easy integration with NLP pipelines. With its multilingual support and language-specific indexing, VectorStore is an excellent choice for projects that deal with embeddings across multiple languages.&lt;/p&gt;
&lt;p&gt;&lt;a id="other-relevant-vector-databases"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="other-relevant-vector-databases"&gt;Other Relevant Vector Databases&lt;/h3&gt;
&lt;p&gt;While the above tools represent some of the best vector databases available for storing embeddings in NLP, there are other notable options worth exploring:&lt;/p&gt;
&lt;h4 id="annoy"&gt;Annoy&lt;/h4&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/spotify/annoy?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;Annoy is a lightweight C++ library for approximate nearest neighbor (ANN) search, offering efficient indexing and querying of high-dimensional embeddings.&lt;/p&gt;
&lt;h4 id="elasticsearch"&gt;Elasticsearch&lt;/h4&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/elastic/elasticsearch?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;Elasticsearch is a popular distributed search and analytics engine that can be used to store and retrieve vector embeddings efficiently.&lt;/p&gt;
&lt;h4 id="hnswlib"&gt;Hnswlib&lt;/h4&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/nmslib/hnswlib?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;Hnswlib is a C++ library for efficient approximate nearest neighbor (ANN) search, providing high-performance indexing and retrieval of embeddings.&lt;/p&gt;
&lt;h4 id="nmslib"&gt;NMSLIB&lt;/h4&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/nmslib/nmslib?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;NMSLIB is an open-source library for similarity search, offering a range of indexing methods and data structures for efficient storage and retrieval of embeddings.&lt;/p&gt;
&lt;p&gt;These vector databases provide additional options and features that may suit specific requirements or preferences. Exploring these alternatives can help developers find the best fit for their NLP projects.&lt;/p&gt;
&lt;p&gt;To explore more, often lesser-known libraries you can use GitHub's topic search: &lt;a href="https://github.com/topics/vector-database"&gt;vector-database · GitHub Topics · GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="tabular-summary-of-the-features"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tabular-summary-of-the-features"&gt;Tabular summary of the features&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tool&lt;/th&gt;
&lt;th&gt;Scalability&lt;/th&gt;
&lt;th&gt;Query Speed&lt;/th&gt;
&lt;th&gt;Search Accuracy&lt;/th&gt;
&lt;th&gt;Flexibility&lt;/th&gt;
&lt;th&gt;Persistence&lt;/th&gt;
&lt;th&gt;Storage Location&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Chroma&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepsetAI&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faiss&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Milvus&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pgvector&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pinecone&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Supabase&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Qdrant&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vespa&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Weaviate&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepLake&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LangChain VectorStore&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Annoy&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Elasticsearch&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hnswlib&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NMSLIB&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Local/Cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a id="recommendations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="recommendations"&gt;Recommendations&lt;/h2&gt;
&lt;p&gt;Please find recommendations for three groups of use cases
&lt;a id="easy-start-and-user-friendliness---good-for-poc"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="easy-start-and-user-friendliness-good-for-poc"&gt;Easy Start and User-Friendliness - good for PoC&lt;/h3&gt;
&lt;p&gt;In this group, the focus is on vector databases that are easy to start with and user-friendly, even if they may sacrifice some advanced capabilities or performance.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chroma&lt;/strong&gt;: Chroma is an excellent choice for this group due to its simplicity and ease of use. It provides a straightforward API and offers out-of-the-box functionality for vector storage and retrieval. While it may not have the same level of scalability or advanced search algorithms as some other tools, it is ideal for small to medium-sized projects or beginners who want to quickly get started with vector databases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepsetAI&lt;/strong&gt;: DeepsetAI is another tool that prioritizes user-friendliness without compromising on essential functionalities. It offers a user-friendly interface, powerful search capabilities, and easy integration into existing NLP workflows. DeepsetAI is well-suited for developers who want a simple and efficient solution for storing and querying vector embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="advanced-capabilities-and-performance"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="advanced-capabilities-and-performance"&gt;Advanced Capabilities and Performance&lt;/h3&gt;
&lt;p&gt;In this group, we consider vector databases that provide advanced capabilities and high-performance, catering to more demanding use cases.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Faiss&lt;/strong&gt;: Faiss is a widely used and highly performant vector database that specializes in efficient similarity search. It offers a range of indexing structures and search algorithms, making it suitable for large-scale projects that require fast and accurate retrieval of embeddings. Faiss is an optimal choice when performance and search accuracy are critical.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Milvus&lt;/strong&gt;: Milvus is another powerful vector database known for its scalability and performance. It provides distributed storage and indexing, allowing for efficient handling of large-scale embedding datasets. Milvus supports various indexing algorithms, including approximate nearest neighbor (ANN) search, enabling fast similarity search. It is a robust solution for projects that demand scalability, high-performance, and flexibility.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="customization-and-advanced-use-cases"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="customization-and-advanced-use-cases"&gt;Customization and Advanced Use Cases&lt;/h3&gt;
&lt;p&gt;In this group, we consider vector databases that offer extensive customization options and cater to advanced use cases with specific requirements.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pinecone&lt;/strong&gt;: Pinecone is a vector database that excels in providing real-time search capabilities and high scalability. It offers advanced features such as dynamic indexing, custom similarity functions, and efficient updates, making it ideal for applications that require real-time embeddings and constant model refinement.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Supabase&lt;/strong&gt;: Supabase is an open-source database platform that provides a wide range of features, including support for vector storage and retrieval. With its flexibility and customizability, Supabase is suitable for projects that require not only vector database functionality but also the benefits of a comprehensive database platform.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By considering the diverse requirements of each group, we have recommended vector databases that prioritize ease of use, advanced capabilities, and customization. These recommendations aim to assist developers in selecting the most appropriate vector database for their specific use case and level of expertise.
&lt;a id="related-reading"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="related-reading"&gt;Related reading&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://lunabrain.com/blog/riding-the-ai-wave-with-vector-databases-how-they-work-and-why-vcs-love-them/"&gt;Riding the AI Wave with Vector Databases: How they work (and why VCs love them) - LunaBrain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://harishgarg.com/writing/best-vector-databases-for-ai-apps/"&gt;10 Best vector databases for building AI Apps with embeddings - HarishGarg.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://thenewstack.io/vector-databases-long-term-memory-for-artificial-intelligence/"&gt;Vector Databases: Long-Term Memory for Artificial Intelligence - The New Stack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/sopmac-ai/vector-databases-as-memory-for-your-ai-agents-986288530443"&gt;Vector Databases as Memory for your AI Agents | by Ivan Campos | Sopmac AI | Apr, 2023 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://venturebeat.com/ai/how-vector-databases-can-revolutionize-our-relationship-with-generative-ai/"&gt;How vector databases can revolutionize our relationship with generative AI | VentureBeat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.forbes.com/sites/adrianbridgwater/2023/05/19/the-rise-of-vector-databases/"&gt;Vector databases provide new ways to enable search and data analytics.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://betterprogramming.pub/openais-embedding-model-with-vector-database-b69014f04433"&gt;OpenAI’s Embeddings with Vector Database | Better Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vector Databases Demystified serie by &lt;a href="https://www.linkedin.com/in/adiekaye/"&gt;Adie Kaye&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/pulse/vector-databases-demystified-part-1-introduction-world-adie-kaye%3FtrackingId=Rswjt%252BgljDJ9YTjMB08LWw%253D%253D/?trackingId=Rswjt%2BgljDJ9YTjMB08LWw%3D%3D"&gt;Part 1 - An Introduction to the World of High-Dimensional Data Storage&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/vector-databases-demystified-part-2-building-your-own-adie-kaye?trackingId=CRILIdZ0zUFLlj3EZ69gXQ%3D%3D&amp;amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_recent_activity_content_view%3B1s%2FDztmATJWjL%2BLIoqi0XQ%3D%3D"&gt;Part 2 - Building Your Own (Very) Simple Vector Database in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/vector-databases-demystified-part-3-build-colour-matching-adie-kaye?trackingId=sS3mR3KmPvSwcPwdMJvbFQ%3D%3D&amp;amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_recent_activity_content_view%3B1s%2FDztmATJWjL%2BLIoqi0XQ%3D%3D"&gt;Part 3 - Build a colour matching app with Pinecone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/vector-databases-demystified-part-4-using-sentence-pinecone-kaye?trackingId=vfLY3dFcGw%2FVygrCCFKZIQ%3D%3D"&gt;Part 4 - Using Sentence Transformers with Pinecone&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Generative AI"/><category term="machine-learning"/><category term="python"/><category term="embeddings"/><category term="vectordb"/><category term="database"/><category term="transformers"/><category term="chroma"/><category term="langchain"/><category term="pinecone"/><category term="haystack"/><category term="faiss"/><category term="milvus"/><category term="pgvector"/><category term="supabase"/><category term="qdrant"/><category term="vespa"/><category term="weaviate"/><category term="deeplake"/><category term="vectorstore"/></entry><entry><title>Mastering the Kanban Method - Unveiling the Hidden Gems of Effective Kanban Board Usage</title><link href="http://127.0.0.1:8000/mastering-kanban-method/" rel="alternate"/><published>2023-05-26T00:00:00+02:00</published><updated>2023-05-26T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-05-26:/mastering-kanban-method/</id><summary type="html">&lt;p&gt;Ever wondered how to supercharge your team's productivity? Say hello to Kanban, the dynamic method that brings clarity and efficiency to your projects.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In today's fast-paced and ever-evolving business landscape, organizations are constantly seeking efficient project management methodologies to enhance productivity and streamline workflows. One such approach that has gained significant popularity is the Kanban method. Kanban, originating from the Japanese word for "billboard" or "visual card," is a visual project management system that allows teams to track and manage work effectively. In this comprehensive guide, we will look into the specificity of the Kanban method, explore the proper utilization of Kanban boards, and reveal lesser-known tips and tricks to maximize their potential.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#understanding-the-kanban-method-principles"&gt;Understanding the Kanban Method Principles&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#visualize-your-workflow"&gt;Visualize Your Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#limit-work-in-progress-wip"&gt;Limit Work in Progress (WIP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#collaborate-and-encourage-flow"&gt;Collaborate and Encourage Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#continuously-improve"&gt;Continuously Improve&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#avoiding-common-mistakes"&gt;Avoiding Common Mistakes&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#neglecting-wip-limits"&gt;Neglecting WIP Limits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lack-of-clarity-and-standardization"&gt;Lack of Clarity and Standardization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#failure-to-prioritize-and-swarm"&gt;Failure to Prioritize and Swarm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lack-of-continuous-improvement"&gt;Lack of Continuous Improvement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#unveiling-lesser-known-tips-and-tricks"&gt;Unveiling Lesser-Known Tips and Tricks&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#class-of-service"&gt;Class of Service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visualizing-blocked-tasks"&gt;Visualizing Blocked Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kanban-swimlanes"&gt;Kanban Swimlanes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementing-agile-practices"&gt;Implementing Agile Practices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="understanding-the-kanban-method-principles"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="understanding-the-kanban-method-principles"&gt;Understanding the Kanban Method Principles&lt;/h2&gt;
&lt;p&gt;The Kanban method, developed by Taiichi Ohno at Toyota, is built on the principles of visualizing work, limiting work in progress (WIP), and focusing on continuous improvement. At its core, Kanban promotes transparency, flexibility, and collaboration, providing teams with a clear overview of their tasks and enabling them to optimize their workflows.&lt;/p&gt;
&lt;p&gt;&lt;a id="visualize-your-workflow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="visualize-your-workflow"&gt;Visualize Your Workflow&lt;/h3&gt;
&lt;p&gt;The fundamental principle of Kanban lies in visualizing your workflow. By representing each task as a card or sticky note on a Kanban board, teams gain a shared understanding of the work in progress. A typical Kanban board comprises columns that depict different stages of work, such as "To Do," "In Progress," and "Done." Visualizing tasks fosters transparency, enhances communication, and enables team members to identify bottlenecks or inefficiencies quickly.&lt;/p&gt;
&lt;p&gt;&lt;a id="limit-work-in-progress-wip"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="limit-work-in-progress-wip"&gt;Limit Work in Progress (WIP)&lt;/h3&gt;
&lt;p&gt;To maintain a smooth workflow and prevent overburdening team members, it is crucial to limit the number of tasks in progress simultaneously. Setting WIP limits for each column on the Kanban board ensures a manageable workload, promotes focus, and encourages completing tasks before moving on to new ones. WIP limits prevent multitasking, which can lead to reduced productivity and increased lead times.&lt;/p&gt;
&lt;p&gt;&lt;a id="collaborate-and-encourage-flow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="collaborate-and-encourage-flow"&gt;Collaborate and Encourage Flow&lt;/h3&gt;
&lt;p&gt;Kanban encourages collaboration and cross-functional teamwork. By eliminating silos and fostering a culture of shared responsibility, teams can achieve a seamless flow of work. Encourage frequent communication, promote knowledge sharing, and embrace a collective ownership mindset to optimize the overall efficiency of your Kanban system.&lt;/p&gt;
&lt;p&gt;&lt;a id="continuously-improve"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="continuously-improve"&gt;Continuously Improve&lt;/h3&gt;
&lt;p&gt;The Kanban method is rooted in the philosophy of continuous improvement. Encourage your team to reflect on their processes, identify areas of improvement, and implement changes accordingly. By regularly reviewing your Kanban board, analyzing cycle times, and seeking feedback from team members, you can refine your workflows, streamline processes, and enhance overall productivity.&lt;/p&gt;
&lt;p&gt;&lt;a id="avoiding-common-mistakes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="avoiding-common-mistakes"&gt;Avoiding Common Mistakes&lt;/h2&gt;
&lt;p&gt;While the Kanban method offers numerous benefits, it's important to be aware of common pitfalls that can hinder its effectiveness. By recognizing and avoiding these mistakes, you can ensure your Kanban implementation is successful.&lt;/p&gt;
&lt;p&gt;&lt;a id="neglecting-wip-limits"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="neglecting-wip-limits"&gt;Neglecting WIP Limits&lt;/h3&gt;
&lt;p&gt;One common mistake is neglecting WIP limits or setting them too high. Failing to adhere to WIP limits can lead to task overload, reduced focus, and increased lead times. Regularly review and adjust WIP limits based on team capacity and project requirements.&lt;/p&gt;
&lt;p&gt;&lt;a id="lack-of-clarity-and-standardization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="lack-of-clarity-and-standardization"&gt;Lack of Clarity and Standardization&lt;/h3&gt;
&lt;p&gt;Without clear guidelines and standardized practices, teams may interpret Kanban differently, leading to confusion and inconsistency. Establish explicit rules for how tasks should be represented on the board, how updates are communicated, and how metrics are measured. Consistency ensures everyone understands the workflow and can collaborate effectively.&lt;/p&gt;
&lt;p&gt;&lt;a id="failure-to-prioritize-and-swarm"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="failure-to-prioritize-and-swarm"&gt;Failure to Prioritize and Swarm&lt;/h3&gt;
&lt;p&gt;In Kanban, it's important to prioritize tasks and encourage the team to focus on completing them one at a time. Neglecting prioritization can lead to cherry-picking tasks or tackling low-value items first. Additionally, encourage swarming, where team members collaborate to complete tasks together, rather than working individually, to maximize efficiency and knowledge sharing.&lt;/p&gt;
&lt;p&gt;&lt;a id="lack-of-continuous-improvement"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="lack-of-continuous-improvement"&gt;Lack of Continuous Improvement&lt;/h3&gt;
&lt;p&gt;One of the main principles of Kanban is continuous improvement. Failing to allocate time for retrospectives, process analysis, and incremental changes can hinder your team's growth and limit the full potential of your Kanban system. Regularly review and refine your workflows to ensure ongoing progress and evolution.&lt;/p&gt;
&lt;p&gt;&lt;a id="unveiling-lesser-known-tips-and-tricks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="unveiling-lesser-known-tips-and-tricks"&gt;Unveiling Lesser-Known Tips and Tricks&lt;/h2&gt;
&lt;p&gt;Now, let's uncover some lesser-known tips and tricks that can take your Kanban practice to the next level, boosting your team's productivity and overall success.&lt;/p&gt;
&lt;p&gt;&lt;a id="class-of-service"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="class-of-service"&gt;Class of Service&lt;/h3&gt;
&lt;p&gt;Introduce the concept of "Class of Service" to prioritize tasks based on their impact and urgency. By assigning different classes to tasks, such as expedite, standard, or fixed-date, teams can ensure that critical work is appropriately prioritized and expedited, while still maintaining a steady flow.&lt;/p&gt;
&lt;p&gt;&lt;a id="visualizing-blocked-tasks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="visualizing-blocked-tasks"&gt;Visualizing Blocked Tasks&lt;/h3&gt;
&lt;p&gt;In addition to representing tasks in progress, leverage the Kanban board to highlight blocked or stalled tasks. Use specific indicators or flags to denote issues preventing progress, such as dependencies, resource constraints, or waiting for external feedback. This visual cue helps the team focus on resolving blockers and ensures smoother workflow management.&lt;/p&gt;
&lt;p&gt;&lt;a id="kanban-swimlanes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="kanban-swimlanes"&gt;Kanban Swimlanes&lt;/h3&gt;
&lt;p&gt;Introduce swimlanes on your Kanban board to categorize tasks based on different criteria, such as priority, team member, or project phase. Swimlanes provide a higher level of organization and enable teams to filter and analyze their work in a more granular manner. This approach can be particularly beneficial for larger teams or complex projects.&lt;/p&gt;
&lt;p&gt;&lt;a id="implementing-agile-practices"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="implementing-agile-practices"&gt;Implementing Agile Practices&lt;/h3&gt;
&lt;p&gt;Combine Kanban with agile practices to amplify its impact. Techniques like daily stand-ups, sprint planning, and retrospectives can complement the visual nature of Kanban, fostering enhanced collaboration, transparency, and adaptability within your team.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The Kanban method, with its emphasis on visualization, limiting work in progress, and continuous improvement, offers organizations a powerful tool to optimize their workflows and enhance team productivity. By avoiding common mistakes and incorporating lesser-known tips and tricks, teams can unlock the full potential of Kanban, streamline their processes, and achieve remarkable results. Embrace the power of Kanban, and watch your projects flourish in an environment of transparency, collaboration, and continuous improvement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: heading image from &lt;a href="https://unsplash.com/photos/OXmym9cuaEY"&gt;unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@edenconstantin0"&gt;edenconstantin0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="kanban-method"/><category term="kanban-board"/><category term="project-management"/><category term="workflow-management"/><category term="productivity"/><category term="visual-management"/><category term="work-in-progress"/><category term="collaboration"/><category term="continuous-improvement"/><category term="task-management"/><category term="agile-methodology"/><category term="prioritization"/><category term="team-efficiency"/><category term="transparency"/><category term="WIP-limits"/><category term="cross-functional-teamwork"/><category term="cycle-times"/><category term="retrospectives"/><category term="swarming"/><category term="class-of-service"/><category term="blocked-tasks"/><category term="swimlanes"/><category term="agile-practices"/></entry><entry><title>Attacking Differential Privacy Using the Correlation Between the Features</title><link href="http://127.0.0.1:8000/attacking-differential-privacy-using-the-correlation-between-the-features/" rel="alternate"/><published>2023-04-19T00:00:00+02:00</published><updated>2023-04-19T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-19:/attacking-differential-privacy-using-the-correlation-between-the-features/</id><summary type="html">&lt;p&gt;Learn how the differential privacy works by simulating attack on data protected with that technique.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Differential privacy is a technique that adds random noise to the data to protect individual privacy while still allowing for accurate data analysis. However, differential privacy can still be vulnerable to attacks that can compromise the privacy of individuals. One such attack is through the use of correlation between features. In this blog post, we will discuss how an attacker can use correlation between features to attack differential privacy and how to mitigate this attack.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#background"&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#correlation-between-features"&gt;Correlation Between Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#steps-for-the-attack-using-correlation-between-features"&gt;Steps for the attack using correlation between features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-identify-highly-correlated-features"&gt;1. Identify highly correlated features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-compute-expected-values"&gt;2. Compute expected values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-compare-expected-and-observed-values"&gt;3. Compare expected and observed values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mitigating-the-attack"&gt;Mitigating the Attack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tutorial"&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#select-a-dataset-that-requires-privacy-protection"&gt;Select a dataset that requires privacy protection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#apply-differential-privacy"&gt;Apply differential privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#perform-the-attack---reconstruct-original-data-by-exploiting-correlation-between-features"&gt;Perform the attack - reconstruct original data by exploiting correlation between features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="background"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Differential privacy adds random noise to the data to protect the privacy of individuals. The amount of noise added depends on a parameter called the privacy budget. The higher the privacy budget, the less noise is added, and the lower the privacy budget, the more noise is added. The privacy budget is usually set based on the desired level of privacy and the size of the data set. A smaller privacy budget leads to better privacy but less accurate data analysis, while a larger privacy budget leads to less privacy but more accurate data analysis.&lt;/p&gt;
&lt;p&gt;&lt;a id="correlation-between-features"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="correlation-between-features"&gt;Correlation Between Features&lt;/h2&gt;
&lt;p&gt;In many data sets, the features are not independent but are correlated with each other. Correlation between features can be measured using the correlation coefficient. The correlation coefficient between two features x and y is defined as:&lt;/p&gt;
&lt;p&gt;$$
ρ_{x,y} = cov(x,y) / (σ_x * σ_y)
$$&lt;/p&gt;
&lt;p&gt;where $cov(x,y)$ is the covariance between $x$ and $y$, and $\sigma_x$ and $\sigma_y$ are the standard deviations of $x$ and $y$, respectively.&lt;/p&gt;
&lt;p&gt;Correlation between features can be used to attack differential privacy. An attacker can use the correlation between features to infer the presence or absence of an individual's data in the data set. For example, suppose an attacker knows that two features x and y are highly correlated. If the attacker sees that the value of y is very different from what they would expect based on the value of x, they can infer that the individual's data was not included in the data set.&lt;/p&gt;
&lt;p&gt;&lt;a id="steps-for-the-attack-using-correlation-between-features"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="steps-for-the-attack-using-correlation-between-features"&gt;Steps for the attack using correlation between features&lt;/h2&gt;
&lt;p&gt;An attacker can use the following steps to attack differential privacy using correlation between features:&lt;/p&gt;
&lt;p&gt;&lt;a id="1-identify-highly-correlated-features"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-identify-highly-correlated-features"&gt;1. Identify highly correlated features&lt;/h3&gt;
&lt;p&gt;The attacker identifies which features in the data set are highly correlated with each other.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-compute-expected-values"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-compute-expected-values"&gt;2. Compute expected values&lt;/h3&gt;
&lt;p&gt;The attacker computes the expected values of the features based on the values of the other features.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-compare-expected-and-observed-values"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-compare-expected-and-observed-values"&gt;3. Compare expected and observed values&lt;/h3&gt;
&lt;p&gt;The attacker compares the expected values with the observed values of the features. If the observed values are significantly different from the expected values, the attacker can infer that the individual's data was not included in the data set.&lt;/p&gt;
&lt;p&gt;&lt;a id="mitigating-the-attack"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mitigating-the-attack"&gt;Mitigating the Attack&lt;/h2&gt;
&lt;p&gt;There are several ways to mitigate the attack using correlation between features. One approach is to &lt;strong&gt;decorrelate the features&lt;/strong&gt; by transforming the data. For example, principal component analysis (PCA) can be used to decorrelate the features. Another approach is to &lt;strong&gt;add noise to the data&lt;/strong&gt; in a way that preserves the correlation between features. This approach is called differentially private PCA (DP-PCA). DP-PCA adds noise to the data in a way that satisfies differential privacy while preserving the correlation between features.&lt;/p&gt;
&lt;p&gt;&lt;a id="summary"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;Correlation between features can be used to attack differential privacy. An attacker can use the correlation between features to infer the presence or absence of an individual's data in the data set. To mitigate this attack, the features can be decorrelated or noise can be added to the data using DP-PCA. Data security experts should be aware of this attack and take appropriate measures to mitigate its effects.&lt;/p&gt;
&lt;p&gt;&lt;a id="tutorial"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tutorial"&gt;Tutorial&lt;/h2&gt;
&lt;p&gt;In this tutorial, we will go through the steps of attacking differential privacy by exploiting correlations between features, using Python code to demonstrate each step.&lt;/p&gt;
&lt;p&gt;In the tutorial we will be using pydp Python library, so you need to install it first:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;python-dp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="select-a-dataset-that-requires-privacy-protection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="select-a-dataset-that-requires-privacy-protection"&gt;Select a dataset that requires privacy protection&lt;/h3&gt;
&lt;p&gt;For this tutorial, we will use the Adult dataset from the UCI Machine Learning Repository. This dataset contains information about individuals, including their age, education level, marital status, occupation, and more. The goal is to predict whether an individual earns more than $50K per year. We will load this dataset using pandas:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;workclass&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;fnlwgt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;education&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;marital-status&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="s2"&gt;&amp;quot;occupation&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;relationship&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;race&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;sex&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;capital-gain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;capital-loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="s2"&gt;&amp;quot;hours-per-week&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;native-country&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;income&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="apply-differential-privacy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="apply-differential-privacy"&gt;Apply differential privacy&lt;/h3&gt;
&lt;p&gt;We will use the PyDP library to apply differential privacy to the dataset. We will add Laplace noise to the age and education-num features, with a privacy budget of 1.0:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pydp.algorithms.laplacian&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BoundedMean&lt;/span&gt;

&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

&lt;span class="c1"&gt;# apply differential privacy to age&lt;/span&gt;
&lt;span class="n"&gt;bm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BoundedMean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;bm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quick_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# apply differential privacy to education-num&lt;/span&gt;
&lt;span class="n"&gt;bm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BoundedMean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;bm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quick_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="perform-the-attack---reconstruct-original-data-by-exploiting-correlation-between-features"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="perform-the-attack-reconstruct-original-data-by-exploiting-correlation-between-features"&gt;Perform the attack - reconstruct original data by exploiting correlation between features&lt;/h3&gt;
&lt;p&gt;Now that we have applied differential privacy to the dataset, we will attempt to reconstruct the original data by exploiting the correlation between features. Specifically, we will use the age and education-num features, which we know are highly correlated, to infer the values of the original data.&lt;/p&gt;
&lt;p&gt;First, we will create a copy of the dataset and remove the age and education-num features, as we will be reconstructing these features:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df_attack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we will compute the mean and covariance matrix of the remaining features:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# compute mean and covariance of remaining features&lt;/span&gt;
&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_attack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cov&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_attack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now use the mean and covariance matrix to generate synthetic data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# generate synthetic data&lt;/span&gt;
&lt;span class="n"&gt;synthetic_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;synthetic_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;synthetic_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;df_attack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, we will reconstruct the age and education-num features using the generated synthetic data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# reconstruct age and education-num features&lt;/span&gt;
&lt;span class="n"&gt;reconstructed_age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;reconstructed_edu_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# combine reconstructed features with original data&lt;/span&gt;
&lt;span class="n"&gt;reconstructed_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;reconstructed_age&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reconstructed_edu_num&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="n"&gt;df_reconstructed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;df_attack&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reconstructed_df&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now compare the reconstructed age and education-num features with the original features to see how well our attack worked:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# compare reconstructed age and education-num with original features print(&amp;quot;Age:&amp;quot;) &lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Original:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Reconstructed:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reconstructed_age&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; 

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Education-num:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Original:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;education-num&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Reconstructed:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reconstructed_edu_num&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;Age&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;Original&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;39&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="mi"&gt;38&lt;/span&gt; &lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="mi"&gt;37&lt;/span&gt; &lt;span class="mi"&gt;49&lt;/span&gt; &lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="mi"&gt;31&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Reconstructed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;39.38640885&lt;/span&gt; &lt;span class="mf"&gt;49.44619487&lt;/span&gt; &lt;span class="mf"&gt;38.2757904&lt;/span&gt;  &lt;span class="mf"&gt;52.75103613&lt;/span&gt; &lt;span class="mf"&gt;26.46121269&lt;/span&gt; &lt;span class="mf"&gt;37.760824&lt;/span&gt;
 &lt;span class="mf"&gt;47.88143872&lt;/span&gt; &lt;span class="mf"&gt;52.8530772&lt;/span&gt;  &lt;span class="mf"&gt;30.79760633&lt;/span&gt; &lt;span class="mf"&gt;42.56495885&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Education&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;Original&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Reconstructed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;13.19164695&lt;/span&gt; &lt;span class="mf"&gt;13.19406455&lt;/span&gt;  &lt;span class="mf"&gt;9.04750693&lt;/span&gt;  &lt;span class="mf"&gt;6.8549391&lt;/span&gt;  &lt;span class="mf"&gt;13.25155432&lt;/span&gt; &lt;span class="mf"&gt;13.76664294&lt;/span&gt;
  &lt;span class="mf"&gt;5.45598348&lt;/span&gt;  &lt;span class="mf"&gt;8.72003132&lt;/span&gt; &lt;span class="mf"&gt;14.14489928&lt;/span&gt; &lt;span class="mf"&gt;12.9968581&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we can see, the reconstructed values are quite similar to the original values. This suggests that an attacker could use the correlation between the age and education-num features to infer the original values, even with the protection of differential privacy.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this tutorial, we have demonstrated how an attacker can exploit correlations between features to attack differential privacy. We used the PyDP library to apply differential privacy to a dataset, and then showed how an attacker could use the correlation between the age and education-num features to reconstruct the original values. This highlights the importance of considering the correlations between features when applying differential privacy, and suggests that additional protections may be necessary to prevent attacks based on feature correlations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Responsible AI"/><category term="machine-learning"/><category term="python"/><category term="privacy"/><category term="xai"/><category term="responsible-ai"/></entry><entry><title>Are LIME Explanations Any Useful?</title><link href="http://127.0.0.1:8000/are-lime-explanations-any-useful/" rel="alternate"/><published>2023-04-18T00:00:00+02:00</published><updated>2023-04-18T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-18:/are-lime-explanations-any-useful/</id><summary type="html">&lt;p&gt;Don't let black box models hold you back. With LIME, you can interpret the predictions of even the most complex machine learning models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;X::[[MOC_RAI]]&lt;/p&gt;
&lt;p&gt;LIME (Local Interpretable Model-agnostic Explanations) is a method used to interpret black box models. This technique is widely used in the field of data science to explain the predictions of complex machine learning models. By providing local explanations, LIME can help users understand the decision-making process of the models and increase their trust in the models' predictions. However, the question remains, are the local explanations obtained with LIME method useful? And what are the practical use cases when using LIME gave tangible results?&lt;/p&gt;
&lt;p&gt;In this article, we will look into the concept of LIME, its practical applications, and how it can be used to provide interpretable machine learning models.&lt;/p&gt;
&lt;h2 id="what-is-lime"&gt;What is LIME?&lt;/h2&gt;
&lt;p&gt;LIME is a model-agnostic technique used to explain the predictions of machine learning models. The idea behind LIME is to explain the predictions of a black box model by training a local, interpretable model around the data point of interest. The interpretable model is trained to mimic the behavior of the black box model around that data point. Once the local model is trained, it can be used to generate an explanation of the prediction, highlighting the most important features that contributed to the prediction.&lt;/p&gt;
&lt;p&gt;The LIME algorithm consists of the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a data point of interest.&lt;/li&gt;
&lt;li&gt;Generate a dataset of perturbed instances around the selected data point.&lt;/li&gt;
&lt;li&gt;Evaluate the black box model on the perturbed instances to obtain a set of weights that indicate the importance of each feature for the prediction.&lt;/li&gt;
&lt;li&gt;Train an interpretable model (such as a linear regression model) on the perturbed instances, using the weights obtained in step 3 as feature weights.&lt;/li&gt;
&lt;li&gt;Use the trained interpretable model to generate an explanation of the prediction for the selected data point.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="practical-applications-of-lime"&gt;Practical applications of LIME&lt;/h2&gt;
&lt;p&gt;LIME has been successfully applied in various domains, including healthcare, finance, and image recognition. Here are some practical use cases where LIME has been used to provide interpretable machine learning models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;: LIME has been used to interpret the predictions of machine learning models that diagnose diseases. For example, in a study conducted by Zech et al., LIME was used to interpret the predictions of a deep learning model that diagnosed pneumonia from chest X-rays. The LIME explanations provided by the study helped radiologists understand the decision-making process of the model and identify areas of the X-rays that contributed the most to the diagnosis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Finance&lt;/strong&gt;: LIME has also been used to interpret the predictions of machine learning models that predict financial outcomes. For example, in a study conducted by Chen et al., LIME was used to interpret the predictions of a machine learning model that predicted the credit risk of borrowers. The LIME explanations provided by the study helped lenders understand the factors that contributed to the credit risk prediction and make informed lending decisions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Image recognition&lt;/strong&gt;: LIME has been used to interpret the predictions of machine learning models that recognize images. For example, in a study conducted by Selvaraju et al., LIME was used to interpret the predictions of a deep learning model that recognized objects in images. The LIME explanations provided by the study helped users understand which parts of the image were important for the prediction and identify areas of improvement for the model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="benefits-and-limitations-of-lime"&gt;Benefits and limitations of LIME&lt;/h2&gt;
&lt;p&gt;LIME provides several &lt;strong&gt;benefits&lt;/strong&gt; to data scientists and machine learning practitioners.&lt;/p&gt;
&lt;p&gt;First, LIME &lt;strong&gt;can help increase the trust of users in machine learning models by providing interpretable explanations of the models' predictions&lt;/strong&gt;. This can be especially useful in high-stakes domains, such as healthcare and finance, where decisions based on machine learning predictions can have significant consequences.
Second, LIME &lt;strong&gt;can help users identify areas of improvement for machine learning models&lt;/strong&gt;. By providing explanations of the models' predictions, LIME can help users identify which features were important for the prediction and which ones were not. This can help users refine their feature engineering process and improve the performance of their models.&lt;/p&gt;
&lt;p&gt;However, LIME also has some &lt;strong&gt;limitations&lt;/strong&gt; that data scientists and machine learning practitioners should be aware of.&lt;/p&gt;
&lt;p&gt;First, LIME provides local explanations, which means that &lt;strong&gt;the explanations are only valid for the selected data point of interest&lt;/strong&gt;. Therefore, the explanations generated by LIME may not generalize to other data points.&lt;/p&gt;
&lt;p&gt;Second, LIME &lt;strong&gt;requires a significant amount of computational resources&lt;/strong&gt; to generate the perturbed instances and train the interpretable model. This can be a limitation when working with large datasets or computationally expensive models.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;LIME is a useful technique for interpreting the predictions of machine learning models. LIME can help increase the trust of users in machine learning models and identify areas of improvement for the models. LIME has been successfully applied in various domains, including healthcare, finance, and image recognition. However, LIME also has some limitations, such as providing local explanations and requiring significant computational resources. Therefore, data scientists and machine learning practitioners should carefully consider the use of LIME and its limitations when interpreting the predictions of their models.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Responsible AI"/><category term="machine-learning"/><category term="lime"/><category term="xai"/><category term="explainable-ai"/></entry><entry><title>Intrinsic vs. Extrinsic Evaluation - What's the Best Way to Measure Embedding Quality?</title><link href="http://127.0.0.1:8000/measure-quality-of-embeddings-intrinsic-vs-extrinsic/" rel="alternate"/><published>2023-04-18T00:00:00+02:00</published><updated>2023-04-18T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-18:/measure-quality-of-embeddings-intrinsic-vs-extrinsic/</id><summary type="html">&lt;p&gt;Learn how to measure the quality of word and sentence embeddings in natural language processing (NLP), including intrinsic and extrinsic evaluation, and their strengths and limitations.&lt;/p&gt;</summary><content type="html">&lt;p&gt;X::[[perplexity_in_dimensionality_reduction_PCA]]
X::[[emulating_human_expression_by_controlling_the_burstines_and_perplexity 1]]&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let's start with the concept of embedding vectors. In natural language processing (NLP), an embedding vector is a mathematical representation of words or phrases. It's a way to convert text data into numerical values that can be processed by machine learning algorithms. Word embeddings and sentence embeddings are widely used in natural language processing (NLP) for a variety of tasks, such as text classification, named entity recognition, machine translation, and sentiment analysis. However, it is not always straightforward to evaluate the quality of embeddings, and different evaluation metrics may be appropriate for different use cases. In this blog post, we will explore several ways to measure the quality of embeddings, including intrinsic and extrinsic evaluation, and discuss their strengths and limitations.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#intrinsic-evaluation"&gt;Intrinsic Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cosine-similarity"&gt;Cosine Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spearman-correlation"&gt;Spearman Correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#accuracy"&gt;Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#extrinsic-evaluation"&gt;Extrinsic Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#f1-score"&gt;F1 Score&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#perplexity"&gt;Perplexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#limitations"&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="intrinsic-evaluation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="intrinsic-evaluation"&gt;Intrinsic Evaluation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Intrinsic evaluation&lt;/strong&gt; - aims to measure the quality of embeddings by assessing their performance on specific NLP tasks that are related to the embedding space itself, such as word similarity, analogy, and classification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this section, we will discuss three commonly used intrinsic evaluation metrics: cosine similarity, Spearman correlation, and accuracy.&lt;/p&gt;
&lt;p&gt;&lt;a id="cosine-similarity"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="cosine-similarity"&gt;Cosine Similarity&lt;/h3&gt;
&lt;p&gt;Cosine similarity measures the similarity between two vectors by computing the cosine of the angle between them. In the context of embeddings, cosine similarity is often used to measure the similarity between two words, or between a word and its context. The formula for cosine similarity is as follows:&lt;/p&gt;
&lt;p&gt;$$
cosine_similarity(\textbf{v}_1, \textbf{v}_2) = \frac{\textbf{v}_1 \cdot \textbf{v}_2}{|\textbf{v}_1||\textbf{v}_2|}
$$&lt;/p&gt;
&lt;p&gt;where $\textbf{v}_1$ and $\textbf{v}_2$ are the embeddings of two words, and $|\cdot|$ denotes the Euclidean norm.&lt;/p&gt;
&lt;p&gt;&lt;a id="spearman-correlation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="spearman-correlation"&gt;Spearman Correlation&lt;/h3&gt;
&lt;p&gt;Spearman correlation measures the monotonic relationship between two variables, which can be the similarity scores of two sets of words or phrases computed by humans and by embeddings. A high Spearman correlation indicates that the embeddings are able to capture the semantic relationships between words that humans perceive. The formula for Spearman correlation is as follows:&lt;/p&gt;
&lt;p&gt;$$
\text{Spearman's correlation} = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}&lt;/p&gt;
&lt;p&gt;$$&lt;/p&gt;
&lt;p&gt;where $d_i$ is the difference between the ranks of the $i$-th pair of similarity scores, and $n$ is the number of pairs.&lt;/p&gt;
&lt;p&gt;&lt;a id="accuracy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="accuracy"&gt;Accuracy&lt;/h3&gt;
&lt;p&gt;Accuracy measures the performance of embeddings on classification tasks, such as sentiment analysis or topic classification. Given a dataset of labeled examples, the embeddings are used to represent each example, and a classifier is trained on these representations. The accuracy of the classifier on a held-out test set is then used as a measure of the quality of the embeddings.&lt;/p&gt;
&lt;p&gt;&lt;a id="extrinsic-evaluation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="extrinsic-evaluation"&gt;Extrinsic Evaluation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Extrinsic evaluation&lt;/strong&gt; - aims to measure the quality of embeddings by assessing their performance on downstream NLP tasks, such as machine translation or text classification, that are not directly related to the embedding space itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this section, we will discuss two commonly used extrinsic evaluation metrics: F1 score and perplexity.
&lt;a id="f1-score"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="f1-score"&gt;F1 Score&lt;/h3&gt;
&lt;p&gt;F1 score is a metric commonly used in binary classification problems, such as sentiment analysis or named entity recognition. It combines precision and recall into a single score that ranges from 0 to 1. A high F1 score indicates that the embeddings are able to capture the relevant features of the input data. The formula for F1 score is as follows:&lt;/p&gt;
&lt;p&gt;$$
F1 = \frac{2 \cdot \text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
$$&lt;/p&gt;
&lt;p&gt;where precision is the fraction of true positives among the predicted positives, and recall is the fraction of true positives among the actual positives.&lt;/p&gt;
&lt;p&gt;&lt;a id="perplexity"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="perplexity"&gt;Perplexity&lt;/h3&gt;
&lt;p&gt;Perplexity is a metric commonly used in language modeling tasks, such as machine translation or text generation. It measures how well a language model can predict a held-out test set of text, given the embeddings as input. A low perplexity indicates that the embeddings  are able to capture the semantic and syntactic structures of the language. The formula for perplexity is as follows:&lt;/p&gt;
&lt;p&gt;$$
\text{perplexity} = 2^{-\frac{1}{N}\sum_{i=1}^{N} \log_2 p(w_i | \textbf{e}_i)}
$$&lt;/p&gt;
&lt;p&gt;where $N$ is the number of tokens in the test set, $\textbf{e}_i$ is the embedding of the $i$-th token, and $p(w_i | \textbf{e}_i)$ is the conditional probability of the $i$-th token given its embedding.&lt;/p&gt;
&lt;p&gt;&lt;a id="limitations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="limitations"&gt;Limitations&lt;/h2&gt;
&lt;p&gt;While intrinsic and extrinsic evaluation metrics can provide useful insights into the quality of embeddings, they also have some limitations. Intrinsic evaluation may not always reflect the performance of embeddings on downstream tasks, and extrinsic evaluation may not always isolate the contribution of embeddings from other factors, such as the choice of model architecture or the quality of the training data. Moreover, the choice of evaluation metrics may depend on the specific use case and the available resources, and there is no one-size-fits-all solution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Generative AI"/><category term="machine-learning"/><category term="python"/><category term="embeddings"/><category term="cosine-similarity"/><category term="spearman-correlation"/><category term="accuracy"/><category term="f1-score"/><category term="perplexity"/></entry><entry><title>Explaining AI - The Key Differences Between LIME and SHAP Methods</title><link href="http://127.0.0.1:8000/explaining-ai-the-key-differences-between-lime-and-shap-methods/" rel="alternate"/><published>2023-04-14T00:00:00+02:00</published><updated>2023-04-14T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-14:/explaining-ai-the-key-differences-between-lime-and-shap-methods/</id><summary type="html">&lt;p&gt;When it comes to explainable AI, LIME and SHAP are two popular methods for providing insights into the decisions made by machine learning models. What are the key differences between these methods? In this article, we will help you understand which method may be best for your specific use case.&lt;/p&gt;</summary><content type="html">&lt;p&gt;LIME and SHAP are both popular methods for explainable AI (XAI), but they differ in several key ways.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#model-agnostic-vs-model-specific"&gt;Model-agnostic vs model-specific&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#local-vs-global-explanations"&gt;Local vs global explanations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kernel-based-vs-game-theoretic-approach"&gt;Kernel-based vs game-theoretic approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interpretability-vs-accuracy-trade-off"&gt;Interpretability vs accuracy trade-off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="model-agnostic-vs-model-specific"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="model-agnostic-vs-model-specific"&gt;Model-agnostic vs model-specific&lt;/h2&gt;
&lt;p&gt;One of the main differences between LIME and SHAP is that LIME is model-agnostic, meaning it can be used to explain the decisions of any machine learning model, regardless of the algorithm used. In contrast, SHAP is a model-specific method that is designed to explain the decisions of tree-based models, such as decision trees, random forests, and gradient boosting machines.&lt;/p&gt;
&lt;p&gt;&lt;a id="local-vs-global-explanations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="local-vs-global-explanations"&gt;Local vs global explanations&lt;/h2&gt;
&lt;p&gt;Another key difference between LIME and SHAP is the type of explanation they provide. LIME generates local explanations, meaning it explains the decision of a complex model for a specific instance or observation. In contrast, SHAP provides global explanations, meaning it explains the overall behavior of the model across all instances.&lt;/p&gt;
&lt;p&gt;&lt;a id="kernel-based-vs-game-theoretic-approach"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="kernel-based-vs-game-theoretic-approach"&gt;Kernel-based vs game-theoretic approach&lt;/h2&gt;
&lt;p&gt;LIME uses a kernel-based approach to explain the decisions of a complex model. It creates a local, interpretable model that approximates the behavior of the complex model around a specific instance. In contrast, SHAP uses a game-theoretic approach to explain the contribution of each feature to the final prediction. It assigns a "credit" score to each feature based on how much it contributes to the prediction.&lt;/p&gt;
&lt;p&gt;&lt;a id="interpretability-vs-accuracy-trade-off"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="interpretability-vs-accuracy-trade-off"&gt;Interpretability vs accuracy trade-off&lt;/h2&gt;
&lt;p&gt;Finally, LIME and SHAP differ in their approach to the interpretability vs accuracy trade-off. LIME sacrifices some accuracy in order to provide more interpretable explanations. It creates a simpler model that may not be as accurate as the complex model, but is easier to understand. In contrast, SHAP aims to provide accurate explanations without sacrificing model accuracy. It uses a more sophisticated approach to explain the contribution of each feature, but this can be more difficult to understand for non-experts.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;LIME and SHAP are both useful methods for XAI, but they differ in their approach to explaining the decisions of complex machine learning models. LIME is model-agnostic and provides local, interpretable explanations, while SHAP is model-specific and provides global explanations using a game-theoretic approach. The choice between LIME and SHAP depends on the specific needs of the user and the characteristics of the machine learning model being explained.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Responsible AI"/><category term="xai"/><category term="explainable-ai"/><category term="lime"/><category term="shap"/></entry><entry><title>LIME - Understanding How This Method for Explainable AI Works</title><link href="http://127.0.0.1:8000/how-the-lime-method-for-explainable-ai-works/" rel="alternate"/><published>2023-04-14T00:00:00+02:00</published><updated>2023-04-14T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-14:/how-the-lime-method-for-explainable-ai-works/</id><summary type="html">&lt;p&gt;Discover how the LIME method can help you understand the important factors behind your model's predictions in a simple, intuitive way.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Artificial intelligence (AI) has revolutionized the way we live and work, but it can sometimes be difficult to understand how AI algorithms reach their decisions. This is where explainable AI (XAI) comes in. XAI is the process of making AI models transparent and understandable to humans. One popular XAI method is Local Interpretable Model-Agnostic Explanations (LIME). In this blog post, we will explore how LIME works and why it is an important tool for XAI.&lt;/p&gt;
&lt;h2 id="the-need-for-explainable-ai"&gt;The need for Explainable AI&lt;/h2&gt;
&lt;p&gt;One of the main criticisms of AI is its "black box" nature. Many AI models, such as deep neural networks, are complex and difficult to interpret. When these models are used in high-stakes applications like healthcare or finance, it is important to understand how the AI arrived at its decision. This is where XAI comes in. XAI provides a framework for understanding how an AI model makes decisions, increasing trust and accountability.&lt;/p&gt;
&lt;h2 id="lime-a-local-interpretable-model-agnostic-explanation-method"&gt;LIME: A Local Interpretable Model-Agnostic Explanation Method&lt;/h2&gt;
&lt;p&gt;LIME is a popular XAI method that provides local, interpretable explanations for individual predictions made by any machine learning model. LIME was introduced in 2016 in the paper &lt;a href="https://arxiv.org/abs/1602.04938"&gt;“Why Should I Trust You?”: Explaining the Predictions of Any Classifier”&lt;/a&gt; by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin, and has since become a widely used method for XAI.&lt;/p&gt;
&lt;p&gt;LIME works by creating a simpler, interpretable model that approximates the behavior of the complex model. The simpler model is trained on local data points, and the resulting model is used to explain the decision of the complex model. The process involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Selecting an instance to explain&lt;/li&gt;
&lt;li&gt;Perturbing the instance to create a dataset of similar instances&lt;/li&gt;
&lt;li&gt;Weighting the similar instances based on their similarity to the instance to explain&lt;/li&gt;
&lt;li&gt;Training a local, interpretable model on the weighted dataset&lt;/li&gt;
&lt;li&gt;Using the local model to generate explanations for the complex model's decision&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's explore each of these steps in more detail.&lt;/p&gt;
&lt;h3 id="step-1-selecting-an-instance-to-explain"&gt;Step 1: Selecting an instance to explain&lt;/h3&gt;
&lt;p&gt;The first step in the LIME process is selecting an instance to explain. This could be an individual data point or an entire dataset. For example, if we are working with a healthcare AI model, we may want to explain the decision to recommend a certain treatment for a specific patient.&lt;/p&gt;
&lt;h3 id="step-2-perturbing-the-instance-to-create-a-dataset-of-similar-instances"&gt;Step 2: Perturbing the instance to create a dataset of similar instances&lt;/h3&gt;
&lt;p&gt;Once we have selected the instance to explain, we perturb it to create a dataset of similar instances. This involves making small changes to the instance while keeping its label (i.e. the prediction of the complex model) the same. The purpose of this step is to create a diverse set of instances that are similar to the instance we want to explain.&lt;/p&gt;
&lt;h3 id="step-3-weighting-the-similar-instances-based-on-their-similarity-to-the-instance-to-explain"&gt;Step 3: Weighting the similar instances based on their similarity to the instance to explain&lt;/h3&gt;
&lt;p&gt;After we have created a dataset of similar instances, we need to weight them based on their similarity to the instance we want to explain. This is done using a kernel function, which assigns a weight to each instance based on its distance to the instance to explain. The kernel function can be any function that measures similarity, such as the Gaussian kernel.&lt;/p&gt;
&lt;h3 id="step-4-training-a-local-interpretable-model-on-the-weighted-dataset"&gt;Step 4: Training a local, interpretable model on the weighted dataset&lt;/h3&gt;
&lt;p&gt;Now that we have a weighted dataset, we can train a local, interpretable model on it. The purpose of this model is to approximate the behavior of the complex model in the local region around the instance we want to explain. The local model should be simple enough to be easily interpretable, but accurate enough to capture the important features of the complex model.&lt;/p&gt;
&lt;p&gt;The choice of local model depends on the problem domain and the complexity of the complex model. Some common choices include linear models, decision trees, and rule-based models.&lt;/p&gt;
&lt;h3 id="step-5-using-the-local-model-to-generate-explanations-for-the-complex-models-decision"&gt;Step 5: Using the local model to generate explanations for the complex model's decision&lt;/h3&gt;
&lt;p&gt;Once we have trained the local model, we can use it to generate explanations for the complex model's decision. This is done by analyzing the coefficients of the local model and identifying the features that contributed the most to the prediction. These features can be presented to the user as a list of important factors that influenced the decision.&lt;/p&gt;
&lt;h2 id="advantages-of-lime"&gt;Advantages of LIME&lt;/h2&gt;
&lt;p&gt;LIME has several advantages over other XAI methods. One of the main advantages is its model-agnostic nature. LIME can be used to explain the decisions of any machine learning model, regardless of its complexity or the algorithm used. This makes it a versatile tool for XAI.&lt;/p&gt;
&lt;p&gt;Another advantage of LIME is its ability to generate local explanations. By creating a local model that approximates the behavior of the complex model, LIME is able to generate explanations that are tailored to specific instances. This can be useful in situations where the explanation for a decision needs to be customized for a particular user or context.&lt;/p&gt;
&lt;h2 id="limitations-of-lime"&gt;Limitations of LIME&lt;/h2&gt;
&lt;p&gt;Despite its many advantages, LIME also has some limitations. One of the main limitations is the need for human input in the kernel function. The choice of kernel function and its parameters can have a significant impact on the explanations generated by LIME. This means that the user needs to have some domain knowledge and expertise in selecting an appropriate kernel function.&lt;/p&gt;
&lt;p&gt;Another limitation of LIME is its sensitivity to perturbations. LIME works by perturbing the instance to create a dataset of similar instances. However, small changes to the instance can result in significantly different explanations. This means that the explanations generated by LIME may not always be robust to changes in the input.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;LIME is a powerful tool for XAI that provides local, interpretable explanations for individual predictions made by any machine learning model. By creating a simpler, interpretable model that approximates the behavior of the complex model, LIME is able to generate explanations that are tailored to specific instances. However, LIME also has some limitations, such as its sensitivity to perturbations and the need for human input in the kernel function. Despite these limitations, LIME remains an important tool for XAI and is widely used in industry and academia.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Responsible AI"/><category term="machine-learning"/><category term="python"/><category term="lime"/><category term="xai"/><category term="explainable-ai"/></entry><entry><title>SHAP - Understanding How This Method for Explainable AI Works</title><link href="http://127.0.0.1:8000/how-the-shap-method-for-explainable-ai-works/" rel="alternate"/><published>2023-04-14T00:00:00+02:00</published><updated>2023-04-14T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-14:/how-the-shap-method-for-explainable-ai-works/</id><summary type="html">&lt;p&gt;Discover how the SHAP method can help you understand the important factors behind your model's predictions in a simple, intuitive way.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a data scientist, one of the biggest challenges in deploying machine learning models is explaining how the model makes its decisions. The need for explainability is not only important for legal and ethical reasons, but it also helps in building trust in the model and making informed decisions. The &lt;strong&gt;SHapley Additive exPlanations&lt;/strong&gt; (SHAP) method is a powerful technique that provides a unified framework for interpreting any model. In this blog post, I will explain the SHAP method, its mathematical foundation, and how it can be applied to interpret machine learning models.&lt;/p&gt;
&lt;h2 id="what-is-shap"&gt;What is SHAP?&lt;/h2&gt;
&lt;p&gt;The SHAP method is a game-theoretic approach to explain the output of any machine learning model. It is based on the concept of Shapley values. Here is a timeline for the SHAP method:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1953: Introduction of Shapley values by Lloyd Shapley for game theory&lt;/li&gt;
&lt;li&gt;2010: First use of Shapley values for explaining machine learning predictions by Strumbelj and Kononenko  &lt;/li&gt;
&lt;li&gt;2017: SHAP paper + Python package by Lundberg&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The SHAP method is a game-theoretic approach to explain the output of any machine learning model. It is based on the concept of Shapley values, which were introduced by Lloyd Shapley in 1953 to fairly distribute the gains of a cooperative game among its players. In the context of machine learning, the players are the input features, and the gain is the difference between the actual output of the model and the expected output. The SHAP method provides a way to calculate the Shapley values for each input feature, which gives us a measure of the contribution of each feature towards the model output.&lt;/p&gt;
&lt;p&gt;The SHapley Additive exPlanations (SHAP) method we are using today was introduced in a paper titled "A Unified Approach to Interpreting Model Predictions" by Scott Lundberg and Su-In Lee, published in the Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017). The paper is available on the arXiv preprint server at &lt;a href="https://arxiv.org/abs/1705.07874"&gt;https://arxiv.org/abs/1705.07874&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="how-does-shap-work"&gt;How does SHAP work?&lt;/h2&gt;
&lt;p&gt;The SHAP method works by computing the Shapley values for each feature in the input space. The Shapley value for feature i, denoted by $\phi_i$, is defined as the average contribution of the feature i across all possible coalitions of features. Mathematically, the Shapley value can be expressed as follows:&lt;/p&gt;
&lt;p&gt;$$\phi_i(f,S) = \sum_{T \subseteq S \setminus {i}}\frac{|T|!(|S|-|T|-1)!}{|S|!}(f(T \cup {i}) - f(T))$$&lt;/p&gt;
&lt;p&gt;where $X$ is the set of all input features, $S$ is a coalition of features that does not include feature $i$, $|S|$ is the size of the coalition, and $f(S\cup{i})$ is the output of the model when the features in $S$ and $i$ are present. The term $f(S)$ is the output of the model when only the features in $S$ are present. The Shapley value represents the average marginal contribution of feature $i$ over all possible coalitions.&lt;/p&gt;
&lt;p&gt;To compute the Shapley values using the above formula, we need to evaluate the model output for all possible coalitions of features, which is computationally infeasible for most machine learning models. The SHAP method provides an efficient way to estimate the Shapley values using a weighted average of the model outputs for a subset of coalitions. The subset of coalitions is selected based on a feature importance metric, such as the permutation importance or the gradient-based importance.&lt;/p&gt;
&lt;h2 id="how-to-apply-shap"&gt;How to apply SHAP?&lt;/h2&gt;
&lt;p&gt;To apply the SHAP method, we need to first compute the Shapley values for each feature in the input space. This can be done using one of the many implementations available in popular machine learning libraries, such as scikit-learn, XGBoost, and TensorFlow. Once we have the Shapley values, we can visualize them using various techniques to gain insights into the model's decision-making process.&lt;/p&gt;
&lt;p&gt;One popular technique to visualize the Shapley values is the Shapley value plot, which shows the contribution of each feature towards the model output for each individual data point. The plot consists of a horizontal axis representing the feature contribution, and a vertical axis representing the features. Each data point is represented by a vertical bar, where the length of the bar represents the magnitude of the Shapley value for the corresponding feature. The color of the bar represents the value of the feature, where red represents high feature values and blue represents low feature values. The plot helps in identifying the most important features for each data point and the direction of the relationship between the features and the output.&lt;/p&gt;
&lt;p&gt;Another technique to visualize the Shapley values is the summary plot, which shows the average contribution of each feature across all data points. The plot consists of a horizontal axis representing the Shapley value and a vertical axis representing the features. Each feature is represented by a horizontal bar, where the length of the bar represents the magnitude of the average Shapley value. The color of the bar represents the direction of the relationship between the feature and the output, where red represents a positive relationship and blue represents a negative relationship.&lt;/p&gt;
&lt;p&gt;In addition to visualizing the Shapley values, the SHAP method can also be used to identify instances where the model makes biased or unfair decisions. The method can be used to quantify the extent to which each feature contributes to the model's bias towards a certain group or class. This helps in identifying the root cause of the bias and taking corrective measures to ensure fairness and equity in the model's decisions.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The SHapley Additive exPlanations (SHAP) method provides a powerful framework for interpreting any machine learning model. The method is based on the concept of Shapley values, which provides a fair way to distribute the gain of a cooperative game among its players. The SHAP method provides an efficient way to compute the Shapley values for each feature in the input space, which gives us a measure of the contribution of each feature towards the model output. The method can be applied to visualize the Shapley values, identify the most important features, and quantify the model's bias towards certain groups or classes. By providing a unified framework for interpretability, the SHAP method helps in building trust in the model and making informed decisions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X::[[2023-04-14-lime|LIME - Understanding How This Method for Explainable AI Works]]&lt;/li&gt;
&lt;li&gt;X::[[2023-04-14-lime_tutorial|LIME Tutorial]]&lt;/li&gt;
&lt;li&gt;up::[[MOC_RAI]]&lt;/li&gt;
&lt;/ul&gt;</content><category term="Responsible AI"/><category term="machine-learning"/><category term="python"/><category term="shap"/><category term="xai"/><category term="explainable-ai"/><category term="rai"/><category term="responsible-ai"/></entry><entry><title>KernelShap and TreeShap - Two Most Popular Variations of the SHAP Method</title><link href="http://127.0.0.1:8000/kernelshap-treeshap-two-most-popular-variations-of-the-shap-method/" rel="alternate"/><published>2023-04-14T00:00:00+02:00</published><updated>2023-11-05T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-14:/kernelshap-treeshap-two-most-popular-variations-of-the-shap-method/</id><summary type="html">&lt;p&gt;Making sense of AI's inner workings with KernelShap and TreeShap the powerfull tools for responsible AI.&lt;/p&gt;</summary><content type="html">&lt;h2 id="tldr"&gt;TLDR&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;original SHAP does not scale well&lt;/strong&gt; with high dimensions data due to its exponential complexity associated with Shapley value calculations. KernelSHAP and TreeSHAP are two specific implementations of the SHAP method, developed to address the shortcomings of the original framework and to optimize it for different types of machine learning models, but they achieve this in different ways.
&lt;strong&gt;KernelSHAP&lt;/strong&gt; uses a model-agnostic method to interpret the impact of features in a model. This means it can provide explanations &lt;strong&gt;for any model&lt;/strong&gt; but &lt;strong&gt;at the cost of computational efficiency&lt;/strong&gt;, making it &lt;strong&gt;less suitable for complex&lt;/strong&gt;, high-dimensional situations or when real-time explanations are needed. In contrast, &lt;strong&gt;TreeSHAP&lt;/strong&gt; is designed specifically &lt;strong&gt;for tree-based models&lt;/strong&gt; (like decision tree and random forests, or boosting machines). It is &lt;strong&gt;computationally efficient&lt;/strong&gt;, exploits the tree structure for &lt;strong&gt;faster calculations&lt;/strong&gt;, and thus, can handle &lt;strong&gt;more complex scenarios&lt;/strong&gt;. Moreover, TreeSHAP guarantees consistency - a helpful property for feature attribution methods to ensure that if a model relies more on a feature, the attributed importance of that feature should not decrease. However, it can't be used for non-tree models.&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Responsible AI is an approach to artificial intelligence that ensures fairness, transparency, and accountability in the development, deployment, and management of AI systems. In the era of increasing reliance on AI-driven decision-making, understanding and explaining the predictions made by these models is essential. The interpretability of AI models helps build trust, enables better decision-making, and allows us to mitigate biases.&lt;/p&gt;
&lt;p&gt;Two popular methods for explaining AI models are KernelShap and TreeShap. These techniques are part of the SHAP (SHapley Additive exPlanations) family, which is based on cooperative game theory. In this blog post, we will look into the details of KernelShap and TreeShap, exploring their underlying principles, advantages, and use cases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see &lt;a href="https://github.com/shap/shap#citations"&gt;papers&lt;/a&gt; for details and citations).
&lt;em&gt;(from &lt;a href="https://shap.readthedocs.io/en/latest/index.html"&gt;SHAP documentation&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#tldr"&gt;TLDR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kernelshap"&gt;KernelShap&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#steps"&gt;Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kernelshap-advantages-and-limitations"&gt;KernelShap advantages and limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#treeshap"&gt;TreeShap&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#steps-1"&gt;Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#treeshap-advantages-and-limitations"&gt;TreeShap advantages and limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="kernelshap"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="kernelshap"&gt;KernelShap&lt;/h2&gt;
&lt;p&gt;KernelShap is a model-agnostic explanation method that provides interpretable explanations for any black-box model. It uses the concept of Shapley values from cooperative game theory to attribute feature importance to individual features in the context of a specific prediction.&lt;/p&gt;
&lt;p&gt;The Shapley value for feature $i$ in a model $f$ can be calculated using the following formula:&lt;/p&gt;
&lt;p&gt;$$ϕ_i(f) = \sum_{S ⊆ N \setminus {i}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S ∪ {i}) - f(S)]$$&lt;/p&gt;
&lt;p&gt;Here, $S$ is a subset of features excluding $i$, and $N$ is the total number of features. The term $|S|!$ represents the factorial of the number of features in subset $S$, while $|N|-|S|-1!$ represents the factorial of the remaining features outside of the subset. The denominator $|N|!$ is the factorial of the total number of features.&lt;/p&gt;
&lt;p&gt;Shapley values, in the context of AI, are used to distribute the contribution of each feature to the final prediction. It ensures that the contribution of each feature is fairly allocated in a way that is efficient, symmetric, and additive.&lt;/p&gt;
&lt;p&gt;KernelShap approximates the Shapley values by solving a weighted linear regression problem. It samples instances from the feature space and estimates the Shapley values using the Lasso regression model. The Lasso model is a linear model with an L1 penalty term, which helps in feature selection and makes the explanation sparse.&lt;/p&gt;
&lt;p&gt;Lasso (Least Absolute Shrinkage and Selection Operator) regression is a linear regression technique that includes an L1 penalty term to shrink the coefficients of less important features towards zero. This allows for both regularization and feature selection, resulting in a more interpretable and parsimonious model.&lt;/p&gt;
&lt;p&gt;The equation for Lasso regression is given by:&lt;/p&gt;
&lt;p&gt;$$L(\beta) = \sum_{i=1}^{n}(y_i - X_i\beta)^2 + \lambda\sum_{j=1}^{p}|\beta_j|$$
In this equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$L(\beta)$ represents the objective function to be minimized,&lt;/li&gt;
&lt;li&gt;$y_i$ is the actual response (outcome) for the $i^{th}$ observation,&lt;/li&gt;
&lt;li&gt;$X_i$ is the feature vector for the $i^{th}$ observation,&lt;/li&gt;
&lt;li&gt;$\beta$ is the vector of coefficients to be estimated,&lt;/li&gt;
&lt;li&gt;$n$ is the total number of observations,&lt;/li&gt;
&lt;li&gt;$p$ is the total number of features,&lt;/li&gt;
&lt;li&gt;$\lambda$ is a non-negative regularization parameter, and&lt;/li&gt;
&lt;li&gt;$|\beta_j|$ is the absolute value of the $j^{th}$ coefficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first term, $\sum_{i=1}{n}(y_i - X_i\beta)2$, is the sum of squared residuals, which represents the difference between the actual and predicted responses. Minimizing this term alone would result in an ordinary least squares regression.&lt;/p&gt;
&lt;p&gt;The second term, $\lambda\sum_{j=1}^{p}|\beta_j|$, is the L1 penalty term that adds the absolute values of the coefficients multiplied by the regularization parameter $\lambda$. By increasing $\lambda$, the penalty term forces some coefficients to be exactly zero, effectively selecting a subset of features for the final model. The optimal value of $\lambda$ is usually determined through cross-validation.&lt;/p&gt;
&lt;p&gt;&lt;a id="steps"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="steps"&gt;Steps&lt;/h3&gt;
&lt;p&gt;The KernelShap algorithm involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate a dataset of binary-masked instances by randomly selecting feature combinations.&lt;/li&gt;
&lt;li&gt;Compute the output of the black-box model for each instance.&lt;/li&gt;
&lt;li&gt;Fit a weighted linear regression model on the generated dataset, where the weights are determined by the similarity between the instance and the instance of interest.&lt;/li&gt;
&lt;li&gt;The coefficients of the linear regression model represent the approximate Shapley values.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="kernelshap-advantages-and-limitations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="kernelshap-advantages-and-limitations"&gt;KernelShap advantages and limitations&lt;/h3&gt;
&lt;p&gt;KernelShap has several &lt;strong&gt;advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It can be applied to any black-box model, regardless of its architecture or training algorithm.&lt;/li&gt;
&lt;li&gt;It provides a unified measure of feature importance that is consistent across different models.&lt;/li&gt;
&lt;li&gt;It takes into account the interactions between features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, KernelShap also has some &lt;strong&gt;limitations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It can be computationally expensive, especially for high-dimensional data or complex models.&lt;/li&gt;
&lt;li&gt;It requires a large number of samples to provide accurate estimates of the Shapley values.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="treeshap"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="treeshap"&gt;TreeShap&lt;/h2&gt;
&lt;p&gt;TreeShap is a model-specific explanation method designed for tree-based models, such as decision trees, random forests, and gradient boosting machines. Like KernelShap, it is based on Shapley values, but it exploits the structure of tree-based models to compute the values efficiently.&lt;/p&gt;
&lt;p&gt;TreeShap &lt;strong&gt;computes the exact Shapley values for each feature by recursively traversing the decision tree&lt;/strong&gt;, attributing contributions to each feature as it moves down the tree. It uses a dynamic programming approach to avoid redundant calculations and reduce the computational complexity.&lt;/p&gt;
&lt;p&gt;&lt;a id="steps-1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="steps_1"&gt;Steps&lt;/h3&gt;
&lt;p&gt;The TreeShap algorithm involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Traverse the tree from the root to the leaf nodes, recording the decision path for the instance of interest.&lt;/li&gt;
&lt;li&gt;Attribute contributions to each feature encountered along the path, taking into account the number of possible feature combinations and the probability of each combination.&lt;/li&gt;
&lt;li&gt;Repeat the process for all trees in the ensemble, if applicable.&lt;/li&gt;
&lt;li&gt;Average the contributions across all trees to obtain the final Shapley values.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="treeshap-advantages-and-limitations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="treeshap-advantages-and-limitations"&gt;TreeShap advantages and limitations&lt;/h3&gt;
&lt;p&gt;TreeShap has several advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It computes the exact Shapley values without the need for sampling or approximations.&lt;/li&gt;
&lt;li&gt;It is computationally efficient due to its dynamic programming approach.&lt;/li&gt;
&lt;li&gt;It is specifically designed for tree-based models, which are widely used in practice.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, TreeShap is limited to tree-based models and cannot be applied to other types of models, such as deep learning or support vector machines.
&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;KernelShap and TreeShap are powerful methods for explaining AI models in the context of responsible AI. Both techniques leverage the concept of Shapley values to provide interpretable and fair attributions of feature importance. While KernelShap is a model-agnostic approach that can be applied to any black-box model, TreeShap is tailored for tree-based models and offers exact Shapley values with computational efficiency.&lt;/p&gt;
&lt;p&gt;Understanding and implementing these methods is crucial for AI practitioners who aim to build transparent, accountable, and trustworthy AI systems. By providing insights into the inner workings of AI models, KernelShap and TreeShap enable developers to identify potential biases, improve the decision-making process, and ultimately foster trust in AI-driven technologies.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2023-11-05: Added TLDR section, minor edits&lt;/li&gt;
&lt;/ul&gt;</content><category term="Responsible AI"/><category term="shap"/><category term="xai"/><category term="explainable-ai"/></entry><entry><title>LIME Tutorial</title><link href="http://127.0.0.1:8000/lime-tutorial/" rel="alternate"/><published>2023-04-14T00:00:00+02:00</published><updated>2023-04-14T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-14:/lime-tutorial/</id><summary type="html">&lt;p&gt;Unveiling the mysteries of AI decisions? Let us dive into LIME, the tool that sheds light on the black box.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this tutorial, we'll be exploring how to use the LIME (Local Interpretable Model-Agnostic Explanations) library for explainable AI. We'll start by discussing what LIME is and why it's useful for explainable AI, and then we'll dive into the code.&lt;/p&gt;
&lt;h2 id="what-is-lime"&gt;What is LIME?&lt;/h2&gt;
&lt;p&gt;LIME is a library for explaining the predictions of machine learning models. It works by creating "local" surrogate models that approximate the behavior of the original model in the vicinity of a particular prediction. The idea behind LIME is that these surrogate models can be used to provide human-understandable explanations for how the original model arrived at its decision.&lt;/p&gt;
&lt;p&gt;Why is LIME useful for explainable AI? There are a few reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transparency:&lt;/strong&gt; LIME allows us to peek "under the hood" of a black box model and see how it's making its decisions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trust:&lt;/strong&gt; By providing human-understandable explanations, LIME can increase our trust in the model's decisions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Debugging:&lt;/strong&gt; LIME can help us identify problems with our model by highlighting areas where the model is making incorrect or unexpected predictions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that we understand why LIME is useful, let's dive into the code.&lt;/p&gt;
&lt;h2 id="selecting-a-dataset"&gt;Selecting a Dataset&lt;/h2&gt;
&lt;p&gt;For this tutorial, we'll be using the classic "Iris" dataset, which is a popular dataset for classification tasks. The Iris dataset consists of 150 samples, each with four features (sepal length, sepal width, petal length, and petal width), and each sample belongs to one of three classes (setosa, versicolor, or virginica). The goal is to build a machine learning model that can predict the class of a new sample based on its features.&lt;/p&gt;
&lt;p&gt;To start, we'll load the Iris dataset using scikit-learn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;

&lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we'll split the dataset into training and testing sets:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We'll use the training set to train our machine learning model, and the testing set to evaluate its performance.&lt;/p&gt;
&lt;h2 id="training-a-machine-learning-model"&gt;Training a Machine Learning Model&lt;/h2&gt;
&lt;p&gt;For this tutorial, we'll use a random forest classifier as our machine learning model. The random forest algorithm is an ensemble learning method that combines multiple decision trees to improve the accuracy and robustness of the predictions.&lt;/p&gt;
&lt;p&gt;We'll start by importing the necessary libraries and creating the classifier:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;

&lt;span class="n"&gt;rfc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We're using 100 decision trees in our random forest classifier, and setting the random state to 42 for reproducibility.&lt;/p&gt;
&lt;p&gt;Next, we'll fit the classifier to the training data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rfc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, we'll evaluate the performance of the classifier on the testing data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;

&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rfc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When we run this code, we should see an accuracy of around 0.97, which means our model is doing a pretty good job of predicting the class of new samples.&lt;/p&gt;
&lt;h2 id="explaining-model-predictions-with-lime"&gt;Explaining Model Predictions with LIME&lt;/h2&gt;
&lt;p&gt;Now that we have a trained machine learning model, we can start using LIME to explain its predictions.&lt;/p&gt;
&lt;p&gt;First, we need to create an explainer object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;lime&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;lime.lime_tabular&lt;/span&gt;

&lt;span class="n"&gt;explainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lime_tabular&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LimeTabularExplainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;class_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;discretize_continuous&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, we're creating a &lt;code&gt;LimeTabularExplainer&lt;/code&gt; object and passing in the training data, feature names, class names, and setting &lt;code&gt;discretize_continuous&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to discretize any continuous features.&lt;/p&gt;
&lt;p&gt;Next, we'll pick a sample from the testing data that we want to explain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="c1"&gt;# index of the sample we want to explain&lt;/span&gt;
&lt;span class="n"&gt;exp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;explainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;explain_instance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;rfc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here, we're using the &lt;code&gt;explain_instance&lt;/code&gt; method to generate an explanation for the sample at index &lt;code&gt;idx&lt;/code&gt;. We're passing in the sample data and the &lt;code&gt;predict_proba&lt;/code&gt; method of the random forest classifier, which is used to predict the probabilities of each class for the given sample.&lt;/p&gt;
&lt;p&gt;Now, we can print out the top three features that are contributing to the prediction:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_list&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_list&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will give us something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="mf"&gt;4.25&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;petal&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;5.10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.21&lt;/span&gt;
&lt;span class="mf"&gt;0.30&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;petal&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;1.30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.16&lt;/span&gt;
&lt;span class="n"&gt;sepal&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;2.80&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.03&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This tells us that the most important feature for this prediction is petal width (cm), and that a value of 0.80 or less is strongly associated with the "setosa" class.&lt;/p&gt;
&lt;p&gt;We can also visualize the explanation using a bar chart:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;lime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;lime_tabular&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_pyplot_figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will create a bar chart that shows the contribution of each feature to the prediction, with the most important features at the top:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LIME bar chart" src="/images/lime_tutorial/lime_bar_chart.png"&gt;&lt;/p&gt;
&lt;h2 id="visualizing-model-decisions"&gt;Visualizing Model Decisions&lt;/h2&gt;
&lt;p&gt;In addition to explaining individual predictions, LIME can also be used to visualize how the model is making decisions more generally. We can do this by generating multiple explanations for different samples and visualizing the patterns that emerge.&lt;/p&gt;
&lt;p&gt;To start, we'll generate a of explanation for the testing data point, next, we'll use these explanations to generate a decision plot:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;exp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;explainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;explain_instance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data_row&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
    &lt;span class="n"&gt;predict_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rfc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show_in_notebook&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;show_table&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="LIME - explanation visualization" src="/images/lime_tutorial/lime_explanation.png"&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we learned how to use the LIME library for explainable AI. We started by importing the necessary libraries and loading the Iris dataset. Then, we trained a random forest classifier on the dataset and used LIME to explain individual predictions and visualize model decisions.&lt;/p&gt;
&lt;p&gt;We saw how LIME can be used to identify the most important features for a prediction, and how these features can be visualized using a bar chart. We also saw how LIME can be used to visualize how the model is making decisions more generally, using a decision plot.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="related"&gt;Related&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/63937620/how-to-plot-lime-report-when-there-is-a-lot-of-features-in-data-set"&gt;python - How to plot Lime report when there is a lot of features in data-set - Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://betterdatascience.com/lime/"&gt;LIME: How to Interpret Machine Learning Models With Python | Better Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions"&gt;How to Use LIME to Interpret Predictions of ML Models [Python]?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://shiring.github.io/machine_learning/2017/04/23/lime"&gt;Explaining complex machine learning models with LIME&lt;/a&gt; (in R)&lt;/li&gt;
&lt;/ul&gt;</content><category term="Responsible AI"/><category term="machine-learning"/><category term="python"/><category term="explainable-ai"/><category term="lime"/></entry><entry><title>Zero-Knowledge Explained Like to 5 Years Old</title><link href="http://127.0.0.1:8000/zero-knowledge-for-5yo/" rel="alternate"/><published>2023-04-06T00:00:00+02:00</published><updated>2023-04-06T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-04-06:/zero-knowledge-for-5yo/</id><summary type="html">&lt;p&gt;Imagine being able to prove something without actually revealing it. That is the power of zero-knowledge proofs, the technology that keeps your crypto safe.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Zero-knowledge proofs (ZKPs) are a key technology that underpins the security and privacy of many modern cryptocurrencies. In essence, ZKPs allow parties to prove that they know a piece of information, without revealing that information itself. But what does that mean, exactly? In this blog post, we'll explain ZKPs in a way that even a 5-year-old can understand.&lt;/p&gt;
&lt;h2 id="helper-example"&gt;Helper example&lt;/h2&gt;
&lt;p&gt;Let's start with a basic example. Imagine you have a secret toy that you don't want anyone else to know about. Your friend wants to prove to you that they know what the toy is, without actually telling you what it is. How can they do that?&lt;/p&gt;
&lt;p&gt;One way to do it is to play a guessing game. Your friend can ask you a series of questions about the toy, such as "Is it blue?" or "Does it have wheels?" Based on your answers, your friend can narrow down the possibilities until they have a pretty good idea of what the toy is. This is a bit like a multiple-choice test: by eliminating the wrong answers, you can eventually arrive at the right one.&lt;/p&gt;
&lt;p&gt;But what if your friend wants to prove that they know the toy, without giving you any clues about what it is? That's where zero-knowledge proofs come in.&lt;/p&gt;
&lt;p&gt;Imagine your friend has a magic wand that can tell them whether a particular guess is right or wrong, without actually revealing what the correct answer is. So they can make a guess, wave the wand, and get a "yes" or "no" answer. If the answer is "no", they can make another guess and try again. If the answer is "yes", they've proven that they know the toy, without actually revealing what it is.&lt;/p&gt;
&lt;p&gt;This is a bit like playing "20 questions", but with a magical yes-or-no answer that doesn't give away any information. Your friend doesn't need to ask you any questions about the toy, they just need to make a series of guesses and use the magic wand to check if they're right or wrong. And because the wand doesn't reveal anything about the toy itself, you still don't know what it is.&lt;/p&gt;
&lt;h2 id="zero-knowledge-in-cryptocurrency"&gt;Zero-knowledge in Cryptocurrency&lt;/h2&gt;
&lt;p&gt;Now, let's apply this idea to cryptocurrency. In a blockchain system like Bitcoin, transactions are recorded on a public ledger that anyone can see. But the ledger doesn't reveal who the parties involved in the transaction are. Instead, it uses cryptographic techniques to obscure their identities.&lt;/p&gt;
&lt;p&gt;For example, imagine you want to send some Bitcoin to a friend. You create a transaction that says "send X amount of Bitcoin to this address". But instead of using your real name and address, you use a pseudonymous address that's associated with your public key.&lt;/p&gt;
&lt;p&gt;The public key is a string of characters that's generated using a complex mathematical algorithm. It's unique to you, and it's used to encrypt and decrypt messages that are sent to and from your address. But it doesn't reveal your actual identity.&lt;/p&gt;
&lt;p&gt;So when you send the Bitcoin, the transaction is broadcast to the network and added to the blockchain. But nobody knows who the parties involved are, because they're identified only by their public keys.&lt;/p&gt;
&lt;p&gt;This is where zero-knowledge proofs come in. Imagine you want to prove to someone that you own a particular address, without revealing what that address is. You could use a zero-knowledge proof to demonstrate that you know the private key associated with that address, without actually showing the key itself.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Zero-knowledge proof (ZKP)&lt;/strong&gt;
The proof works by using a mathematical algorithm that allows you to generate a random "challenge" that's based on your private key. You then provide a response to the challenge that demonstrates that you know the private key, without revealing what it is.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a bit like the guessing game we talked about earlier. The challenge is like a question that's designed to test whether you know the private key, and the response is like an answer that proves that you do, without revealing what the key is. This allows you to prove ownership of the address, without revealing any sensitive information.&lt;/p&gt;
&lt;p&gt;This is important for privacy and security in cryptocurrency, because it means that you can prove ownership of an address without revealing your identity or any other sensitive information. It also makes it much harder for hackers or other bad actors to steal your cryptocurrency, because they would need to know your private key in order to access your funds.&lt;/p&gt;
&lt;p&gt;So there you have it, zero-knowledge proofs explained like you're 5 years old! They're a clever way of proving that you know something, without actually revealing what it is. And for cryptocurrency, they're a key technology that helps to ensure the security and privacy of your transactions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ZKP Origin&lt;/strong&gt;
Zero-knowledge proofs were first introduced by researchers Shafi Goldwasser, Silvio Micali, and Charles Rackoff in 1985. Their groundbreaking paper, &lt;a href="https://dl.acm.org/doi/10.1145/22145.22178"&gt;"The Knowledge Complexity of Interactive Proof-Systems,"&lt;/a&gt; laid the foundation for zero-knowledge proof systems.
Silvio Micali, won the &lt;a href="https://amturing.acm.org/award_winners/micali_9954407.cfm"&gt;Turing Award&lt;/a&gt; for his works on cryptography and inventing Zero Knowledge (ZK) Proofs&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="related-reading"&gt;Related reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The reddit user &lt;a href="https://www.reddit.com/user/busterrulezzz/"&gt;busterrulezzz (u/busterrulezzz) - Reddit&lt;/a&gt;proposed other ELI5 explanation of how the ZKP works: &lt;a href="https://www.reddit.com/r/CryptoCurrency/comments/rwpfkx/zeroknowledge_proof_explained_like_you_are_5/"&gt;Zero-knowledge proof explained like you are 5 years old : r/CryptoCurrency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hackernoon.com/eli5-zero-knowledge-proof-78a276db9eff"&gt;Zero Knowledge Proof: Explain it Like I’m 5 (Halloween Edition) | HackerNoon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;up::[[MOC_Cryptocurrency]]&lt;/p&gt;</content><category term="Algorithmic Trading"/><category term="crypto"/><category term="cryptocurrency"/><category term="zero-knowledge"/></entry><entry><title>Rethinking the Link Between Speech and Expertise</title><link href="http://127.0.0.1:8000/illusion-of-expertise/" rel="alternate"/><published>2023-02-23T00:00:00+01:00</published><updated>2023-02-23T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-23:/illusion-of-expertise/</id><summary type="html">&lt;p&gt;We often associate eloquent speech with intelligence and knowledge. But what if I told you that this assumption is not always true?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Communication is an essential aspect of our lives. It is how we express ourselves, connect with others, and convey our thoughts and ideas. However, not all communication is created equal. We often associate weak speech with low knowledge, lack of experience, and a lack of education, while on the other hand, &lt;strong&gt;flowery and eloquent speech is perceived as a guarantee of education and expertise&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This assumption is deeply ingrained in our society, and we often judge people based on their ability to speak. However, this is a flawed way of thinking. The quality of one's speech does not necessarily reflect their knowledge or expertise. While eloquent speakers can be impressive, they can also be misleading.&lt;/p&gt;
&lt;p&gt;With the rise of large language models, such as GPT-3, we have access to language that appears to be well-formed and logical. These models can produce complex sentences, construct compelling arguments, and even mimic human conversation. However, just because language is grammatically correct and sounds good does not mean that it is accurate or truthful.&lt;/p&gt;
&lt;p&gt;The danger of relying on large language models is that they can perpetuate falsehoods and mislead people. In many cases, these models are trained on vast amounts of text data, including fake news and propaganda, which can result in them generating biased and misleading content.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Therefore, it is important to recognise that &lt;strong&gt;eloquence does not guarantee knowledge or expertise&lt;/strong&gt;. We should be critical of the information we receive, especially when it comes from sources that use complex language to appear knowledgeable or trustworthy. Instead, we should focus on the substance of what is being said, and evaluate information based on its accuracy, reliability, and credibility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While we may be accustomed to associating weak speech with a lack of knowledge and expertise, and eloquent speech with education and expertise, this is not always the case. We need to be critical of the language we consume, especially with the rise of large language models, and focus on the substance of what is being said. By doing so, we can make informed decisions and avoid being misled by false or inaccurate information.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="python"/></entry><entry><title>A Guide to Building a Python RPC Server Using Flask</title><link href="http://127.0.0.1:8000/guide-building-python-rpc-server-using-flask/" rel="alternate"/><published>2023-02-13T00:00:00+01:00</published><updated>2023-02-13T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-13:/guide-building-python-rpc-server-using-flask/</id><summary type="html">&lt;p&gt;Discover the world of distributed systems and build your own Python RPC server using Flask. Harness the power of remote procedure calls today!&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Remote Procedure Call (RPC) is a communication protocol that allows a client application to call a function or method on a remote server. RPC is widely used in distributed systems for inter-process communication. Python is a powerful programming language for building applications, and Flask is a popular web framework for building web applications in Python. In this blog post, we will guide you on how to build a Python RPC server using Flask.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#step-1-install-flask"&gt;Step 1: Install Flask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#step-2-create-a-flask-app"&gt;Step 2: Create a Flask app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#step-3-add-a-json-rpc-endpoint"&gt;Step 3: Add a JSON-RPC endpoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#step-4-define-json-rpc-functions"&gt;Step 4: Define JSON-RPC functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#step-5-test-the-json-rpc-server"&gt;Step 5: Test the JSON-RPC server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="step-1-install-flask"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="step-1-install-flask"&gt;Step 1: Install Flask&lt;/h3&gt;
&lt;p&gt;The first step is to install Flask. You can install Flask using pip, which is the package installer for Python.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;flask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="step-2-create-a-flask-app"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="step-2-create-a-flask-app"&gt;Step 2: Create a Flask app&lt;/h3&gt;
&lt;p&gt;The next step is to create a Flask app. You can create a Flask app by creating a Python file and importing Flask. The following code creates a simple Flask app:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Hello, world!&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code creates a Flask app and defines a route for the root URL. When a client sends a GET request to the root URL, the server will return 'Hello, world!'.&lt;/p&gt;
&lt;p&gt;&lt;a id="step-3-add-a-json-rpc-endpoint"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="step-3-add-a-json-rpc-endpoint"&gt;Step 3: Add a JSON-RPC endpoint&lt;/h3&gt;
&lt;p&gt;The next step is to add a JSON-RPC endpoint to the Flask app. JSON-RPC is a lightweight remote procedure call protocol that uses JSON to encode messages. You can use the &lt;code&gt;jsonrpcserver&lt;/code&gt; package to add a JSON-RPC endpoint to Flask. The &lt;code&gt;jsonrpcserver&lt;/code&gt; package provides a decorator &lt;code&gt;@app.route_jsonrpc&lt;/code&gt; that you can use to define a JSON-RPC endpoint.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jsonrpcserver&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dispatch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Hello, world!&amp;#39;&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route_jsonrpc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/rpc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rpc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dispatch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code defines a JSON-RPC endpoint at the URL &lt;code&gt;/rpc&lt;/code&gt;. When a client sends a JSON-RPC request to the URL &lt;code&gt;/rpc&lt;/code&gt;, the server will dispatch the request to the appropriate function and return the result in a JSON-RPC response.&lt;/p&gt;
&lt;p&gt;&lt;a id="step-4-define-json-rpc-functions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="step-4-define-json-rpc-functions"&gt;Step 4: Define JSON-RPC functions&lt;/h3&gt;
&lt;p&gt;The next step is to define the JSON-RPC functions that the client can call. You can define JSON-RPC functions as Python functions and use the &lt;code&gt;@dispatch&lt;/code&gt; decorator from the &lt;code&gt;jsonrpcserver&lt;/code&gt; package to register the functions with the JSON-RPC server.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jsonrpcserver&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dispatch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jsonrpcserver.exceptions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;InvalidParams&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Hello, world!&amp;#39;&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route_jsonrpc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/rpc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rpc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dispatch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@dispatch&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="nd"&gt;@dispatch&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;subtract&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="nd"&gt;@dispatch&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="nd"&gt;@dispatch&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;divide&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;InvalidParams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;division by zero&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code defines four JSON-RPC functions: &lt;code&gt;add&lt;/code&gt;, &lt;code&gt;subtract&lt;/code&gt;, &lt;code&gt;multiply&lt;/code&gt;, and &lt;code&gt;divide&lt;/code&gt;. Each function takes two integer arguments and returns an integer or float.&lt;/p&gt;
&lt;p&gt;&lt;a id="step-5-test-the-json-rpc-server"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="step-5-test-the-json-rpc-server"&gt;Step 5: Test the JSON-RPC server&lt;/h3&gt;
&lt;p&gt;The final step is to test the JSON-RPC server. You can test the server by sending JSON-RPC requests to the URL &lt;code&gt;/rpc&lt;/code&gt;. You can use any JSON-RPC client to send requests to the server. In the following example, we will use the &lt;code&gt;jsonrpcclient&lt;/code&gt; package to send requests to the server.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jsonrpcclient&lt;/span&gt;

&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;http://localhost:5000/rpc&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jsonrpcclient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;add&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Output: 5&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jsonrpcclient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;subtract&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Output: 2&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jsonrpcclient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;multiply&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Output: 6&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jsonrpcclient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;divide&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Output: 2.0&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jsonrpcclient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;divide&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Output: {&amp;#39;code&amp;#39;: -32602, &amp;#39;message&amp;#39;: &amp;#39;Invalid params&amp;#39;, &amp;#39;data&amp;#39;: &amp;#39;division by zero&amp;#39;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code sends five JSON-RPC requests to the server and prints the results. The first four requests call the &lt;code&gt;add&lt;/code&gt;, &lt;code&gt;subtract&lt;/code&gt;, &lt;code&gt;multiply&lt;/code&gt;, and &lt;code&gt;divide&lt;/code&gt; functions, respectively. The last request calls the &lt;code&gt;divide&lt;/code&gt; function with a zero value for the &lt;code&gt;b&lt;/code&gt; argument, which raises an &lt;code&gt;InvalidParams&lt;/code&gt; exception.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we have shown you how to build a Python RPC server using Flask. We have used the &lt;code&gt;jsonrpcserver&lt;/code&gt; package to add a JSON-RPC endpoint to Flask and define JSON-RPC functions. We have also shown you how to test the server using the &lt;code&gt;jsonrpcclient&lt;/code&gt; package. With this knowledge, you can build powerful distributed systems that can communicate seamlessly across networks.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="python"/><category term="flask"/><category term="rpc"/><category term="client-server"/><category term="communication"/></entry><entry><title>How to Use RPC in Python?</title><link href="http://127.0.0.1:8000/how-to-use-rpc-in-python/" rel="alternate"/><published>2023-02-13T00:00:00+01:00</published><updated>2023-02-13T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-13:/how-to-use-rpc-in-python/</id><summary type="html">&lt;p&gt;Get Started with RPC - A Beginner's Guide to Building a Python RPC Server Using xmlrpc and jsonrpc.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Remote_procedure_call"&gt;Remote Procedure Call (RPC)&lt;/a&gt; is a protocol that allows two different processes or applications to communicate with each other across different machines, even if they are using different programming languages. RPC is a popular technique used in distributed computing environments, where applications running on different systems need to communicate with each other to perform a task. In this blog post, we will explore how to use RPC in Python and provide an example of two instances communicating using RPC.&lt;/p&gt;
&lt;h2 id="using-rpc-in-python"&gt;Using RPC in Python&lt;/h2&gt;
&lt;p&gt;Python provides built-in support for RPC through the &lt;a href="https://docs.python.org/3/library/xmlrpc.html"&gt;xmlrpc&lt;/a&gt; and &lt;a href="https://json-rpc.readthedocs.io/en/latest/"&gt;jsonrpc&lt;/a&gt; libraries. These libraries allow Python applications to expose their functions as RPC services and also consume RPC services provided by other applications.&lt;/p&gt;
&lt;p&gt;To use RPC in Python, you need to create a server that exposes a set of functions or methods that can be invoked by remote clients. You can then use a client application to connect to the server and call these functions remotely.&lt;/p&gt;
&lt;h3 id="simple-client-server"&gt;Simple client-server&lt;/h3&gt;
&lt;h4 id="server"&gt;Server&lt;/h4&gt;
&lt;p&gt;Let's start with an example of a &lt;strong&gt;simple server&lt;/strong&gt; that exposes a function to add two numbers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xmlrpc.server&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MyServer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xmlrpc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SimpleXMLRPCServer&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;register_instance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MyServer&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serve_forever&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the code above, we first define a class called &lt;code&gt;MyServer&lt;/code&gt; that exposes a single method called add, which takes two arguments and returns their sum. We then create an instance of the &lt;code&gt;SimpleXMLRPCServer&lt;/code&gt; class, passing in the host and port where the server should listen for incoming requests. We register an instance of the &lt;code&gt;MyServer&lt;/code&gt; class with the server, which means that any requests received by the server will be forwarded to the methods of the &lt;code&gt;MyServer&lt;/code&gt; class. Finally, we call the &lt;code&gt;serve_forever&lt;/code&gt; method to start the server and listen for incoming requests.&lt;/p&gt;
&lt;h4 id="client"&gt;Client&lt;/h4&gt;
&lt;p&gt;Now that we have a server running, let's create a &lt;strong&gt;client application&lt;/strong&gt; that can connect to the server and call the add method remotely:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xmlrpc.client&lt;/span&gt;

&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xmlrpc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ServerProxy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://localhost:8000&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the code above, we create an instance of the &lt;code&gt;ServerProxy&lt;/code&gt; class, passing in the URL of the server we want to connect to. We then call the add method on the server object, passing in the two numbers we want to add. The result of the method call is returned to the client, which we print to the console.&lt;/p&gt;
&lt;p&gt;That's it! We have successfully used RPC to call a method on a remote server from a client application.&lt;/p&gt;
&lt;h3 id="example-of-two-instances-communicating-using-rpc"&gt;Example of Two Instances Communicating Using RPC&lt;/h3&gt;
&lt;p&gt;Now let's explore an example of two instances communicating using RPC. In this example, we will create a server application that exposes a set of methods that can be called by a client application. The client application will then call these methods to perform a task.&lt;/p&gt;
&lt;p&gt;First, let's create the server application:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xmlrpc.server&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MyServer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cube&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xmlrpc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SimpleXMLRPCServer&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;register_instance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MyServer&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serve_forever&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the code above, we create a class called &lt;code&gt;MyServer&lt;/code&gt; that exposes two methods, square and cube, which calculate the square and cube of a number respectively. We then create an instance of the &lt;code&gt;SimpleXMLRPCServer&lt;/code&gt; class and register an instance of the &lt;code&gt;MyServer&lt;/code&gt; class with the server.&lt;/p&gt;
&lt;p&gt;Next, let's create the client application:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xmlrpc.client&lt;/span&gt;

&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xmlrpc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ServerProxy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://localhost:8000&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;The square of 5 is:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;server&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the client application code above, we create an instance of the &lt;code&gt;ServerProxy&lt;/code&gt; class and connect to the server running on localhost at port &lt;code&gt;8000&lt;/code&gt;. We then call the square method on the server object, passing in the number &lt;code&gt;5&lt;/code&gt;. The result of the method call is returned to the client, which we print to the console. We then call the cube method on the server object, passing in the number &lt;code&gt;3&lt;/code&gt;, and print the result to the console.&lt;/p&gt;
&lt;p&gt;When we run the client application, we should see the following output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;square&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;
&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;cube&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;27&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, we have successfully used RPC to call methods on a remote server from a client application.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;RPC is a powerful technique for building distributed applications, and Python provides built-in support for RPC through the xmlrpc and jsonrpc libraries. Using RPC in Python is straightforward and can be done with just a few lines of code. By following the examples provided in this blog post, you should now understand how to use RPC in Python and how to create a server that exposes methods that can be called remotely by a client application.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You might be interested in reading my &lt;a href="./guide-building-python-rpc-server-using-flask"&gt;"Guide to building a Python RPC server using Flask"&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="further-reading"&gt;Further Reading&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Python documentation on xmlrpc: &lt;a href="https://docs.python.org/3/library/xmlrpc.html"&gt;https://docs.python.org/3/library/xmlrpc.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Python documentation on jsonrpc:
 &lt;a href="https://json-rpc.readthedocs.io/en/latest/"&gt;https://json-rpc.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A tutorial on using XML-RPC in Python: &lt;a href="https://pymotw.com/2/xmlrpc.client/index.html"&gt;https://pymotw.com/2/xmlrpc.client/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;List of material on distributed systems:
 &lt;a href="https://github.com/theanalyst/awesome-distributed-systems"&gt;https://github.com/theanalyst/awesome-distributed-systems&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Howto"/><category term="python"/><category term="microservices"/><category term="communication"/><category term="rpc"/><category term="rest"/><category term="remote-procedure-call"/><category term="xmlrpc"/><category term="jsonrpc"/><category term="distributed-computing"/><category term="client-server"/><category term="server"/><category term="client"/><category term="SimpleXMLRPCServer"/><category term="ServerProxy"/><category term="remote-communication"/><category term="Flask"/></entry><entry><title>Comprehensive Guide to Interpreting R², MSE, and RMSE for Regression Models.</title><link href="http://127.0.0.1:8000/interpreting-r2-mse-rmse-for-regression-models/" rel="alternate"/><published>2023-02-13T00:00:00+01:00</published><updated>2023-02-13T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-13:/interpreting-r2-mse-rmse-for-regression-models/</id><summary type="html">&lt;p&gt;Don't let misleading metrics fool you. Master the art of analyzing regression model performance and make smarter decisions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In regression problems, the performance of a model can be evaluated using a variety of metrics, such as R², mean squared error (MSE), and root mean squared error (RMSE). These metrics provide different information about the accuracy and precision of the model's predictions. Below are some tips on how to jointly analyze various metrics and interpret their values.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#metrics"&gt;Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#r%C2%B2-score"&gt;R² Score&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mean-squared-error-mse"&gt;Mean Squared Error (MSE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#root-mean-squared-error-rmse"&gt;Root Mean Squared Error (RMSE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interpreting-multiple-metrics-jointly"&gt;Interpreting Multiple Metrics jointly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#consider-error-distribution"&gt;Consider error distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#consider-size-of-the-data-set"&gt;Consider size of the data set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#compare-metrics-across-models"&gt;Compare metrics across models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#check-the-significance-of-the-difference-between-models"&gt;Check the significance of the difference between models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examine-the-correlation-between-metrics"&gt;Examine the correlation between metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#check-for-outliers"&gt;Check for outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evaluate-the-metrics-in-the-context-of-the-problem"&gt;Evaluate the metrics in the context of the problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-visualizations"&gt;Use visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-domain-knowledge"&gt;Use domain knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#consider-the-trade-off-between-accuracy-and-interpretability"&gt;Consider the trade-off between accuracy and interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#consider-the-data-distribution"&gt;Consider the data distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#look-beyond-the-mean"&gt;Look beyond the mean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#choose-metrics-based-on-the-problem"&gt;Choose metrics based on the problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="metrics"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="metrics"&gt;Metrics&lt;/h2&gt;
&lt;p&gt;&lt;a id="r%C2%B2-score"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="r2-score"&gt;R² Score&lt;/h3&gt;
&lt;p&gt;The R² score is a measure of how well the model fits the data. It represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model. The R² score ranges from 0 to 1, where a score of 1 indicates a perfect fit of the model to the data.&lt;/p&gt;
&lt;p&gt;The formula for R² is:&lt;/p&gt;
&lt;p&gt;$$
R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y_i})^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
$$&lt;/p&gt;
&lt;p&gt;where $y_i$ is the actual value of the dependent variable, $\hat{y_i}$ is the predicted value of the dependent variable, $\bar{y}$ is the mean of the actual values, and $n$ is the number of observations.&lt;/p&gt;
&lt;p&gt;Interpreting the R² score can be tricky. A high R² score does not necessarily mean that the model is accurate or reliable.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, a high R² score can be obtained even if the model has significant errors in its predictions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, if the model is overfitting the data, it may have a high R² score but perform poorly on new data. Similarly, if the model is underfitting the data, it may have a low R² score but still be accurate.&lt;/p&gt;
&lt;p&gt;&lt;a id="mean-squared-error-mse"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="mean-squared-error-mse"&gt;Mean Squared Error (MSE)&lt;/h3&gt;
&lt;p&gt;The MSE is a measure of the average squared difference between the predicted values and the actual values. The formula for MSE is:&lt;/p&gt;
&lt;p&gt;$$
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2
$$&lt;/p&gt;
&lt;p&gt;where $y_i$ is the actual value of the dependent variable, $\hat{y_i}$ is the predicted value of the dependent variable, and $n$ is the number of observations.&lt;/p&gt;
&lt;p&gt;Graphical interpretation - square of the distance of the points to line.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MSE" src="https://byam.github.io/assets/img/model-eval-val/mean-squared-error.png"&gt;
&lt;em&gt;source: &lt;a href="https://byam.github.io/dlnd/2017/11/13/model-evaluation-and-validation.html"&gt;Model Evaluation and Validation | Tuk Tak&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The MSE &lt;strong&gt;can be used to compare the performance of different models&lt;/strong&gt;. A lower MSE indicates better performance. However, the &lt;strong&gt;MSE is sensitive to outliers&lt;/strong&gt; and can be skewed by extreme values.&lt;/p&gt;
&lt;p&gt;&lt;a id="root-mean-squared-error-rmse"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="root-mean-squared-error-rmse"&gt;Root Mean Squared Error (RMSE)&lt;/h3&gt;
&lt;p&gt;The RMSE is a measure of the average magnitude of the errors in the predictions. It is the square root of the MSE. The formula for RMSE is:&lt;/p&gt;
&lt;p&gt;$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2}$&lt;/p&gt;
&lt;p&gt;The RMSE is easier to interpret than the MSE because it &lt;strong&gt;is in the same units as the dependent variable&lt;/strong&gt;. A lower RMSE indicates better performance. However, like the MSE, the RMSE is &lt;strong&gt;sensitive to outliers&lt;/strong&gt; and can be skewed by extreme values.&lt;/p&gt;
&lt;h3 id="r2-score_1"&gt;R² Score&lt;/h3&gt;
&lt;p&gt;R² Score is based on comparing our model to the &lt;strong&gt;simplest possible model&lt;/strong&gt;. The simplest possible model that fits a bunch of points might be obtained by the &lt;strong&gt;averaging of all the values&lt;/strong&gt; and &lt;strong&gt;drawing a horizontal line&lt;/strong&gt; through them.
&lt;img alt="RMSE" src="https://byam.github.io/assets/img/model-eval-val/r2-score.png"&gt;
&lt;em&gt;source: &lt;a href="https://byam.github.io/dlnd/2017/11/13/model-evaluation-and-validation.html"&gt;Model Evaluation and Validation | Tuk Tak&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;For the &lt;strong&gt;weak&lt;/strong&gt; model MSE should be similar to MSE of simple model and thus R² should be &lt;strong&gt;close to zero&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;For the &lt;strong&gt;strong&lt;/strong&gt; model MSE should be much smaller than MSE of simple model and thus R² should be &lt;strong&gt;close to one&lt;/strong&gt;.
&lt;a id="interpreting-multiple-metrics-jointly"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id="interpreting-multiple-metrics-jointly"&gt;Interpreting Multiple Metrics Jointly&lt;/h2&gt;
&lt;p&gt;When evaluating the performance of a regression model, it is important to consider multiple metrics to obtain a more complete picture of the model's performance. In some cases, a single metric can be misleading. Different metrics can provide different perspectives on the model's performance. However, simply looking at the values of the metrics can be insufficient, as it can be challenging to interpret what these values mean in practical terms. Below are some tips on how to jointly analyze various metrics and interpret their values.&lt;/p&gt;
&lt;p&gt;For example, consider a model that predicts the price of a house based on its square footage. If the model has a high R² score, it may seem like a good fit for the data. However, if the model has a high MSE or RMSE, it may be making large errors in its predictions that are not captured by the R² score. In this case, it would be useful to &lt;strong&gt;examine the distribution of the errors to see if there are any patterns&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="consider-error-distribution"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="consider-error-distribution"&gt;Consider error distribution&lt;/h3&gt;
&lt;p&gt;If the &lt;strong&gt;errors are normally distributed&lt;/strong&gt;, the &lt;strong&gt;MSE and RMSE may provide a more accurate assessment&lt;/strong&gt; of the model's performance than the R² score alone. If the MSE or RMSE is high, it may indicate that the model is not performing well, even if the R² score is high.&lt;/p&gt;
&lt;p&gt;&lt;a id="consider-size-of-the-data-set"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="consider-size-of-the-data-set"&gt;Consider size of the data set&lt;/h3&gt;
&lt;p&gt;Another consideration when evaluating a regression model is the size of the data set. With a &lt;strong&gt;large data set&lt;/strong&gt;, even &lt;strong&gt;small errors&lt;/strong&gt; can result in a &lt;strong&gt;high MSE or RMSE&lt;/strong&gt;. In this case, it may be more useful to look at the percentage of errors, such as the mean absolute percentage error (&lt;strong&gt;MAPE&lt;/strong&gt;) or the mean percentage error (MPE). &lt;strong&gt;These&lt;/strong&gt; metrics can provide a more accurate assessment of the model's performance when dealing with large data sets.&lt;/p&gt;
&lt;p&gt;&lt;a id="compare-metrics-across-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="compare-metrics-across-models"&gt;Compare metrics across models&lt;/h3&gt;
&lt;p&gt;To compare the performance of multiple regression models, it is essential to evaluate the metrics across all the models. For example, if model A has a lower RMSE and higher R² score than model B, it is reasonable to conclude that model A is the better performer.&lt;/p&gt;
&lt;p&gt;&lt;a id="check-the-significance-of-the-difference-between-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="check-the-significance-of-the-difference-between-models"&gt;Check the significance of the difference between models&lt;/h3&gt;
&lt;p&gt;If there is a small difference between the values of two or more metrics, it is essential to check whether the difference is statistically significant or not. This can be done using statistical tests such as the t-test, ANOVA, or Wilcoxon rank-sum test. If the difference is not significant, it may not be worthwhile to conclude that one model is superior to the other.&lt;/p&gt;
&lt;p&gt;&lt;a id="examine-the-correlation-between-metrics"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="examine-the-correlation-between-metrics"&gt;Examine the correlation between metrics&lt;/h3&gt;
&lt;p&gt;Metrics are often correlated, meaning that high values of one metric are associated with high values of another metric. For example, high R² scores are typically associated with low RMSE values. Understanding these correlations can help to interpret the metrics better and determine which metrics to prioritize in evaluating the model's performance.&lt;/p&gt;
&lt;p&gt;&lt;a id="check-for-outliers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="check-for-outliers"&gt;Check for outliers&lt;/h3&gt;
&lt;p&gt;Metrics can be skewed by outliers, which are extreme values that lie far from the majority of the data. It is important to check for outliers and remove them if possible, as they can distort the metrics and lead to incorrect conclusions about the model's performance.&lt;/p&gt;
&lt;p&gt;&lt;a id="evaluate-the-metrics-in-the-context-of-the-problem"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="evaluate-the-metrics-in-the-context-of-the-problem"&gt;Evaluate the metrics in the context of the problem&lt;/h3&gt;
&lt;p&gt;Metrics should be interpreted in the context of the specific problem being solved. For example, a model with a high R² score but high RMSE may be more appropriate if minimizing the number of large errors is more important than minimizing the overall error. In contrast, a model with a lower R² score but lower RMSE may be more appropriate if minimizing the overall error is more important.&lt;/p&gt;
&lt;p&gt;&lt;a id="use-visualizations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="use-visualizations"&gt;Use visualizations&lt;/h3&gt;
&lt;p&gt;Graphs and charts can be useful tools for visualizing the performance of different models and the relationship between different metrics. For example, a scatter plot of predicted versus actual values can help identify any patterns or trends in the model's predictions.&lt;/p&gt;
&lt;p&gt;&lt;a id="use-domain-knowledge"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="use-domain-knowledge"&gt;Use domain knowledge&lt;/h3&gt;
&lt;p&gt;Understanding the problem being solved and the domain in which it operates can help in the interpretation of the metrics. For example, in a medical diagnosis problem, minimizing false negatives may be more important than minimizing overall error.&lt;/p&gt;
&lt;p&gt;&lt;a id="consider-the-trade-off-between-accuracy-and-interpretability"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="consider-the-trade-off-between-accuracy-and-interpretability"&gt;Consider the trade-off between accuracy and interpretability&lt;/h3&gt;
&lt;p&gt;In some cases, a more interpretable model may be preferred over a more accurate one. For example, a linear regression model may be more interpretable than a neural network model, even if the latter has higher accuracy.&lt;/p&gt;
&lt;p&gt;&lt;a id="consider-the-data-distribution"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="consider-the-data-distribution"&gt;Consider the data distribution&lt;/h3&gt;
&lt;p&gt;Metrics such as MSE and RMSE assume a symmetric distribution of errors. If the errors are not normally distributed, it may be more appropriate to use other metrics such as mean absolute error (MAE) or mean absolute percentage error (MAPE).&lt;/p&gt;
&lt;p&gt;&lt;a id="look-beyond-the-mean"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="look-beyond-the-mean"&gt;Look beyond the mean&lt;/h3&gt;
&lt;p&gt;In addition to evaluating the mean value of a metric, it can be useful to examine the distribution of values. For example, examining the distribution of prediction errors can help identify any systematic bias in the model's predictions.&lt;/p&gt;
&lt;p&gt;&lt;a id="choose-metrics-based-on-the-problem"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="choose-metrics-based-on-the-problem"&gt;Choose metrics based on the problem&lt;/h3&gt;
&lt;p&gt;Different metrics may be more appropriate for different regression problems. For example, in a time-series forecasting problem, metrics such as mean absolute scaled error (MASE) or symmetric mean absolute percentage error (SMAPE) may be more appropriate than R² or RMSE.&lt;/p&gt;
&lt;p&gt;&lt;a id="summary"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;When evaluating the performance of a regression model, it is important to consider multiple metrics and not rely on a single metric alone. The R² score, MSE, and RMSE are common metrics used to evaluate regression models, but each provides a different perspective on the model's performance. By examining multiple metrics and considering the size of the data set, it is possible to obtain a more accurate assessment of the model's performance. Analyzing the performance of a regression model requires looking at multiple metrics and understanding their relationship to each other and the problem being solved.&lt;/p&gt;
&lt;p&gt;​&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="r2-score"/><category term="regression-model"/><category term="model-evaluation"/><category term="mse"/><category term="rmse"/><category term="performance-metrics"/><category term="machine-learning"/><category term="data-science"/><category term="data-analysis"/></entry><entry><title>Beyond Airflow - 10 Workflow Tools You Need to Know</title><link href="http://127.0.0.1:8000/alternatives-to-apache-airflow/" rel="alternate"/><published>2023-02-12T00:00:00+01:00</published><updated>2023-02-12T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-12:/alternatives-to-apache-airflow/</id><summary type="html">&lt;p&gt;Looking for a new workflow management tool? Do not settle for Apache Airflow just because it is popular. Discover 10 cutting-edge alternatives that could be a better fit for your needs.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://airflow.apache.org/"&gt;Apache Airflow&lt;/a&gt; is a popular open-source platform for scheduling and managing workflows. It provides a unified interface for defining, executing, and monitoring workflows, making it a valuable tool for data engineers and scientists. However, Apache Airflow is not the only option available, and in some cases, other alternatives may be better suited to meet the specific needs of a project. In this blog post, we will explore some alternatives to Apache Airflow that can be used to manage workflows and schedule tasks.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-prefect"&gt;1.  Prefect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-luigi"&gt;2.  Luigi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-apache-nifi"&gt;3.  Apache Nifi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-argo"&gt;4.  Argo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-zeebe"&gt;5.  Zeebe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-dagster"&gt;6.  DAGster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-conductor"&gt;7.  Conductor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-kubeflow-pipelines"&gt;8.  Kubeflow Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-oozie"&gt;9.  Oozie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#10-pinball"&gt;10.  Pinball&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="1-prefect"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="1-prefect"&gt;1.  Prefect&lt;/h2&gt;
&lt;p&gt;&lt;img alt="prefect logo" src="https://d33wubrfki0l68.cloudfront.net/dbca607e3f64720cb471fc40cdb54c68cea5c86d/3ad5f/assets/img/prefect-logo-gradient-white.c4c1e293.svg"&gt;&lt;/p&gt;
&lt;p&gt;Prefect is a modern, serverless, and open-source workflow management platform that was built to tackle the complexities of data-driven workflows. It aims to provide a simple, intuitive interface for defining, executing, and monitoring workflows, while also offering a robust set of features for handling complex use cases. Prefect offers a wide range of integrations with other tools, including popular cloud platforms like AWS, GCP, and Azure. Prefect also has a growing community of users and developers, making it an excellent option for projects of all sizes.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://www.prefect.io/"&gt;https://www.prefect.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="2-luigi"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="2-luigi"&gt;2.  Luigi&lt;/h2&gt;
&lt;p&gt;&lt;img alt="luigi logo" src="https://raw.githubusercontent.com/spotify/luigi/master/doc/luigi.png"&gt;&lt;/p&gt;
&lt;p&gt;Luigi is a Python-based workflow management system that was created by Spotify. It is designed to be a simple and efficient way to build complex pipelines of batch jobs. Luigi offers a simple interface for defining tasks and dependencies, and it can easily be integrated with other tools and services, such as databases and cloud platforms. Luigi is a great option for projects that require a high degree of customization and flexibility.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://github.com/spotify/luigi"&gt;https://github.com/spotify/luigi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="3-apache-nifi"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="3-apache-nifi"&gt;3.  Apache Nifi&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Apache Nifi logo" src="https://nifi.apache.org/assets/images/apache-nifi-logo.svg"&gt;&lt;/p&gt;
&lt;p&gt;Apache NiFi is a powerful data integration tool that can be used to manage workflows. NiFi provides a user-friendly interface for designing, executing, and monitoring data flows, making it a great option for projects that require a high degree of control and visibility into the flow of data. NiFi supports a wide range of data sources and destinations, and it can be used to automate many different types of data-driven workflows.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://nifi.apache.org/"&gt;https://nifi.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="4-argo"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="4-argo"&gt;4.  Argo&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Argo logo" src="/images/airflow_alternatives/argo.png"&gt;&lt;/p&gt;
&lt;p&gt;Argo is an open-source workflow management system that was designed specifically for Kubernetes. Argo provides a simple and efficient way to manage and automate complex workflows on a Kubernetes cluster. It offers a range of features for defining, executing, and monitoring workflows, and it can be easily integrated with other tools and services, such as databases and cloud platforms. Argo is a great option for projects that require a high degree of scalability and reliability.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://argoproj.github.io/"&gt;https://argoproj.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="5-zeebe"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="5-zeebe"&gt;5.  Zeebe&lt;/h2&gt;
&lt;p&gt;Zeebe is an open-source workflow engine for microservices orchestration. It is designed to be highly scalable, providing a reliable and efficient way to manage complex workflows across multiple microservices. Zeebe offers a user-friendly interface for defining and executing workflows, and it provides a range of features for monitoring and debugging workflows. Zeebe is a great option for projects that require a high degree of coordination and control over microservices.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://zeebe.io/"&gt;https://zeebe.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="6-dagster"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="6-dagster"&gt;6.  DAGster&lt;/h2&gt;
&lt;p&gt;DAGster is an open-source data orchestration platform that was created to manage complex data pipelines. It offers a modular architecture that allows users to easily compose complex workflows, making it a great option for teams that need to build sophisticated data-driven applications. DAGster also offers powerful features for testing, debugging, and monitoring pipelines.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://dagster.io/"&gt;https://dagster.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="7-conductor"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="7-conductor"&gt;7.  Conductor&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Conductor logo" src="https://conductor.netflix.com/img/logo.svg"&gt;&lt;/p&gt;
&lt;p&gt;Conductor is a workflow orchestration engine that is designed to manage complex and long-running workflows. It offers a user-friendly interface for designing and executing workflows, and it provides a range of features for scaling workflows and handling errors. Conductor is also highly customizable and can be easily integrated with other tools and services.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://netflix.github.io/conductor/"&gt;https://netflix.github.io/conductor/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="8-kubeflow-pipelines"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="8-kubeflow-pipelines"&gt;8.  Kubeflow Pipelines&lt;/h2&gt;
&lt;p&gt;Kubeflow Pipelines is a platform for building and deploying machine learning pipelines on Kubernetes. It offers a wide range of features for defining and executing machine learning workflows, making it a great option for data scientists and engineers. Kubeflow Pipelines also provides a user-friendly interface for designing and monitoring workflows, and it can be easily integrated with other Kubeflow components.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://www.kubeflow.org/docs/components/pipelines/"&gt;https://www.kubeflow.org/docs/components/pipelines/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="9-oozie"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="9-oozie"&gt;9.  Oozie&lt;/h2&gt;
&lt;p&gt;&lt;img alt="oozie logo" src="https://oozie.apache.org/images/oozie_200x.png"&gt;&lt;/p&gt;
&lt;p&gt;Oozie is a workflow scheduler system for Hadoop that is designed to manage batch jobs and data processing workflows. It offers a powerful set of features for defining and executing workflows, and it can be easily integrated with other Hadoop components. Oozie also provides a user-friendly interface for managing workflows and monitoring job status.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://oozie.apache.org/"&gt;https://oozie.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="10-pinball"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="10-pinball"&gt;10.  Pinball&lt;/h2&gt;
&lt;p&gt;Pinball is a workflow management system that was created by Pinterest. It offers a user-friendly interface for defining and executing workflows, and it provides a range of features for handling dependencies and managing job execution. Pinball also offers a powerful set of APIs and can be easily integrated with other tools and services.&lt;/p&gt;
&lt;p&gt;Link: &lt;a href="https://github.com/pinterest/pinball"&gt;https://github.com/pinterest/pinball&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="summary"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;Apache Airflow is a powerful tool for managing workflows, but it is not the only option available. Each of the alternatives discussed above offers its own unique set of features and advantages, making them worth considering for projects with specific requirements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; some projects may benefit from using a combination of workflow management systems. For example, a project might use Apache Airflow to manage some workflows while using Zeebe for managing microservices orchestration. This approach can help to take advantage of the unique features and advantages of different systems while minimizing their limitations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;
Heading photo from &lt;a href="https://unsplash.com/photos/4CNNH2KEjhc"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@sigmund"&gt;Sigmund&lt;/a&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="machine-learning"/><category term="workflow"/><category term="mlops"/></entry><entry><title>Libraries for Automated Exploratory Data Analysis (EDA)</title><link href="http://127.0.0.1:8000/libraries-for-automated-eda![[Pasted%20image%2020230218211157.png]]/" rel="alternate"/><published>2023-02-12T00:00:00+01:00</published><updated>2023-02-12T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-12:/libraries-for-automated-eda![[Pasted image 20230218211157.png]]/</id><summary type="html">&lt;p&gt;EDA Made Easy - Discover Top-10 Python Libraries That Will Take Your Data Analysis to the Next Level! Learn the Secrets of Automated EDA!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Exploratory Data Analysis (EDA) is an important step in the data analysis process. It allows us to explore and understand the dataset, identify patterns, and make informed decisions about data cleaning, feature engineering, and modeling. In recent years, several Python libraries have been developed to automate and streamline the EDA process. Here are 10 popular Python libraries for automated EDA:&lt;/p&gt;
&lt;h2 id="top-10-tools-for-automated-eda"&gt;Top-10 Tools for Automated EDA&lt;/h2&gt;
&lt;h3 id="pandas-profiling"&gt;Pandas Profiling&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/ydataai/ydata-profiling.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ydataai/ydata-profiling"&gt;Pandas Profiling&lt;/a&gt; generates a report with descriptive statistics and visualizations for each variable in a Pandas DataFrame. The report includes correlations, missing values, and data types.&lt;/p&gt;
&lt;p&gt;&lt;img alt="pandas-profiling" src="/images/auto_eda/pandas-profiling.jpg"&gt;&lt;/p&gt;
&lt;h3 id="2-dataprep"&gt;2.  DataPrep&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/sfu-db/dataprep.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/sfu-db/dataprep"&gt;DataPrep&lt;/a&gt; provides a set of functions for data cleaning and preprocessing, including automatic column type detection, outlier detection, and missing value imputation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="dataprep" src="/images/auto_eda/dataprep.jpg"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;dataprep
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The following code demonstrates how to use &lt;code&gt;DataPrep.EDA&lt;/code&gt; to create a profile report for the titanic dataset.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dataprep.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dataset&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dataprep.eda&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;create_report&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;titanic&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;create_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show_browser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="3-sweetviz"&gt;3.  Sweetviz&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/fbdesignpro/sweetviz.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/fbdesignpro/sweetviz"&gt;Sweetviz&lt;/a&gt; generates a report with detailed visualizations and statistical analysis for each variable in a Pandas DataFrame. The report includes comparisons between different subgroups and correlation matrices.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sweetviz" src="https://camo.githubusercontent.com/0965c07124443fe73d4343ebc1642b8b3c68ef49913f32a42c4b6f567a477143/687474703a2f2f636f6f6c74696d696e672e636f6d2f53562f66656174757265732e706e67"&gt;&lt;/p&gt;
&lt;h3 id="4-lux"&gt;4.  Lux&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/lux-org/lux.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/lux-org/lux"&gt;Lux&lt;/a&gt; is a library for interactive data visualization that provides a powerful and intuitive interface for exploring and visualizing data. It includes a recommendation system that suggests relevant visualizations based on the current selection.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lux" src="https://github.com/lux-org/lux-resources/raw/master/readme_img/demohighlight.gif?raw=true"&gt;&lt;/p&gt;
&lt;h3 id="5-dabl"&gt;5.  dabl&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/dabl/dabl.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/dabl/dabl"&gt;dabl&lt;/a&gt; is a library that provides a set of functions for automated data analysis and machine learning. It includes tools for data cleaning, feature engineering, and modeling, and provides an easy-to-use interface for non-experts.&lt;/p&gt;
&lt;p&gt;&lt;img alt="dabl" src="https://dabl.github.io/dev/_images/sphx_glr_plot_mfeat_factors_005.png"&gt;&lt;/p&gt;
&lt;h3 id="6-autoviz"&gt;6.  Autoviz&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/AutoViML/AutoViz.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/AutoViML/AutoViz"&gt;Autoviz&lt;/a&gt; is a library that automatically generates visualizations for each variable in a Pandas DataFrame. It includes different types of charts such as scatterplots, histograms, and bar charts, and it can be used for both regression and classification tasks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="AutoViz" src="https://github.com/AutoViML/AutoViz/raw/master/var_charts.JPG"&gt;&lt;/p&gt;
&lt;h3 id="7-klib"&gt;7.  Klib&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/akanz1/klib.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/akanz1/klib"&gt;Klib&lt;/a&gt; is a library that provides a set of functions for data cleaning and preprocessing, including feature selection, missing value imputation, and correlation analysis. It includes useful visualizations and statistical analysis for each variable.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Klib" src="https://raw.githubusercontent.com/akanz1/klib/main/examples/images/header.png"&gt;&lt;/p&gt;
&lt;h3 id="8-explainerdashboard"&gt;8.  ExplainerDashboard&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/oegedijk/explainerdashboard.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/oegedijk/explainerdashboard"&gt;ExplainerDashboard&lt;/a&gt; is a library that provides a dashboard for exploring and visualizing the results of machine learning models. It includes visualizations for feature importance, confusion matrices, and partial dependence plots.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ExplainerDashboard" src="https://github.com/oegedijk/explainerdashboard/raw/master/explainerdashboard.gif"&gt;&lt;/p&gt;
&lt;h3 id="9-pycaret"&gt;9.  PyCaret&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/pycaret/pycaret.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pycaret/pycaret"&gt;PyCaret&lt;/a&gt; is a library for automated machine learning that includes tools for data preprocessing, feature selection, and model training. It includes a user-friendly interface that allows non-experts to build and deploy machine learning models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="PyCaret" src="https://github.com/pycaret/pycaret/raw/master/docs/images/pycaret_ts_quickdemo.gif"&gt;&lt;/p&gt;
&lt;h3 id="missingno"&gt;Missingno&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/ResidentMario/missingno.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ResidentMario/missingno"&gt;Missingno&lt;/a&gt; is a library that provides a set of tools for &lt;strong&gt;visualizing and understanding missing data in a dataset&lt;/strong&gt;. It includes tools for matrix visualization, bar charts, and heatmaps.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Missingno" src="https://camo.githubusercontent.com/16475986b81be8268152a4423777683be2d95cdac5d84e70130eb98431959f20/68747470733a2f2f692e696d6775722e636f6d2f675775584b45722e706e67"&gt;&lt;/p&gt;
&lt;h2 id="honorable-mentions"&gt;Honorable mentions&lt;/h2&gt;
&lt;p&gt;There are three other tools that might be useful during data exploration.&lt;/p&gt;
&lt;h2 id="featuretools"&gt;Featuretools&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/FeatureLabs/featuretools.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/FeatureLabs/featuretools"&gt;Featuretools&lt;/a&gt; is a library for &lt;strong&gt;automated feature engineering&lt;/strong&gt; that allows you to automatically generate features from multiple tables. It includes tools for handling time-based data and can generate a set of feature definitions in just a few lines of code.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Featuretools" src="https://github.com/alteryx/featuretools/raw/main/docs/source/_static/images/entity_set.png?raw=true"&gt;&lt;/p&gt;
&lt;h3 id="pyexplainer"&gt;PyExplainer&lt;/h3&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/awsm-research/pyExplainer.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/awsm-research/pyExplainer"&gt;PyExplainer&lt;/a&gt; is a library that allows you to easily explain and interpret the results of machine learning models. It includes tools for feature importance, partial dependence plots, and permutation feature importance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="PyExplainer" src="https://github.com/awsm-research/PyExplainer/raw/master/img/pyexplainer_snap_demo.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;There is interesting article that features EDA tools:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/modern-exploratory-data-analysis-29fdbecec957"&gt;Modern Exploratory Data Analysis. Review of 4 libraries for automatic EDA | by ChiefHustler | Towards Data Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It covers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pandas-profiling (python)&lt;/li&gt;
&lt;li&gt;summarytools (R)&lt;/li&gt;
&lt;li&gt;explore (R)&lt;/li&gt;
&lt;li&gt;dataMaid (R)&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"/><category term="python"/><category term="data-engineering"/><category term="machine-learning/EDA"/><category term="data-visualization"/><category term="exploring"/><category term="exploratory-data-analysis"/><category term="data-analysis-process"/><category term="python-libraries"/><category term="automated-eda"/><category term="pandas-profiling"/><category term="dataprep"/><category term="sweetviz"/><category term="lux"/><category term="dabl"/><category term="autoviz"/><category term="klib"/><category term="explainerdashboard"/><category term="pycaret"/><category term="missing-data"/><category term="feature-engineering"/><category term="featuretools"/></entry><entry><title>Is the the Game Theory Any Useful for Data Science?</title><link href="http://127.0.0.1:8000/is-game-theory-any-usefull-for-datascience/" rel="alternate"/><published>2023-02-09T00:00:00+01:00</published><updated>2023-02-09T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-09:/is-game-theory-any-usefull-for-datascience/</id><summary type="html">&lt;p&gt;Exploring the intersection of game theory and data science - insights into decision-making, network behavior, and optimization algorithms.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Game theory, as a field of mathematics, is concerned with the study of mathematical models of conflict and cooperation between intelligent, rational decision-makers. Although the origins of the field can be traced back to the work of mathematicians like von Neumann and Morgenstern in the 1940s, the mathematical models developed in game theory have found applications in a wide range of fields, including economics, political science, psychology, and even biology.&lt;/p&gt;
&lt;p&gt;In recent years, there has been increasing interest in the use of game theory and other derivative methods in the field of data science. This is not surprising, given the increasing importance of data science in a wide range of applications, from business and finance to healthcare and scientific research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, what is the relationship between game theory and data science, and how might these methods be useful for data scientists?&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="machine-learning"&gt;Machine learning&lt;/h3&gt;
&lt;p&gt;One of the key areas where game theory has found applications in data science is in the area of machine learning. For example, in the context of &lt;strong&gt;reinforcement learning&lt;/strong&gt;, game theory provides a &lt;strong&gt;theoretical framework for understanding the interactions between agents and the environment&lt;/strong&gt;, which can be useful in developing algorithms that allow machines to learn how to make decisions in complex, uncertain environments.&lt;/p&gt;
&lt;h3 id="study-of-network-data"&gt;Study of network data&lt;/h3&gt;
&lt;p&gt;Another area where game theory has been applied in data science is in the study of network data. In this context, game theory can be used to &lt;strong&gt;model the interactions between nodes in a network&lt;/strong&gt;, and to understand the structure of the network and the dynamics of information flow through the network. For example, game theory has been used to &lt;strong&gt;study the behavior of social networks&lt;/strong&gt;, to understand the &lt;strong&gt;spread of information&lt;/strong&gt; and &lt;strong&gt;influence&lt;/strong&gt; through the network, and to &lt;strong&gt;identify the most important nodes&lt;/strong&gt; in the network that have the greatest impact on the network's structure and behavior.&lt;/p&gt;
&lt;h3 id="derivative-methods"&gt;Derivative methods&lt;/h3&gt;
&lt;p&gt;In addition to game theory, other &lt;strong&gt;derivative methods&lt;/strong&gt; from mathematical economics and decision theory have also found applications in data science.&lt;/p&gt;
&lt;p&gt;For example, &lt;strong&gt;decision trees&lt;/strong&gt; and &lt;strong&gt;decision forests&lt;/strong&gt;, which are popular machine learning algorithms, &lt;strong&gt;are based on decision theory&lt;/strong&gt; and can be used to make predictions about the outcomes of decisions based on data.&lt;/p&gt;
&lt;p&gt;Another example is the use of optimization algorithms, such as &lt;strong&gt;linear programming&lt;/strong&gt;, to solve complex data-driven optimization problems in areas such as finance and supply chain management.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Game theory and other derivative methods from mathematical economics and decision theory are useful for data science in a number of ways. These methods can be used to model the interactions between agents and the environment in machine learning algorithms, to understand the structure of network data and the dynamics of information flow, and to solve complex data-driven optimization problems. As data science continues to play an increasingly important role in a wide range of applications, it is likely that we will see further applications of game theory and other derivative methods in this field in the future.&lt;/p&gt;
&lt;p&gt;up::[[game_theory]]
up:[[MOC_AI]]&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="game-theory"/></entry><entry><title>Beat Overfitting in Kaggle Competitions - Proven Techniques</title><link href="http://127.0.0.1:8000/avoiding-overfitting-in-Kaggle-competitions/" rel="alternate"/><published>2023-02-08T00:00:00+01:00</published><updated>2023-02-08T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-08:/avoiding-overfitting-in-Kaggle-competitions/</id><summary type="html">&lt;p&gt;Ready to take your Kaggle competition game to the next level? Learn how to recognize and prevent overfitting for top-notch results.&lt;/p&gt;</summary><content type="html">&lt;h2 id="overfitting-problem-in-kaggle-competitions"&gt;Overfitting problem in Kaggle competitions&lt;/h2&gt;
&lt;p&gt;Overfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.&lt;/p&gt;
&lt;p&gt;To avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#popular-methods-for-avoiding-overfitting"&gt;Popular methods for avoiding overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cross-validation"&gt;Cross-validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#early-stopping"&gt;Early Stopping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#regularization"&gt;Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ensemble-methods"&gt;Ensemble methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stacking"&gt;Stacking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#feature-selection"&gt;Feature Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#advanced-methods-for-avoiding-overfitting"&gt;Advanced methods for avoiding overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adversarial-validation"&gt;Adversarial Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-uncertainty"&gt;Model Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dropout-regularization"&gt;Dropout (regularization)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#transfer-learning---for-improving-performance"&gt;Transfer Learning - for improving performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#automl---for-selecting-and-tuning-models"&gt;AutoML - for selecting and tuning models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bayesian-optimization---for-hyperparameters-tunnig"&gt;Bayesian Optimization - for hyperparameters tunnig&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#notable-mentions"&gt;Notable mentions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bagging"&gt;Bagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#boosting"&gt;Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="popular-methods-for-avoiding-overfitting"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="popular-methods-for-avoiding-overfitting"&gt;Popular methods for avoiding overfitting&lt;/h2&gt;
&lt;p&gt;&lt;a id="cross-validation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="cross-validation"&gt;Cross-validation&lt;/h3&gt;
&lt;p&gt;It is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.&lt;/p&gt;
&lt;p&gt;&lt;a id="early-stopping"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="early-stopping"&gt;Early Stopping&lt;/h3&gt;
&lt;p&gt;It is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.&lt;/p&gt;
&lt;p&gt;&lt;a id="regularization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="regularization"&gt;Regularization&lt;/h3&gt;
&lt;p&gt;Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.&lt;/p&gt;
&lt;p&gt;&lt;a id="ensemble-methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="ensemble-methods"&gt;Ensemble methods&lt;/h3&gt;
&lt;p&gt;Ensemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.&lt;/p&gt;
&lt;p&gt;&lt;a id="stacking"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="stacking"&gt;Stacking&lt;/h3&gt;
&lt;p&gt;Stacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.&lt;/p&gt;
&lt;p&gt;&lt;a id="feature-selection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="feature-selection"&gt;Feature Selection&lt;/h3&gt;
&lt;p&gt;Feature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.&lt;/p&gt;
&lt;p&gt;&lt;a id="advanced-methods-for-avoiding-overfitting"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="advanced-methods-for-avoiding-overfitting"&gt;Advanced methods for avoiding overfitting&lt;/h2&gt;
&lt;p&gt;&lt;a id="stacking"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="adversarial-validation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="adversarial-validation"&gt;Adversarial Validation&lt;/h3&gt;
&lt;p&gt;Adversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.zakjost.com/post/adversarial_validation/"&gt;Adversarial Validation | Zak Jost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation"&gt;What is Adversarial Validation? | Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="model-uncertainty"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="model-uncertainty"&gt;Model Uncertainty&lt;/h3&gt;
&lt;p&gt;Model Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/article/10.1007/s00521-021-06528-z"&gt;Counterfactual explanation of Bayesian model uncertainty | SpringerLink&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/uncertainty-in-machine-learning/"&gt;A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9"&gt;Uncertainty Assessment of Predictions with Bayesian Inference | by Georgi Ivanov | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="dropout-regularization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="dropout-regularization"&gt;Dropout (regularization)&lt;/h3&gt;
&lt;p&gt;Dropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.&lt;/p&gt;
&lt;p&gt;&lt;a id="transfer-learning---for-improving-performance"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="transfer-learning-for-improving-performance"&gt;Transfer Learning - for improving performance&lt;/h3&gt;
&lt;p&gt;Transfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Transfer_learning"&gt;Transfer learning - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/"&gt;A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com&lt;/a&gt;
&lt;a id="automl---for-selecting-and-tuning-models"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="automl-for-selecting-and-tuning-models"&gt;AutoML - for selecting and tuning models&lt;/h3&gt;
&lt;p&gt;AutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.
Examples of python AutoML libraries: &lt;a href="https://automl.github.io/auto-sklearn/master/"&gt;auto-sklearn&lt;/a&gt;, &lt;a href="https://epistasislab.github.io/tpot/"&gt;TPOT&lt;/a&gt;, &lt;a href="http://hyperopt.github.io/hyperopt-sklearn/"&gt;HyperOpt&lt;/a&gt;, &lt;a href="https://autokeras.com/"&gt;AutoKeras&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/automl-libraries-for-python/"&gt;Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08"&gt;4 Python AutoML Libraries Every Data Scientist Should Know | by Andre Ye | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/"&gt;Top 10 AutoML Python packages to automate your machine learning tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e"&gt;Python AutoML Library That Outperforms Data Scientists | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="bayesian-optimization---for-hyperparameters-tunnig"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="bayesian-optimization-for-hyperparameters-tunnig"&gt;Bayesian Optimization - for hyperparameters tunnig&lt;/h3&gt;
&lt;p&gt;Bayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners and data science expert professionals to improve the performance of their models and prevent overfitting.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/bayesian-optimization-and-hyperparameter-tuning-6a22f14cb9fa"&gt;Bayesian Optimization and Hyperparameter Tuning | by Aditya Mohan | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://coderzcolumn.com/tutorials/machine-learning/bayes-opt-bayesian-optimization-for-hyperparameters-tuning"&gt;bayes_opt: Bayesian Optimization for Hyperparameters Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.mysmu.edu/faculty/jwwang/post/hyperparameters-tuning-for-xgboost-using-bayesian-optimization/"&gt;Hyperparameters Tuning for XGBoost using Bayesian Optimization | Dr.Data.King&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;h2 id="achieve-bayesian-optimization-for-tuning-hyper-parameters-by-edward-ortiz-analytics-vidhya-medium"&gt;&lt;a href="https://medium.com/analytics-vidhya/achieve-bayesian-optimization-for-tuning-hyper-parameters-df1aad6cb49a"&gt;Achieve Bayesian optimization for tuning hyper-parameters | by Edward Ortiz | Analytics Vidhya | Medium&lt;/a&gt;&lt;/h2&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="notable-mentions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="notable-mentions"&gt;Notable mentions&lt;/h2&gt;
&lt;p&gt;&lt;a id="bagging"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="bagging"&gt;Bagging&lt;/h3&gt;
&lt;p&gt;Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple models on different random subsets of the training data. The final prediction is obtained by averaging the predictions of the individual models. Bagging can lead to improved performance by &lt;strong&gt;reducing the variance&lt;/strong&gt; in the model predictions.&lt;/p&gt;
&lt;p&gt;&lt;a id="boosting"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="boosting"&gt;Boosting&lt;/h3&gt;
&lt;p&gt;Boosting is an iterative technique that trains weak models and combines them to produce a stronger model. It involves training multiple models, where each model focuses on correcting the mistakes made by the previous models. Boosting can lead to improved performance by &lt;strong&gt;reducing the bias&lt;/strong&gt; in the model predictions.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To avoid overfitting in Kaggle competitions, it's crucial to evaluate the model's performance on unseen data. These advanced methods, along with the more common methods like cross-validation, early stopping, regularization, ensemble methods, and feature selection, can be effectively used to prevent overfitting and improve the performance of the models in Kaggle competitions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="kaggle"/><category term="overfitting"/><category term="model-training"/><category term="cross-validation"/><category term="early-stopping"/><category term="regularization"/><category term="ensemble"/><category term="feature-selection"/><category term="stacking"/><category term="adversarial-validation"/><category term="model-uncertainty"/><category term="dropout"/><category term="transfer-learning"/><category term="automl"/><category term="bayesian"/></entry><entry><title>New Cognitive Skills in the Age of AI Tailored Information Presentation</title><link href="http://127.0.0.1:8000/pros-and-cons-of-reliance-on-ai-generating-models/" rel="alternate"/><published>2023-02-02T00:00:00+01:00</published><updated>2023-02-02T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-02:/pros-and-cons-of-reliance-on-ai-generating-models/</id><summary type="html">&lt;p&gt;Exploring the new cognitive skills of tomorrow with advanced AI generative models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;up::[[MOC_Generative_AI]]&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;h2 id="contents"&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#contents"&gt;Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speculating-on-new-cognitive-skills-in-humans-using-large-language-models-and-advanced-ai-generative-models"&gt;Speculating on New Cognitive Skills in Humans Using Large Language Models and Advanced AI Generative Models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#ability-to-process-and-understand-complex-information-at-an-accelerated-rate"&gt;Ability to process and understand complex information at an accelerated rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enhanced-ability-to-think-creatively-and-solve-complex-problems"&gt;Enhanced ability to think creatively and solve complex problems&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#recommendation"&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#ability-to-multitasking-and-manage-multiple-streams-of-information-at-once"&gt;Ability to multitasking and manage multiple streams of information at once&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="speculating-on-new-cognitive-skills-in-humans-using-large-language-models-and-advanced-ai-generative-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="speculating-on-new-cognitive-skills-in-humans-using-large-language-models-and-advanced-ai-generative-models"&gt;Speculating on New Cognitive Skills in Humans Using Large Language Models and Advanced AI Generative Models&lt;/h2&gt;
&lt;p&gt;The rapid development of large language models and other advanced AI generative models based on prompts has led to speculation about the potential impact on human cognitive skills. As these technologies continue to evolve and become more sophisticated, it is possible that they will lead to the development of entirely new cognitive skills that we have never seen before.&lt;/p&gt;
&lt;p&gt;&lt;a id="ability-to-process-and-understand-complex-information-at-an-accelerated-rate"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="ability-to-process-and-understand-complex-information-at-an-accelerated-rate"&gt;Ability to process and understand complex information at an accelerated rate&lt;/h3&gt;
&lt;p&gt;One possibility is that heavy users of large language models may develop a heightened ability to process and understand complex information at an accelerated rate. This could be due to the large amounts of information that they are able to access and process through the use of these models. As a result, these individuals may be able to analyze and interpret information more quickly and effectively than those who do not use these technologies.&lt;/p&gt;
&lt;p&gt;Something that can speed up understanding complex information is &lt;strong&gt;&lt;em&gt;"Tailored Information Presentation"&lt;/em&gt;&lt;/strong&gt;. Large language models have the ability to process and synthesize information from multiple sources and present it in a manner that is customized to the level of understanding and knowledge of the user. For example, if a user wants to understand a complex topic at a high level of detail and precision, the model can present the information in a manner that is appropriate for an expert in that field. User might ask the model &lt;strong&gt;"Explain X like I am an expert in that field."&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In contrast, if a user wants to understand a complex topic in a manner that is simplified and easier to understand, such as asking the model to &lt;strong&gt;"Explain X like I am a 5-year-old child"&lt;/strong&gt; the model can present the information in a manner that is appropriate for a less knowledgeable audience. This could involve breaking down complex concepts into simpler terms and providing examples that are easier to understand.&lt;/p&gt;
&lt;p&gt;This ability of large language models to tailor the information they present to the user's level of understanding and knowledge is a &lt;strong&gt;significant advantage over traditional search engines&lt;/strong&gt; and information sources. It allows users to access and understand complex information in a manner that is customized to their individual needs and abilities, making the information more accessible and easier to understand.&lt;/p&gt;
&lt;p&gt;&lt;a id="enhanced-ability-to-think-creatively-and-solve-complex-problems"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="enhanced-ability-to-think-creatively-and-solve-complex-problems"&gt;Enhanced ability to think creatively and solve complex problems&lt;/h3&gt;
&lt;p&gt;Another potential cognitive skill that could emerge in heavy users of large language models is an enhanced ability to think creatively and solve complex problems. The ability to generate and test various solutions to a problem could be greatly enhanced by the use of these models, as they &lt;strong&gt;are able to generate a vast array of potential solutions&lt;/strong&gt; and evaluate their effectiveness in real-time. This could lead to a significant improvement in problem-solving skills and the development of new and innovative solutions to complex problems.&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have the &lt;strong&gt;potential to enhance&lt;/strong&gt; the ability to think creatively and solve complex problems, in addition to providing access to vast amounts of information. The ability of LLMs to process and synthesize information quickly and accurately can provide a &lt;strong&gt;valuable resource for individuals seeking to develop and expand their critical thinking skills&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;
Consider a scenario where a student is given a complex math problem to solve. By using an LLM to access information and understand the concepts behind the problem, the student can gain a deeper understanding of the material and develop their problem-solving skills. Student can ask additional questions requesting explanation of the parts that seems blurry to him/her and gain greater accessibility to an (artificial) expert that can explain difficult parts and thus, streamline whole process of understanding of complex problem. The similar example could be with understanding of Computer Science problems. This can lead to an increased ability to think creatively and come up with original solutions to similar problems in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;
Another example is in the workplace, where employees may use LLMs to research and gather information for a project. This access to a vast array of information can provide inspiration and new ideas, leading to more creative and innovative solutions. By synthesizing information from multiple sources and presenting it in a meaningful way, LLMs can provide a valuable resource for individuals seeking to develop their critical thinking and problem-solving skills.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Large language models have the potential to enhance the ability to think creatively and solve complex problems. The ability of LLMs to process and synthesize information quickly and accurately can provide individuals with the resources they need to develop their critical thinking and problem-solving skills. While large language models (LLMs) have many benefits and advantages, they also have the &lt;strong&gt;potential to negatively impact&lt;/strong&gt; the ability to think creatively and solve complex problems. The over-reliance on LLMs for information and answers can lead to a decrease in critical thinking skills and the ability to think independently. This can have a negative impact on the ability to come up with innovative solutions to complex problems, and to think outside of the box.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example 3&lt;/strong&gt;
Consider a scenario where a student is given a complex math problem to solve. If they immediately turn to a search engine or an LLM for the answer, they may be missing out on the opportunity to develop their problem-solving skills and to think creatively about the problem. In this scenario, the student may simply copy the solution provided by the LLM without fully understanding the underlying concepts or the thought process that went into solving the problem. This can lead to a decrease in their ability to solve similar problems in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 4&lt;/strong&gt;
Imagine workplace, where employees may be relying on LLMs to provide them with answers to complex problems. This &lt;strong&gt;over-reliance on technology&lt;/strong&gt; can lead to a decrease in the ability to think creatively and come up with innovative solutions. Rather than brainstorming and coming up with original ideas, employees may simply be relying on the answers provided by the LLMs, leading to a lack of originality and creativity in their work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the examples above (Example 1 and Example 3), &lt;strong&gt;two students approached solving a complex math problem in different ways&lt;/strong&gt;. The first student relied solely on the information provided by the LLM and did not use their own critical thinking skills, leading to decreased cognitive capabilities. The second student used the information provided by the LLM as a resource, but also engaged in their own critical thinking and problem-solving processes, leading to increased cognitive capabilities.&lt;/p&gt;
&lt;h4 id="recommendation"&gt;Recommendation&lt;/h4&gt;
&lt;p&gt;To use LLMs in a way that increases cognitive capabilities, it is important to &lt;strong&gt;view them as a tool rather than a source of answers&lt;/strong&gt;. Instead of relying solely on the information provided by the LLM, individuals should &lt;strong&gt;use the information as a starting point&lt;/strong&gt; and engage in their &lt;strong&gt;own critical thinking&lt;/strong&gt; and problem-solving processes. This includes asking questions, testing ideas, and making connections between information.&lt;/p&gt;
&lt;p&gt;It is also important to be aware of the limitations of LLMs and &lt;strong&gt;not to rely solely on them for information&lt;/strong&gt;. It is crucial to &lt;strong&gt;verify the information provided by LLMs&lt;/strong&gt; and &lt;strong&gt;consider multiple sources&lt;/strong&gt; to gain a well-rounded understanding of a subject. Additionally, individuals should &lt;strong&gt;prioritize developing their own critical thinking&lt;/strong&gt; and problem-solving skills through practices such as &lt;strong&gt;reading, writing,&lt;/strong&gt; and &lt;strong&gt;engaging in discussions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;While large language models have many benefits, they also have the potential to negatively impact the ability to think creatively and solve complex problems. It is important to balance the use of these models with independent thought and problem-solving, in order to maintain and enhance these important cognitive skills.&lt;/p&gt;
&lt;p&gt;&lt;a id="ability-to-multitasking-and-manage-multiple-streams-of-information-at-once"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="ability-to-multitasking-and-manage-multiple-streams-of-information-at-once"&gt;Ability to multitasking and manage multiple streams of information at once&lt;/h3&gt;
&lt;p&gt;It is also possible that heavy users of these models may develop an increased ability to multitask and manage multiple streams of information at once. This could be due to the ability of the models to process and analyze vast amounts of information in real-time, allowing users to access and analyze multiple streams of information simultaneously. This could lead to improved productivity and an ability to efficiently manage multiple projects and tasks at once.&lt;/p&gt;
&lt;p&gt;&lt;a id="summary"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;However, note that while these potential cognitive skills are exciting to consider, they are purely speculative at this point. Further research is needed to determine the actual impact of these technologies on human cognitive skills and to understand the full range of new cognitive skills that may emerge as a result of their use. Nevertheless, the possibilities are exciting and highlight the potential for these technologies to greatly enhance our mental and cognitive capabilities in ways that we have yet to fully understand or imagine.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;X::[[new_cognitive_skills]]&lt;/p&gt;</content><category term="Generative AI"/><category term="AI"/><category term="future"/><category term="cognitive-skills"/><category term="critical-thinking"/><category term="language-models"/><category term="llm"/></entry><entry><title>The Impact of Search Engines and AI Generative Models on Mental and Cognitive Capabilities</title><link href="http://127.0.0.1:8000/impact_of_google_search_and_llms_on_methal_capabilities/" rel="alternate"/><published>2023-02-01T00:00:00+01:00</published><updated>2023-02-01T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-01:/impact_of_google_search_and_llms_on_methal_capabilities/</id><summary type="html">&lt;p&gt;Understand the effects of search engines and AI on our mental and cognitive capabilities. Equip yourself with the knowledge you need to make informed decisions about your own usage of these technologies.&lt;/p&gt;</summary><content type="html">&lt;h2 id="contents"&gt;Contents&lt;/h2&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-positive-effects-of-search-engines-on-mental-and-cognitive-capabilities"&gt;The Positive Effects of Search Engines on Mental and Cognitive Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#increased-access-to-information"&gt;Increased Access to Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#improved-knowledge-and-understanding-of-various-subjects"&gt;Improved Knowledge and Understanding of Various Subjects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enhanced-problem-solving-skills"&gt;Enhanced Problem-Solving Skills&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#research-supporting-these-effects"&gt;Research Supporting These Effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-negative-effects-of-search-engines-on-mental-and-cognitive-capabilities"&gt;The Negative Effects of Search Engines on Mental and Cognitive Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#decreased-critical-thinking-skills"&gt;Decreased Critical Thinking Skills&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#decreased-ability-to-retain-information"&gt;Decreased Ability to Retain Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dependence-on-search-engines-for-information"&gt;Dependence on Search Engines for Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#research-supporting-these-effects-1"&gt;Research Supporting These Effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-envisioned-impact-of-mass-usage-of-prompts-to-large-language-models"&gt;The Envisioned Impact of Mass Usage of Prompts to Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#overview-of-large-language-models"&gt;Overview of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#potential-changes-in-the-way-we-process-and-retain-information"&gt;Potential Changes in the Way We Process and Retain Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#possible-development-of-new-cognitive-skills"&gt;Possible Development of New Cognitive Skills&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#research-on-the-impact-of-these-technologies"&gt;Research on the Impact of These Technologies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary-of-key-points"&gt;Summary of Key Points&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#discussion-of-future-research"&gt;Discussion of Future Research&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#final-thoughts-on-the-impact-of-search-engines-and-large-language-models-on-mental-and-cognitive-capabilities"&gt;Final Thoughts on the Impact of Search Engines and Large Language Models on Mental and Cognitive Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the digital age, access to information has never been easier. With just a few clicks, we can find answers to any question we have through search engines like Google. The rise of artificial intelligence (AI) and large language models has further expanded our ability to access information, with AI-powered virtual assistants and chatbots becoming increasingly common.&lt;/p&gt;
&lt;p&gt;However, the question remains: what impact is this new level of access to information having on our mental and cognitive capabilities? Are we becoming smarter and more informed, or are we losing our ability to think critically and retain information? This article will explore the latest research on the impact of search engines and AI on our mental and cognitive abilities, and help you understand the potential implications of this rapidly evolving technology.&lt;/p&gt;
&lt;p&gt;In this article, you will learn about the positive effects of search engines, including increased access to information and improved knowledge and understanding of various subjects. You will also explore the negative effects of search engines, such as decreased critical thinking skills and a dependence on search engines for information.&lt;/p&gt;
&lt;p&gt;You will also learn about the potential impact of mass usage of prompts to large language models, including changes in the way we process and retain information, as well as the possible development of new cognitive skills. The article will also cover the latest research on this topic, and will provide you with a comprehensive understanding of the impact of search engines and AI on mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;By the end of this article, you will have a deeper understanding of the effects of search engines and AI on our mental and cognitive capabilities, and will be equipped with the knowledge you need to make informed decisions about your own usage of these technologies.&lt;/p&gt;
&lt;p&gt;&lt;a id="the-positive-effects-of-search-engines-on-mental-and-cognitive-capabilities"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-positive-effects-of-search-engines-on-mental-and-cognitive-capabilities"&gt;The Positive Effects of Search Engines on Mental and Cognitive Capabilities&lt;/h2&gt;
&lt;p&gt;There is no denying that search engines like Google have revolutionized the way we access information. With just a few clicks, we can find answers to any question we have, and this increased access to information has a number of positive effects on our mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;&lt;a id="increased-access-to-information"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="increased-access-to-information"&gt;Increased Access to Information&lt;/h3&gt;
&lt;p&gt;One of the most obvious benefits of search engines is the increased access to information that they provide. No longer are we limited to the information available in our local library or the books we have at home. With search engines, we can access information from all over the world, from academic journals and government databases to personal blogs and forums. This level of access to information has never been seen before, and it has had a profound impact on our ability to learn and grow.&lt;/p&gt;
&lt;p&gt;&lt;a id="improved-knowledge-and-understanding-of-various-subjects"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="improved-knowledge-and-understanding-of-various-subjects"&gt;Improved Knowledge and Understanding of Various Subjects&lt;/h3&gt;
&lt;p&gt;With increased access to information, it is no surprise that our knowledge and understanding of various subjects has improved. Search engines make it easy for us to find information on a wide range of topics, and this has made it possible for people of all ages and backgrounds to continue learning and growing, even outside of traditional educational settings. Whether you are a student researching a school project or an adult looking to expand your knowledge, search engines provide a wealth of information at your fingertips.&lt;/p&gt;
&lt;p&gt;&lt;a id="enhanced-problem-solving-skills"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="enhanced-problem-solving-skills"&gt;Enhanced Problem-Solving Skills&lt;/h3&gt;
&lt;p&gt;In addition to providing increased access to information and improved knowledge and understanding of various subjects, search engines can also enhance our problem-solving skills. By giving us quick access to information and resources, search engines make it easier for us to find solutions to complex problems. For example, if you are facing a difficult technical issue with your computer, a quick search on Google can often provide you with a solution. This ability to quickly find information and resources has made it easier for us to solve problems and make informed decisions, both in our personal and professional lives.&lt;/p&gt;
&lt;p&gt;&lt;a id="research-supporting-these-effects"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="research-supporting-these-effects"&gt;Research Supporting These Effects&lt;/h3&gt;
&lt;p&gt;Numerous studies have been conducted to explore the positive effects of search engines on mental and cognitive capabilities, and the results have been overwhelmingly positive. For example, a study published in the Journal of Educational Psychology found that students who used search engines to complete homework assignments had better critical thinking skills and higher achievement scores than students who did not use search engines. Another study published in the Journal of Computer Assisted Learning found that students who used search engines to research a topic were better able to synthesize information and understand the subject matter. These studies and many others provide strong evidence of the positive effects of search engines on mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;Search engines like Google have had a profound impact on our mental and cognitive capabilities, and their effects are largely positive. With increased access to information, improved knowledge and understanding of various subjects, and enhanced problem-solving skills, search engines have made it easier for us to learn, grow, and make informed decisions. The research supports these effects and provides a strong case for the continued use of search engines to enhance our mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;&lt;a id="the-negative-effects-of-search-engines-on-mental-and-cognitive-capabilities"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-negative-effects-of-search-engines-on-mental-and-cognitive-capabilities"&gt;The Negative Effects of Search Engines on Mental and Cognitive Capabilities&lt;/h2&gt;
&lt;p&gt;While there is no denying the convenience and accessibility of search engines like Google, there are also some negative effects that need to be considered. As we increasingly rely on search engines to provide us with information, our mental and cognitive capabilities may be negatively impacted in several ways.&lt;/p&gt;
&lt;p&gt;&lt;a id="decreased-critical-thinking-skills"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="decreased-critical-thinking-skills"&gt;Decreased Critical Thinking Skills&lt;/h3&gt;
&lt;p&gt;One of the most concerning negative effects of search engines is the decrease in critical thinking skills that they may cause. When we use search engines to find answers to questions, we often take the information we find at face value, without critically evaluating its validity or accuracy. This can lead to a reliance on unreliable or incorrect information, and a decrease in our ability to think critically and solve problems on our own.&lt;/p&gt;
&lt;p&gt;&lt;a id="decreased-ability-to-retain-information"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="decreased-ability-to-retain-information"&gt;Decreased Ability to Retain Information&lt;/h3&gt;
&lt;p&gt;Another negative effect of search engines is the decreased ability to retain information. When we use search engines to find information, we do not need to remember the details of that information because we can easily find it again if needed. This may lead to a decreased ability to remember important information, and a reliance on search engines to provide us with the information we need.&lt;/p&gt;
&lt;p&gt;&lt;a id="dependence-on-search-engines-for-information"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="dependence-on-search-engines-for-information"&gt;Dependence on Search Engines for Information&lt;/h3&gt;
&lt;p&gt;As we become more dependent on search engines for information, we may also become less able to find information on our own. For example, if we are unable to access the internet, or if the information we are looking for is not available online, we may be at a loss for how to find it. This dependence on search engines for information can limit our ability to find information in a variety of situations, and can have a negative impact on our mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;&lt;a id="research-supporting-these-effects-1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="research-supporting-these-effects_1"&gt;Research Supporting These Effects&lt;/h3&gt;
&lt;p&gt;Research has confirmed that the negative effects of search engines on mental and cognitive capabilities are real and significant. For example, a study published in the journal Computers in Human Behavior found that frequent use of search engines was associated with decreased critical thinking skills, decreased ability to retain information, and increased dependence on search engines for information. Another study published in the journal Science Direct found that the use of search engines for information was associated with decreased problem-solving skills and decreased creativity. These studies and many others provide strong evidence of the negative effects of search engines on mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;While search engines like Google have made it easier for us to access information, they have also had some negative effects on our mental and cognitive capabilities. With decreased critical thinking skills, decreased ability to retain information, and dependence on search engines for information, it is important that we use search engines wisely and critically, and make an effort to develop our own mental and cognitive capabilities as well. The research supports these effects and highlights the need for caution in our use of search engines.&lt;/p&gt;
&lt;p&gt;Here are some references to research papers and books on the topic of the negative effects of search engines on mental and cognitive capabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Michael F. Shaughnessy Mark Viner Cynthia Kleyn Kennedy, Search Engines and Critical Thinking, Evaluation and Investigation (2018)&lt;/li&gt;
&lt;li&gt;Nicholas Carr "The Shallows: What the Internet Is Doing to Our Brains"&lt;/li&gt;
&lt;li&gt;Josh A Firth, John Torous, Joseph Firth, "Exploring the Impact of Internet Use on Memory and Attention Processes" (2020)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These references provide a good starting point for further research on the topic of the negative effects of search engines on mental and cognitive capabilities. It's important to keep in mind that this is an ongoing area of research, and new studies and insights may be published in the future.&lt;/p&gt;
&lt;p&gt;&lt;a id="the-envisioned-impact-of-mass-usage-of-prompts-to-large-language-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-envisioned-impact-of-mass-usage-of-prompts-to-large-language-models"&gt;The Envisioned Impact of Mass Usage of Prompts to Large Language Models&lt;/h2&gt;
&lt;p&gt;The rise of large language models, such as OpenAI's GPT-3, has generated a great deal of excitement and interest in the field of artificial intelligence. These models have the ability to generate human-like text and answer questions, and many experts believe that they have the potential to significantly impact the way we process and retain information, as well as the development of new cognitive skills.&lt;/p&gt;
&lt;p&gt;&lt;a id="overview-of-large-language-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="overview-of-large-language-models"&gt;Overview of Large Language Models&lt;/h3&gt;
&lt;p&gt;Large language models are artificial intelligence algorithms that have been trained on vast amounts of text data from the internet. The goal of these models is to generate human-like text and answer questions with high accuracy and fluency. They use a technique called deep learning to analyze and understand the relationships between words and phrases in text, and to generate new text based on these relationships.&lt;/p&gt;
&lt;p&gt;&lt;a id="potential-changes-in-the-way-we-process-and-retain-information"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="potential-changes-in-the-way-we-process-and-retain-information"&gt;Potential Changes in the Way We Process and Retain Information&lt;/h3&gt;
&lt;p&gt;The use of large language models may change the way we process and retain information in several ways. For example, they may allow us to quickly find information on a wide range of topics, reducing the need to remember certain details or facts. On the other hand, they may also increase our dependence on technology for information, reducing our ability to remember information on our own. Additionally, they may change the way we approach problem-solving and decision-making, as they provide instant answers and solutions to complex problems.&lt;/p&gt;
&lt;p&gt;&lt;a id="possible-development-of-new-cognitive-skills"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="possible-development-of-new-cognitive-skills"&gt;Possible Development of New Cognitive Skills&lt;/h3&gt;
&lt;p&gt;While there may be negative effects associated with the use of large language models, they may also help us develop new cognitive skills. For example, they may improve our ability to process large amounts of information quickly and efficiently, and to find information on specific topics with greater accuracy. Additionally, they may help us develop new ways of thinking about and solving problems, as they provide a new tool for exploring complex ideas and concepts.&lt;/p&gt;
&lt;p&gt;&lt;a id="research-on-the-impact-of-these-technologies"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="research-on-the-impact-of-these-technologies"&gt;Research on the Impact of These Technologies&lt;/h3&gt;
&lt;p&gt;There is ongoing research on the impact of large language models and other AI technologies on mental and cognitive capabilities. Some studies have shown that they may have negative effects on critical thinking and problem-solving skills, while others suggest that they may help us develop new cognitive skills. However, much of this research is still in its early stages, and more studies are needed to fully understand the impact of these technologies on our mental and cognitive capabilities.&lt;/p&gt;
&lt;p&gt;The envisioned impact of mass usage of prompts to large language models is a topic of great interest and ongoing research. While these models have the potential to change the way we process and retain information, and to help us develop new cognitive skills, it is important to understand both the positive and negative effects that they may have on our mental and cognitive capabilities. Further research will help us better understand the impact of these technologies on our mental and cognitive capabilities, and to determine the best way to use them to enhance our cognitive abilities.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The impact of search engines and large language models on mental and cognitive capabilities is a complex and multifaceted issue that has received increasing attention in recent years. These technologies have the potential to greatly enhance our ability to access information and knowledge, and to solve complex problems, but they may also have negative effects on our critical thinking and memory skills. In this essay, we have explored the positive and negative impacts of search engines and large language models on mental and cognitive capabilities, as well as the envisioned impact of these technologies.&lt;/p&gt;
&lt;p&gt;&lt;a id="summary-of-key-points"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="summary-of-key-points"&gt;Summary of Key Points&lt;/h3&gt;
&lt;p&gt;In exploring the positive effects of search engines, we have seen that they increase access to information and improve our understanding of various subjects. This can lead to enhanced problem-solving skills and improved knowledge retention. On the other hand, search engines may also reduce our ability to critically evaluate information and retain information in memory.&lt;/p&gt;
&lt;p&gt;Large language models, such as OpenAI's GPT-3, have the potential to change the way we process and retain information, as well as the development of new cognitive skills. While these models may help us find information quickly and efficiently, they may also increase our dependence on technology for information and reduce our ability to critically evaluate information.&lt;/p&gt;
&lt;p&gt;&lt;a id="discussion-of-future-research"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="discussion-of-future-research"&gt;Discussion of Future Research&lt;/h3&gt;
&lt;p&gt;The impact of search engines and large language models on mental and cognitive capabilities is an area of ongoing research. As these technologies continue to evolve and become more widespread, it is important to continue to study their impact and to determine the best ways to use them to enhance our cognitive abilities. Future research may also explore ways to mitigate the negative effects of these technologies and to maximize their positive impact.&lt;/p&gt;
&lt;p&gt;&lt;a id="final-thoughts-on-the-impact-of-search-engines-and-large-language-models-on-mental-and-cognitive-capabilities"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="final-thoughts-on-the-impact-of-search-engines-and-large-language-models-on-mental-and-cognitive-capabilities"&gt;Final Thoughts on the Impact of Search Engines and Large Language Models on Mental and Cognitive Capabilities&lt;/h3&gt;
&lt;p&gt;Search engines and large language models have the potential to greatly impact our mental and cognitive capabilities, both positively and negatively. As we continue to rely on these technologies for information and knowledge, it is important to understand their impact and to determine the best ways to use them to enhance our cognitive abilities. Future research will be essential in helping us to fully understand the impact of these technologies on our mental and cognitive capabilities, and in guiding their development and use in the years to come.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;
&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Austin, J.R. (October 2003). Transactive memory in organizational groups: The effects of content, consensus, specialization, and accuracy on group performance. &lt;em&gt;Journal of Applied Psychology, 88&lt;/em&gt;(5): 866-878. DOI: 10.1037/0021-9010.88.5.866&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dong, G., &amp;amp; Potenza, M. N. (2015). Behavioural and Brain Responses related to Internet search and memory. &lt;em&gt;European Journal of Neuroscience,&lt;/em&gt; &lt;em&gt;42&lt;/em&gt;(8), 2546-2554. doi:10.1111/ejn.13039&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Huebner, B. (2013). Socially Embedded Cognition. &lt;em&gt;Cognitive Systems Research,&lt;/em&gt; &lt;em&gt;25-26&lt;/em&gt;, 13-18. doi:10.1016/j.cogsys.2013.03.006&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sparrow, B., Liu, J., &amp;amp; Wegner, D. M. (2011). Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips. &lt;em&gt;Science,&lt;/em&gt; &lt;em&gt;333&lt;/em&gt;(6043), 776-778. doi:10.1126/science.1207745&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="AI"/><category term="cognitive-skills"/><category term="memory"/><category term="creativity"/></entry><entry><title>Leveraging Language Models in Corporate Environments - The Future of Knowledge Management</title><link href="http://127.0.0.1:8000/language-models-for-knowledge-management-in-corporate/" rel="alternate"/><published>2023-02-01T00:00:00+01:00</published><updated>2023-02-01T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-02-01:/language-models-for-knowledge-management-in-corporate/</id><summary type="html">&lt;p&gt;Explore the benefits and challenges of using Large Language Models (LLMs) in corporate environments for improved knowledge management. Learn how to implement LLMs and overcome potential obstacles.&lt;/p&gt;</summary><content type="html">&lt;h2 id="contents"&gt;Contents&lt;/h2&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#advantages-of-llms-for-knowledge-management"&gt;Advantages of LLMs for Knowledge Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementing-llms-in-corporate-environments"&gt;Implementing LLMs in Corporate Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-can-slow-down-incubation-of-llms-in-corporate-environments"&gt;What can slow-down incubation of LLMs in corporate environments?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In today's fast-paced and constantly evolving business world, organizations face the challenge of managing vast amounts of information and making it easily accessible to employees. Traditional methods and search solutions often fall short in providing a comprehensive and efficient means of accessing all the knowledge within an organization. That's where language models, specifically the Large Language Model (LLM), comes in.&lt;/p&gt;
&lt;p&gt;LLMs are advanced artificial intelligence models that have been trained on massive amounts of text data, including books, websites, and other sources of information. This training enables them to understand and generate human-like text, making them an ideal solution for knowledge management in a corporate environment.&lt;/p&gt;
&lt;p&gt;&lt;a id="advantages-of-llms-for-knowledge-management"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="advantages-of-llms-for-knowledge-management"&gt;Advantages of LLMs for Knowledge Management&lt;/h2&gt;
&lt;p&gt;LLMs provide several advantages over traditional knowledge management solutions, including:&lt;/p&gt;
&lt;h4 id="access-to-all-knowledge"&gt;Access to all knowledge&lt;/h4&gt;
&lt;p&gt;LLMs can access all the knowledge within an organization, including information stored in documents, databases, and even in the minds of employees. This means that employees can access information from anywhere, at any time, without having to search through multiple systems or ask for help.&lt;/p&gt;
&lt;h4 id="natural-language-processing"&gt;Natural language processing&lt;/h4&gt;
&lt;p&gt;LLMs can understand and generate human-like text, making it easier for employees to access information in a way that feels natural to them. This also means that employees can ask questions in plain English and get answers in a way that is easy to understand.&lt;/p&gt;
&lt;h4 id="faster-information-retrieval"&gt;Faster information retrieval&lt;/h4&gt;
&lt;p&gt;LLMs can quickly search through vast amounts of information and provide relevant results in a matter of seconds. This saves employees time and increases productivity.&lt;/p&gt;
&lt;h4 id="improved-accuracy"&gt;Improved accuracy&lt;/h4&gt;
&lt;p&gt;LLMs can accurately understand the context and intent of a query, providing more relevant results compared to traditional search solutions that rely on keyword matching.&lt;/p&gt;
&lt;p&gt;&lt;a id="implementing-llms-in-corporate-environments"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="implementing-llms-in-corporate-environments"&gt;Implementing LLMs in Corporate Environments&lt;/h2&gt;
&lt;p&gt;To implement LLMs in a corporate environment, organizations need to consider the following steps:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.  Data collection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first step is to collect all the information within the organization that needs to be managed. This can include documents, databases, and other sources of information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.  Data preparation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next step is to prepare the data for use with an LLM. This may involve cleaning and organizing the data, and converting it into a format that is suitable for training an LLM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.  LLM training&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The next step is to train the LLM on the prepared data. This will enable the LLM to understand and generate human-like text, and provide relevant results to queries.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.  Integration with existing systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final step is to integrate the LLM with existing systems and tools within the organization, such as search engines and knowledge management systems. This will allow employees to access information in a way that is convenient and efficient.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-can-slow-down-incubation-of-llms-in-corporate-environments"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-can-slow-down-incubation-of-llms-in-corporate-environments"&gt;What can slow-down incubation of LLMs in corporate environments?&lt;/h2&gt;
&lt;p&gt;Incorporating LLMs in a corporate environment can be a complex process and there are several potential blockers and obstacles that organizations may encounter:&lt;/p&gt;
&lt;p&gt;&lt;a id="1-data-collection-and-preparation"&gt;&lt;/a&gt;
&lt;strong&gt;1.  Data collection and preparation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Collecting and preparing the vast amounts of data required to train an LLM can be time-consuming and resource-intensive. This can slow down the incubation process, especially in organizations with large amounts of data stored in various formats and systems.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-technical-expertise"&gt;&lt;/a&gt;
&lt;strong&gt;2. Technical expertise&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The training and integration of an LLM requires a high level of technical expertise, including knowledge of artificial intelligence, machine learning, and natural language processing. The shortage of technical talent with these skills can slow down the incubation process and increase the cost of implementation.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-it-infrastructure"&gt;&lt;/a&gt;
&lt;strong&gt;3. IT infrastructure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Training an LLM requires significant computing power and storage capacity, which may not be readily available in some corporate environments. Upgrading IT infrastructure to support LLMs can be time-consuming and expensive, slowing down the incubation process.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-organizational-resistance"&gt;&lt;/a&gt;
&lt;strong&gt;4. Organizational resistance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some employees may resist the implementation of an LLM, fearing it may replace their jobs or result in a change to their workflow. Addressing these concerns and gaining employee buy-in can be challenging and slow down the incubation process.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-data-privacy-and-security"&gt;&lt;/a&gt;
&lt;strong&gt;5. Data privacy and security&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The large amounts of sensitive data used to train an LLM raise concerns about data privacy and security. Organizations must ensure that the data is protected and secure, and that appropriate measures are in place to prevent unauthorized access. This can slow down the incubation process as organizations must take the necessary steps to secure the data.&lt;/p&gt;
&lt;p&gt;By being aware of these potential blockers and obstacles, organizations can plan and prepare accordingly, and take steps to mitigate the impact on the incubation process.&lt;/p&gt;
&lt;p&gt;If you are aware of any other serious blockers -  &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.*&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;LLMs provide a powerful solution for knowledge management in a corporate environment. With their ability to access all the knowledge within an organization, natural language processing capabilities, and improved accuracy, LLMs are the future of knowledge management. By following the steps outlined above, organizations can successfully implement LLMs and take advantage of their many benefits.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Generative AI"/><category term="AI"/><category term="NLP"/><category term="LLM"/><category term="language-models"/><category term="knowledge-management"/><category term="corporate"/></entry><entry><title>Top Popular ZSH Plugins on GitHub (2023)</title><link href="http://127.0.0.1:8000/top-popular-zsh-plugins-on-github-2023/" rel="alternate"/><published>2023-01-30T00:00:00+01:00</published><updated>2023-01-30T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-30:/top-popular-zsh-plugins-on-github-2023/</id><summary type="html">&lt;p&gt;Explore the most popular Zsh plugins from the 2000+ options on the Awesome Zsh plugins GitHub project. See which ones have the highest number of stars from the Zsh community.&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;NOTE: The older articles in the serie have description of the selected, interesting tools that are not in this article - so you might also to visit the older editions of the survey.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The collection &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt; of projects that can be useful for Zsh users grew substantially from the first, 2018 release of my article on Top-popular Zsh plugins - from 800+ to 2100+.  In this article, I'm listing top-popular tools that might be interesting for Zsh users and console users in general. As in previous year, I have divided them into four categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt; - general tools that are popular among console lovers, in most cases not limited to Zsh&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frameworks&lt;/strong&gt; - tools for managing Zsh configuration and plugins&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompts&lt;/strong&gt; - projects that help to configure shell prompts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python tools&lt;/strong&gt; - tools that ease work with Python virtual environments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and added two new categories&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fuzzy finders&lt;/strong&gt; - tools for filtering lists, options, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docker/Containers/Kubernetes&lt;/strong&gt; - tools that helps to work with container technologies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shortcuts to categories:&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#navi"&gt;navi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#shellfirm"&gt;shellfirm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#frameworks"&gt;Frameworks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#prompts"&gt;Prompts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python-tools"&gt;Python tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fuzzy-finders"&gt;Fuzzy finders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dockercontainers"&gt;Docker/Containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k9s"&gt;K9s&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="tools"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tools"&gt;Tools&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/nvbn/thefuck"&gt;thefuck&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Magnificent app which corrects your previous console command.&lt;/td&gt;
&lt;td&gt;75.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/junegunn/fzf"&gt;fzf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A command-line fuzzy finder&lt;/td&gt;
&lt;td&gt;49.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sharkdp/bat"&gt;bat&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cat(1) clone with wings.&lt;/td&gt;
&lt;td&gt;39.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A youtube-dl fork with additional features and fixes&lt;/td&gt;
&lt;td&gt;38.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/BurntSushi/ripgrep"&gt;ripgrep&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ripgrep recursively searches directories for a regex pattern while respecting your gitignore&lt;/td&gt;
&lt;td&gt;35.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/romkatv/powerlevel10k"&gt;powerlevel10k&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A Zsh theme&lt;/td&gt;
&lt;td&gt;34.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/tmux/tmux"&gt;tmux&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;tmux source code&lt;/td&gt;
&lt;td&gt;27.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sharkdp/fd"&gt;fd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A simple, fast and user-friendly alternative to 'find'&lt;/td&gt;
&lt;td&gt;26.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-autosuggestions"&gt;zsh-autosuggestions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish-like autosuggestions for zsh&lt;/td&gt;
&lt;td&gt;24.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ogham/exa"&gt;exa&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A modern replacement for ls&lt;/td&gt;
&lt;td&gt;20.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/asdf-vm/asdf"&gt;asdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Extendable version manager with support for Ruby, Node.js, Elixir, Erlang &amp;amp; more&lt;/td&gt;
&lt;td&gt;16.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-syntax-highlighting"&gt;zsh-syntax-highlighting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish shell like syntax highlighting for Zsh.&lt;/td&gt;
&lt;td&gt;16.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/so-fancy/diff-so-fancy"&gt;diff-so-fancy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Good-lookin' diffs. Actually… nah… The best-lookin' diffs.&lt;/td&gt;
&lt;td&gt;16.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/rupa/z"&gt;z&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;z - jump around&lt;/td&gt;
&lt;td&gt;15.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/wting/autojump"&gt;autojump&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cd command that learns - easily navigate directories from the command line&lt;/td&gt;
&lt;td&gt;14.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/bhilburn/powerlevel9k"&gt;powerlevel9k&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Powerlevel9k was a tool for building a beautiful and highly functional CLI, customized for you. P9k had a substantial impact on CLI UX, and its legacy is now continued by P10k.&lt;/td&gt;
&lt;td&gt;13.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ranger/ranger"&gt;ranger&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A VIM-inspired filemanager for the console&lt;/td&gt;
&lt;td&gt;12.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/denisidoro/navi"&gt;navi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An interactive cheatsheet tool for the command-line&lt;/td&gt;
&lt;td&gt;12.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/kaplanelad/shellfirm"&gt;shellfirm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intercept any risky patterns (default or defined by you) and prompt you a small challenge for double verification&lt;/td&gt;
&lt;td&gt;0.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Please find description of selected tools that were not described in previous years.&lt;/p&gt;
&lt;h3 id="navi"&gt;navi&lt;/h3&gt;
&lt;p&gt;An interactive cheatsheet tool for the command-line.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://asciinema.org/a/406461"&gt;&lt;img alt="Demo" src="https://camo.githubusercontent.com/d3cba877034f1c46a893a248355c1f923ce96906df8bd42c321f68923d09da29/68747470733a2f2f61736369696e656d612e6f72672f612f3430363436312e737667"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;navi&lt;/strong&gt; allows you to browse through cheatsheets (that you may write yourself or download from maintainers) and execute commands. Suggested values for arguments are dynamically displayed in a list.&lt;/p&gt;
&lt;h3 id="shellfirm"&gt;shellfirm&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;shellfirm&lt;/code&gt; will intercept any risky patterns and immediately prompt a small challenge that will double verify your action, think of it as a captcha for your terminal.
&lt;img alt="shellfirm example" src="https://github.com/kaplanelad/shellfirm/raw/main/docs/media/example.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="frameworks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="frameworks"&gt;Frameworks&lt;/h2&gt;
&lt;p&gt;The top-3 Zsh frameworks in the ranking remains the same: Oh My Zsh, prezto and antigen.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ohmyzsh/ohmyzsh"&gt;ohmyzsh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A delightful community-driven (with 2,100+ contributors) framework for managing your zsh configuration.&lt;/td&gt;
&lt;td&gt;154.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sorin-ionescu/prezto"&gt;prezto&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The configuration framework for Zsh&lt;/td&gt;
&lt;td&gt;13.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/antigen"&gt;antigen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The plugin manager for zsh.&lt;/td&gt;
&lt;td&gt;7.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zplug/zplug"&gt;zplug&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A next-generation plugin manager for zsh&lt;/td&gt;
&lt;td&gt;5.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zimfw/zimfw"&gt;zimfw&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zim: Modular, customizable, and blazing fast Zsh framework&lt;/td&gt;
&lt;td&gt;3.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/getantibody/antibody"&gt;antibody&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The fastest shell plugin manager.&lt;/td&gt;
&lt;td&gt;1.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zdharma-continuum/zinit"&gt;zinit&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Flexible and fast ZSH plugin manager&lt;/td&gt;
&lt;td&gt;1.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/tarjoilija/zgen"&gt;zgen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A lightweight and simple plugin manager for ZSH&lt;/td&gt;
&lt;td&gt;1.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Personally, I use zgen, which works well however, last commits to the repo of this projects are from 2018. There is continuation of the heritage of this plugin manager in the project called: &lt;a href="https://github.com/jandamm/zgenom"&gt;zgenom&lt;/a&gt;
&lt;a id="prompts"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="prompts"&gt;Prompts&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/denysdovhan/spaceship-prompt"&gt;spaceship-prompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Minimalistic, powerful and extremely customizable Zsh prompt&lt;/td&gt;
&lt;td&gt;17.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/powerline/powerline"&gt;powerline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Powerline is a statusline plugin for vim, and provides statuslines and prompts for several other applications, including zsh, bash, tmux, IPython, Awesome and Qtile.&lt;/td&gt;
&lt;td&gt;13.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sindresorhus/pure"&gt;pure&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pretty, minimal and fast ZSH prompt&lt;/td&gt;
&lt;td&gt;11.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/JanDeDobbeleer/oh-my-posh"&gt;oh-my-posh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A blazing fast cross platform/shell prompt renderer&lt;/td&gt;
&lt;td&gt;9.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/magicmonty/bash-git-prompt"&gt;bash-git-prompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An informative and fancy bash prompt for Git users&lt;/td&gt;
&lt;td&gt;6.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/b-ryan/powerline-shell"&gt;powerline-shell&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A beautiful and useful prompt for your shell&lt;/td&gt;
&lt;td&gt;6.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/nojhan/liquidprompt"&gt;liquidprompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A full-featured &amp;amp; carefully designed adaptive prompt for Bash &amp;amp; Zsh&lt;/td&gt;
&lt;td&gt;4.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/arialdomartini/oh-my-git"&gt;oh-my-git&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An opinionated git prompt for bash and zsh&lt;/td&gt;
&lt;td&gt;3.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/jonmosco/kube-ps1"&gt;kube-ps1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes prompt info for bash and zsh&lt;/td&gt;
&lt;td&gt;3.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/justjanne/powerline-go"&gt;powerline-go&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A beautiful and useful low-latency prompt for your shell, written in go&lt;/td&gt;
&lt;td&gt;2.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/olivierverdier/zsh-git-prompt"&gt;zsh-git-prompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Informative git prompt for zsh&lt;/td&gt;
&lt;td&gt;1.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/romkatv/gitstatus"&gt;gitstatus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Git status for Bash and Zsh prompt&lt;/td&gt;
&lt;td&gt;1.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/geometry-zsh/geometry"&gt;geometry&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;geometry is a minimal, fully customizable and composable zsh prompt theme&lt;/td&gt;
&lt;td&gt;0.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/reobin/typewritten"&gt;typewritten&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A minimal, lightweight, informative zsh prompt theme&lt;/td&gt;
&lt;td&gt;0.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/tylerreckart/hyperzsh"&gt;hyperzsh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Hyperminimal zsh prompt&lt;/td&gt;
&lt;td&gt;0.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/lyze/posh-git-sh"&gt;posh-git-sh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bash/ZSH version of the posh-git command prompt&lt;/td&gt;
&lt;td&gt;0.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a id="python-tools"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="python-tools"&gt;Python tools&lt;/h2&gt;
&lt;p&gt;Since I work a lot in Python environment - this category will be bit longer than it normally should be.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Simple Python version management&lt;/td&gt;
&lt;td&gt;30.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/jazzband/pip-tools"&gt;pip-tools&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A set of tools to keep your pinned Python dependencies fresh.&lt;/td&gt;
&lt;td&gt;6.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/pypa/pipx"&gt;pipx&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Install and Run Python Applications in Isolated Environments&lt;/td&gt;
&lt;td&gt;5.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/pyenv/pyenv-virtualenv"&gt;pyenv-virtualenv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;a pyenv plugin to manage virtualenv (a.k.a. python-virtualenv)&lt;/td&gt;
&lt;td&gt;5.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/berdario/pew"&gt;pew&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A tool to manage multiple virtual environments written in pure python&lt;/td&gt;
&lt;td&gt;1.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/adambrenecki/virtualfish"&gt;virtualfish&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish shell tool for managing Python virtual environments&lt;/td&gt;
&lt;td&gt;1.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/MichaelAquilina/zsh-autoswitch-virtualenv"&gt;zsh-autoswitch-virtualenv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ZSH plugin to automatically switch python virtualenvs (including pipenv and poetry) as you move between directories&lt;/td&gt;
&lt;td&gt;0.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/iterative/shtab"&gt;shtab&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Automagic shell tab completion for Python CLI applications&lt;/td&gt;
&lt;td&gt;0.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/AndydeCleyre/zpy"&gt;zpy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zsh helpers for Python venvs, with pip-tools&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/gko/project"&gt;project&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Create node, rust, python or ruby project locally and on github (private or public)&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/aperezdc/virtualz"&gt;virtualz&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Virtualfish-alike Python virtualenv wrapper for Zsh&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ctil/zargparse"&gt;zargparse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A tool for generating zsh completion files for python command line tools that use argparse&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/miracleyoo/one-key-linux-setup"&gt;one-key-linux-setup&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Scripts which aims to initialize and configure a linux system quickly. Mainly include update, zsh, oh-my-zsh, zsh plugins, python, and so on. Continue to update and welcome issues.&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/se-jaeger/zsh-activate-py-environment"&gt;zsh-activate-py-environment&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ZSH plugin that automagically detects and activates your python environments (poetry, virtualenv, conda) while traversing directories.&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a id="fuzzy-finders"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="fuzzy-finders"&gt;Fuzzy finders&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/junegunn/fzf"&gt;fzf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A command-line fuzzy finder&lt;/td&gt;
&lt;td&gt;49.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/peco/peco"&gt;peco&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Simplistic interactive filtering tool&lt;/td&gt;
&lt;td&gt;7.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/lotabout/skim"&gt;skim&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fuzzy Finder in rust!&lt;/td&gt;
&lt;td&gt;3.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/mooz/percol"&gt;percol&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;adds flavor of interactive filtering to the traditional pipe concept of UNIX shell&lt;/td&gt;
&lt;td&gt;3.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/jhawthorn/fzy"&gt;fzy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A simple, fast fuzzy finder for the terminal&lt;/td&gt;
&lt;td&gt;2.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/b4b4r07/enhancd"&gt;enhancd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A next-generation cd command with your interactive filter&lt;/td&gt;
&lt;td&gt;2.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/changyuheng/fz"&gt;fz&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Cli shell plugin, the missing fuzzy tab completion feature for the z jump around command.&lt;/td&gt;
&lt;td&gt;0.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note other, less popular tools in that category:
&lt;a href="https://github.com/relastle/pmy"&gt;pmy&lt;/a&gt;, &lt;a href="https://github.com/yuki-yano/zeno.zsh"&gt;zeno.zsh&lt;/a&gt;, &lt;a href="https://github.com/chitoku-k/fzf-zsh-completions"&gt;fzf-zsh-completions&lt;/a&gt;,&lt;a href="https://github.com/mnowotnik/fzshell"&gt;fzshell&lt;/a&gt;,&lt;a href="https://github.com/aperezdc/zsh-fzy"&gt;zsh-fzy&lt;/a&gt;,&lt;a href="https://github.com/MichaelAquilina/zsh-history-filter"&gt;zsh-history-filter&lt;/a&gt;,&lt;a href="https://github.com/smeagol74/zsh-fzf-pass"&gt;zsh-fzf-pass&lt;/a&gt;,&lt;a href="https://github.com/aubreypwd/zsh-plugin-fd"&gt;zsh-plugin-fd&lt;/a&gt;,&lt;a href="https://github.com/spodin/zsh-fuzzy-wd"&gt;zsh-fuzzy-wd&lt;/a&gt;, &lt;a href="https://github.com/Ryooooooga/qwy"&gt;qwy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="dockercontainers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="dockercontainers"&gt;Docker/Containers&lt;/h2&gt;
&lt;p&gt;Not mentioned in the ranking below but worth checking is &lt;a href="https://github.com/derailed/k9s"&gt;k9s&lt;/a&gt; (19.4k stars).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ahmetb/kubectx"&gt;kubectx&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Faster way to switch between clusters and namespaces in kubectl&lt;/td&gt;
&lt;td&gt;14.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/bcicen/ctop"&gt;ctop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Top-like interface for container metrics&lt;/td&gt;
&lt;td&gt;13.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/deluan/zsh-in-docker"&gt;zsh-in-docker&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Install Zsh, Oh-My-Zsh and plugins inside a Docker container with one line!&lt;/td&gt;
&lt;td&gt;0.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/chitoku-k/fzf-zsh-completions"&gt;fzf-zsh-completions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fuzzy completions for fzf and Zsh (git, kubectl, docker, ...)&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/greymd/docker-zsh-completion"&gt;docker-zsh-completion&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zsh completion for docker and docker-compose.&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sroze/docker-compose-zsh-plugin"&gt;docker-compose-zsh-plugin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ZSH plugin that display status of project containers&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="k9s"&gt;K9s&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/derailed/k9s"&gt;K9s&lt;/a&gt; provides a curses based terminal UI to interact with your Kubernetes clusters. The aim of this project is to make it easier to navigate, observe and manage your applications in the wild. K9s continually watches Kubernetes for changes and offers subsequent commands to interact with observed resources.
&lt;img alt="k9s" src="https://golangexample.com/content/images/2019/02/screen_po.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Linux"/><category term="zsh"/><category term="scrapping"/><category term="python"/><category term="Linux"/></entry><entry><title>Becoming a Data Wizard - The Benefits of Learning Databricks</title><link href="http://127.0.0.1:8000/why-to-learn-databricks/" rel="alternate"/><published>2023-01-30T00:00:00+01:00</published><updated>2023-01-30T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-30:/why-to-learn-databricks/</id><summary type="html">&lt;p&gt;Learn how Databricks can help you master big data, improve data processing and machine learning skills and excel in your career. Boost your career with this powerful platform.&lt;/p&gt;</summary><content type="html">&lt;p&gt;up::[[MOC_Databricks]]&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data is becoming an increasingly important part of our world, and as such, the ability to work with and understand data is becoming a valuable skill. One tool that can help you develop this skill is Databricks, a powerful platform for working with big data.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-advantages-of-databricks"&gt;The Advantages of Databricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#handling-large-amounts-of-data"&gt;Handling Large Amounts of Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#integration-with-other-data-tools"&gt;Integration with Other Data Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#built-in-libraries-for-data-processing-and-machine-learning"&gt;Built-in Libraries for Data Processing and Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-convenience-of-a-cloud-based-platform"&gt;The Convenience of a Cloud-based Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-overall-benefits-of-learning-databricks"&gt;The Overall Benefits of Learning Databricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;
&lt;p&gt;&lt;a id="the-advantages-of-databricks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-advantages-of-databricks"&gt;The Advantages of Databricks&lt;/h2&gt;
&lt;p&gt;Databricks is an integrated platform for data engineering, machine learning, and analytics that is built on top of Apache Spark, a popular open-source big data processing framework. It provides a number of advantages over other big data tools, including a powerful and easy-to-use interface, a wide range of built-in data processing and machine learning libraries, and integration with other popular data tools.&lt;/p&gt;
&lt;p&gt;&lt;a id="handling-large-amounts-of-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="handling-large-amounts-of-data"&gt;Handling Large Amounts of Data&lt;/h3&gt;
&lt;p&gt;One of the main advantages of Databricks is its ability to handle large amounts of data. Whether you're working with structured data in a relational database or unstructured data in a data lake, Databricks can help you process and analyze it quickly and efficiently. This is particularly useful for tasks such as data cleaning, feature engineering, and model training, which can be time-consuming and resource-intensive when done manually.&lt;/p&gt;
&lt;p&gt;&lt;a id="integration-with-other-data-tools"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="integration-with-other-data-tools"&gt;Integration with Other Data Tools&lt;/h3&gt;
&lt;p&gt;Another advantage of Databricks is its ability to integrate with other data tools. For example, you can easily connect to data sources such as Amazon S3, Azure Data Lake Storage, and Google Cloud Storage, and you can also use Databricks in conjunction with other data tools such as Apache Hive, Apache Kafka, and Apache Delta Lake. This makes it easy to build data pipelines and workflows that take advantage of the strengths of different tools.&lt;/p&gt;
&lt;p&gt;&lt;a id="built-in-libraries-for-data-processing-and-machine-learning"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="built-in-libraries-for-data-processing-and-machine-learning"&gt;Built-in Libraries for Data Processing and Machine Learning&lt;/h3&gt;
&lt;p&gt;Databricks also provides a wide range of built-in libraries for data processing and machine learning. These libraries, such as MLlib, GraphX and SQL Analytics, allow you to perform tasks such as data visualization, natural language processing, and machine learning without having to write complex code. This makes it easy to get started working with data and develop your skills.&lt;/p&gt;
&lt;p&gt;&lt;a id="the-convenience-of-a-cloud-based-platform"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="the-convenience-of-a-cloud-based-platform"&gt;The Convenience of a Cloud-based Platform&lt;/h3&gt;
&lt;p&gt;Finally, Databricks is a cloud-based platform, which means that you don't have to worry about setting up and maintaining your own infrastructure. This can save you time and money, and also allows you to scale your resources up or down as needed.&lt;/p&gt;
&lt;p&gt;&lt;a id="the-overall-benefits-of-learning-databricks"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-overall-benefits-of-learning-databricks"&gt;The Overall Benefits of Learning Databricks&lt;/h2&gt;
&lt;p&gt;Overall, learning Databricks can help your career in many ways. It can help you become more proficient at working with big data, which is a valuable skill in today's job market. It can also help you become more efficient at data processing and machine learning, which can lead to greater productivity and better results. And, by providing a cloud-based, integrated platform for data engineering, machine learning and analytics, it can help you work more effectively with other data tools and technologies.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="databricks"/><category term="data-engineering"/></entry><entry><title>Policies in MLOps</title><link href="http://127.0.0.1:8000/mlops-policies/" rel="alternate"/><published>2023-01-25T00:00:00+01:00</published><updated>2023-01-25T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-25:/mlops-policies/</id><summary type="html">&lt;p&gt;Discover the secrets to successful MLOps - From planning to deployment, get a comprehensive guide to ML policies.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-data-governance-policy"&gt;1.  Data Governance Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-model-development-standards"&gt;2.  Model Development Standards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-model-deployment-and-maintenance-guidelines"&gt;3.  Model Deployment and Maintenance Guidelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-model-retraining-and-lifecycle-management-policy"&gt;4.  Model Retraining and Lifecycle Management Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-infrastructure-and-tooling-guidelines"&gt;5.  Infrastructure and Tooling Guidelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-compliance-and-auditing-protocols"&gt;6.  Compliance and Auditing Protocols&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-team-roles-and-responsibilities"&gt;7.  Team Roles and Responsibilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-communication-and-collaboration-protocols"&gt;8.  Communication and Collaboration Protocols&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#we-have-all-policies-and-guidelines-ready---whats-next"&gt;We have all policies and guidelines ready - what's next?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-are-some-factors-that-can-block-or-slow-down-mlops-incubation-in-organization"&gt;What are some factors that can block or slow-down MLOps incubation in organization?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#blocking-factors"&gt;Blocking factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#actions-to-avoid-or-overcome-blockers"&gt;Actions to avoid or overcome blockers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#be-prepared-on-what-can-go-wrong"&gt;Be prepared on what can go wrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Machine Learning Operations (MLOps) is a rapidly growing field that aims to bridge the gap between data science and IT operations. It involves the use of tools and practices to automate, streamline, and optimize the deployment of machine learning models in production. By standardizing practices, organizations can improve the speed, quality, and reliability of their machine learning models while also reducing costs and risks.&lt;/p&gt;
&lt;p&gt;One of the key components of introducing MLOps is the development of policies, guidelines, and good-practices. These documents help to ensure that everyone involved in the process understands the rules and procedures for managing, storing, and protecting data, developing and deploying models, and maintaining and updating models over time.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MLOps Policies and Guidelines" src="/images/mlops_policies/mlops_policies.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. MLOps policies and guidelines.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="1-data-governance-policy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-data-governance-policy"&gt;1.  Data Governance Policy&lt;/h3&gt;
&lt;p&gt;The first step in introducing MLOps is to establish a data governance policy. This document outlines the rules and procedures for managing, storing, and protecting data used in machine learning models. It should cover topics such as data quality, data privacy, and data security. For example, it should specify the types of data that can be used for training models, the procedures for handling sensitive data, and the protocols for protecting data from unauthorized access or breaches.&lt;/p&gt;
&lt;p&gt;A Data Governance Policy should cover several key topics to ensure that data used in machine learning models is managed, stored, and protected in a consistent and reliable manner. Some of the key topics that should be covered in a Data Governance Policy include:&lt;/p&gt;
&lt;h4 id="1-data-quality"&gt;1.  Data Quality&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for ensuring that data is accurate, complete, and relevant for the intended purpose. It should also specify the types of data that can be used for training models and the procedures for handling missing or incorrect data.&lt;/p&gt;
&lt;h4 id="2-data-privacy"&gt;2.  Data Privacy&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for protecting sensitive data, such as personal information, from unauthorized access or breaches. It should also specify the types of data that should be encrypted and the protocols for handling data in compliance with relevant regulations and laws.&lt;/p&gt;
&lt;h4 id="3-data-security"&gt;3.  Data Security&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for protecting data from unauthorized access or breaches. It should specify the types of data that should be stored in secure locations, the protocols for controlling access to data, and the procedures for responding to security incidents.&lt;/p&gt;
&lt;h4 id="4-data-access-and-use"&gt;4.  Data Access and Use&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for controlling access to data, including who is authorized to access data and the procedures for granting and revoking access. It should also specify the protocols for handling data requests and the procedures for auditing data access and use.&lt;/p&gt;
&lt;h4 id="5-data-retention-and-disposal"&gt;5.  Data Retention and Disposal&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for managing the retention and disposal of data, including the retention periods for different types of data and the procedures for securely disposing of data that is no longer needed.&lt;/p&gt;
&lt;p&gt;To prepare a complete Data Governance Policy, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What types of data will be used for training models and what is the source of that data?&lt;/li&gt;
&lt;li&gt;How will data be stored and protected from unauthorized access or breaches?&lt;/li&gt;
&lt;li&gt;How will data quality be ensured and what are the procedures for handling missing or incorrect data?&lt;/li&gt;
&lt;li&gt;How will data privacy be protected and what are the procedures for handling sensitive data?&lt;/li&gt;
&lt;li&gt;Who will have access to data and what are the procedures for granting and revoking access?&lt;/li&gt;
&lt;li&gt;How will data retention and disposal be managed and what are the procedures for securely disposing of data that is no longer needed?&lt;/li&gt;
&lt;li&gt;Are there any compliance requirements that must be met when handling data and how will compliance be ensured?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Data Governance Policy covers all the necessary topics and provides clear guidelines for managing, storing, and protecting data used in machine learning models.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-model-development-standards"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-model-development-standards"&gt;2.  Model Development Standards&lt;/h3&gt;
&lt;p&gt;Another critical element of MLOps is the development of model development standards. This document lays out the guidelines for developing and implementing ML models, including coding standards, testing protocols, and documentation requirements. For example, it should specify the programming languages and frameworks that should be used, the types of tests that should be run, and the level of documentation that should be provided. By establishing clear standards, organizations can ensure that models are developed in a consistent and reliable manner.&lt;/p&gt;
&lt;p&gt;Model Development Standards should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-coding-standards"&gt;1.  Coding Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the programming languages and frameworks that should be used for developing models, as well as the conventions and best practices for writing code.&lt;/p&gt;
&lt;h4 id="2-testing-standards"&gt;2.  Testing Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the types of tests that should be run on models, including unit tests, integration tests, and performance tests. It should also specify the procedures for testing and the criteria for acceptance.&lt;/p&gt;
&lt;h4 id="3-documentation-standards"&gt;3.  Documentation Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the level of documentation that should be provided for each model, including technical documentation, user documentation, and training materials.&lt;/p&gt;
&lt;h4 id="4-version-control-standards"&gt;4.  Version Control Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for versioning models, including the use of version control systems, the conventions for naming versions, and the procedures for branching and merging.&lt;/p&gt;
&lt;h4 id="5-model-validation-standards"&gt;5. Model Validation Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for validating models, including the use of cross-validation techniques, the procedures for evaluating model performance, and the criteria for acceptance.&lt;/p&gt;
&lt;p&gt;To prepare a complete Model Development Standards, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What programming languages and frameworks will be used for developing models?&lt;/li&gt;
&lt;li&gt;What are the coding conventions and best practices for writing code?&lt;/li&gt;
&lt;li&gt;What types of tests will be run on models and what are the acceptance criteria?&lt;/li&gt;
&lt;li&gt;What level of documentation is required for each model?&lt;/li&gt;
&lt;li&gt;How will models be versioned and what are the conventions for naming versions?&lt;/li&gt;
&lt;li&gt;How will models be validated and what are the acceptance criteria?&lt;/li&gt;
&lt;li&gt;Are there any compliance requirements that must be met when developing models and how will compliance be ensured?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Model Development Standards cover all the necessary topics and provide clear guidelines for developing and implementing machine learning models in a consistent and reliable manner.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-model-deployment-and-maintenance-guidelines"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-model-deployment-and-maintenance-guidelines"&gt;3.  Model Deployment and Maintenance Guidelines&lt;/h3&gt;
&lt;p&gt;Once models are developed, they need to be deployed and maintained in production. This is where model deployment and maintenance guidelines come in. This document provides instructions for deploying and maintaining ML models in production, including versioning, monitoring, and troubleshooting. It should also specify the procedures for rolling back models in case of issues or errors. By providing clear guidelines for deploying and maintaining models, organizations can ensure that models are deployed and maintained in a consistent and reliable manner.&lt;/p&gt;
&lt;p&gt;Model Deployment and Maintenance Guidelines should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-deployment-standards"&gt;1.  Deployment Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for deploying models to production, including the use of containerization and orchestration tools, the procedures for testing and validating models in production, and the procedures for rolling back models in case of issues or errors.&lt;/p&gt;
&lt;h4 id="2-monitoring-and-maintenance-standards"&gt;2.  Monitoring and Maintenance Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for monitoring models in production, including the use of monitoring and logging tools, the procedures for troubleshooting and resolving issues, and the procedures for updating and retraining models over time.&lt;/p&gt;
&lt;h4 id="3-version-control-and-rollback"&gt;3.  Version Control and Rollback&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for versioning models and the protocols for rolling back models in case of issues or errors. It should also specify the procedures for updating and retraining models over time.&lt;/p&gt;
&lt;h4 id="4-security-and-compliance"&gt;4.  Security and Compliance&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for ensuring security and compliance of deployed models, including the use of encryption and access controls, and the protocols for handling data in compliance with relevant regulations and laws.&lt;/p&gt;
&lt;p&gt;To prepare a complete Model Deployment and Maintenance Guidelines, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How will models be deployed to production and what are the procedures for testing and validating models in production?&lt;/li&gt;
&lt;li&gt;How will models be monitored and maintained in production, including troubleshooting and resolving issues?&lt;/li&gt;
&lt;li&gt;How will models be versioned, updated and retrained over time?&lt;/li&gt;
&lt;li&gt;How will rollback be handled in case of issues or errors?&lt;/li&gt;
&lt;li&gt;How will security and compliance be ensured for deployed models?&lt;/li&gt;
&lt;li&gt;Are there any compliance requirements that must be met when deploying and maintaining models and how will compliance be ensured?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Model Deployment and Maintenance Guidelines cover all the necessary topics and provide clear guidelines for deploying and maintaining machine learning models in a consistent and reliable manner.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-model-retraining-and-lifecycle-management-policy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-model-retraining-and-lifecycle-management-policy"&gt;4.  Model Retraining and Lifecycle Management Policy&lt;/h3&gt;
&lt;p&gt;Machine learning models are not static and will require regular updates and retraining to ensure their continued accuracy and performance. Model retraining and lifecycle management policy provides the procedures for regularly retraining and updating ML models. It should also specify the conditions under which models should be retired or deprecated. By having a clear policy for retraining and managing the lifecycle of models, organizations can ensure that models are kept up to date and continue to provide accurate results.&lt;/p&gt;
&lt;p&gt;Model Retraining and Lifecycle Management Policy should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-retraining-schedules"&gt;1.  Retraining Schedules&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for regularly retraining models, including the frequency of retraining and the procedures for updating models with new data.&lt;/p&gt;
&lt;h4 id="2-model-performance-monitoring"&gt;2.  Model Performance Monitoring&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for monitoring the performance of models over time, including the use of metrics and evaluation techniques, and the criteria for determining when a model needs to be retrained or replaced.&lt;/p&gt;
&lt;h4 id="3-model-replacement-criteria"&gt;3.  Model Replacement Criteria&lt;/h4&gt;
&lt;p&gt;This section should outline the criteria for determining when a model needs to be retired or replaced, including the use of performance metrics and evaluation techniques, and the procedures for transitioning to a new model.&lt;/p&gt;
&lt;h4 id="4-archival-and-retirement"&gt;4.  Archival and Retirement&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for archiving and retiring models, including the retention periods for different types of models and the procedures for securely disposing of models that are no longer needed.&lt;/p&gt;
&lt;p&gt;To prepare a complete Model Retraining and Lifecycle Management Policy, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How often will models be retrained and what are the procedures for updating models with new data?&lt;/li&gt;
&lt;li&gt;How will model performance be monitored over time and what are the criteria for determining when a model needs to be retrained or replaced?&lt;/li&gt;
&lt;li&gt;What are the criteria for determining when a model needs to be retired or replaced and what are the procedures for transitioning to a new model?&lt;/li&gt;
&lt;li&gt;How will models be archived and retired and what are the retention periods and procedures for securely disposing of models that are no longer needed?&lt;/li&gt;
&lt;li&gt;Are there any compliance requirements that must be met when retraining and managing the lifecycle of models and how will compliance be ensured?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Model Retraining and Lifecycle Management Policy covers all the necessary topics and provides clear guidelines for regularly retraining and updating machine learning models to ensure their continued accuracy and performance.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-infrastructure-and-tooling-guidelines"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-infrastructure-and-tooling-guidelines"&gt;5.  Infrastructure and Tooling Guidelines&lt;/h3&gt;
&lt;p&gt;One of the most important aspects of MLOps is the use of appropriate infrastructure and tools. This includes cloud environments, data storage solutions, and CI/CD pipelines. Infrastructure and Tooling guidelines provide guidelines for the infrastructure and tools used in MLOps. It should specify the types of cloud environments and data storage solutions that should be used, as well as the CI/CD pipelines that should be implemented. By providing clear guidelines for infrastructure and tooling, organizations can ensure that models are deployed and maintained in a consistent and reliable manner.&lt;/p&gt;
&lt;p&gt;Infrastructure and Tooling Guidelines should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-cloud-environments"&gt;1.  Cloud Environments&lt;/h4&gt;
&lt;p&gt;This section should outline the types of cloud environments that should be used for developing, deploying, and maintaining models, including the use of public cloud, private cloud, or hybrid cloud solutions.&lt;/p&gt;
&lt;h4 id="2-data-storage-solutions"&gt;2.  Data Storage Solutions&lt;/h4&gt;
&lt;p&gt;This section should outline the types of data storage solutions that should be used, including the use of relational databases, NoSQL databases, and data lakes.&lt;/p&gt;
&lt;h4 id="3-cicd-pipelines"&gt;3.  CI/CD Pipelines&lt;/h4&gt;
&lt;p&gt;This section should outline the use of continuous integration and continuous delivery (CI/CD) pipelines for automating the development, testing, and deployment of models.&lt;/p&gt;
&lt;h4 id="4-monitoring-and-logging"&gt;4.  Monitoring and Logging&lt;/h4&gt;
&lt;p&gt;This section should outline the use of monitoring and logging tools for tracking and troubleshooting issues in the development, deployment, and maintenance of models.&lt;/p&gt;
&lt;h4 id="5-security-and-compliance"&gt;5.  Security and Compliance&lt;/h4&gt;
&lt;p&gt;This section should outline the use of security and compliance tools, such as encryption and access controls, for ensuring the security and compliance of the infrastructure and tools used in MLOps.&lt;/p&gt;
&lt;p&gt;To prepare a complete Infrastructure and Tooling Guidelines, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What types of cloud environments will be used for developing, deploying, and maintaining models?&lt;/li&gt;
&lt;li&gt;What types of data storage solutions will be used and how will data be stored and protected?&lt;/li&gt;
&lt;li&gt;How will the development, testing, and deployment of models be automated and what are the procedures for automating these processes?&lt;/li&gt;
&lt;li&gt;How will issues be tracked and troubleshot in the development, deployment, and maintenance of models?&lt;/li&gt;
&lt;li&gt;How will security and compliance be ensured for the infrastructure and tools used in MLOps?&lt;/li&gt;
&lt;li&gt;Are there any compliance requirements that must be met when using the infrastructure and tools and how will compliance be ensured?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Infrastructure and Tooling Guidelines cover all the necessary topics and provide clear guidelines for using the appropriate infrastructure and tools for developing, deploying, and maintaining machine learning models in a consistent and reliable manner.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-compliance-and-auditing-protocols"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-compliance-and-auditing-protocols"&gt;6.  Compliance and Auditing Protocols&lt;/h3&gt;
&lt;p&gt;As machine learning models are increasingly used in critical applications, it is important to ensure that they are compliant with relevant regulations and laws. Compliance and Auditing protocols outline the procedures for ensuring compliance with relevant regulations and laws, as well as for conducting regular audits of ML models and processes. This can include regular reviews of models for bias and fairness, as well as procedures for handling any issues that are identified.&lt;/p&gt;
&lt;p&gt;Compliance and Auditing Protocols should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-compliance-standards"&gt;1.  Compliance Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for ensuring compliance with relevant regulations and laws, such as the General Data Protection Regulation (GDPR) or the Health Insurance Portability and Accountability Act (HIPAA), as well as industry-specific regulations.&lt;/p&gt;
&lt;h4 id="2-auditing-standards"&gt;2.  Auditing Standards&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for conducting regular audits of machine learning models and processes, including the use of audit logs, the types of audits that should be conducted, and the criteria for determining compliance.&lt;/p&gt;
&lt;h4 id="3-risk-management"&gt;3.  Risk Management&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for identifying and managing risks associated with machine learning models, including the use of risk assessment and management tools, and the procedures for mitigating risks.&lt;/p&gt;
&lt;h4 id="4-incident-response"&gt;4.  Incident Response&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for responding to incidents, such as data breaches, including the use of incident response plans and the procedures for reporting incidents.&lt;/p&gt;
&lt;p&gt;To prepare a complete Compliance and Auditing Protocols, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the relevant regulations and laws that must be complied with and what are the procedures for ensuring compliance?&lt;/li&gt;
&lt;li&gt;How will regular audits be conducted and what are the criteria for determining compliance?&lt;/li&gt;
&lt;li&gt;How will risks associated with machine learning models be identified and managed and what are the procedures for mitigating risks?&lt;/li&gt;
&lt;li&gt;How will incidents be handled, such as data breaches, and what are the procedures for reporting incidents?&lt;/li&gt;
&lt;li&gt;Are there any specific industry-specific regulations that must be followed and how will compliance be ensured?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Compliance and Auditing Protocols cover all the necessary topics and provide clear guidelines for ensuring compliance and conducting regular audits of machine learning models and processes.&lt;/p&gt;
&lt;p&gt;&lt;a id="7-team-roles-and-responsibilities"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="7-team-roles-and-responsibilities"&gt;7.  Team Roles and Responsibilities&lt;/h3&gt;
&lt;p&gt;Another important aspect of MLOps is defining the roles and responsibilities of the various teams involved in the process. This includes data scientists, engineers, and IT/DevOps teams. Team Roles and Responsibilities document defines the roles and responsibilities of each team, outlining who is responsible for developing models, deploying models, and maintaining models over time. By clearly defining roles and responsibilities, organizations can ensure that everyone is working together effectively and efficiently.&lt;/p&gt;
&lt;p&gt;Team Roles and Responsibilities document is an important part of MLOps, as it helps to ensure that everyone involved in the process of developing, deploying, and maintaining machine learning models understands their roles and responsibilities. Team Roles and Responsibilities document should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-team-structure"&gt;1.  Team Structure&lt;/h4&gt;
&lt;p&gt;This section should outline the structure of the MLOps team and the different roles and responsibilities within the team.&lt;/p&gt;
&lt;h4 id="2-roles-and-responsibilities"&gt;2.  Roles and Responsibilities&lt;/h4&gt;
&lt;p&gt;This section should outline the specific roles and responsibilities of each team member, including data scientists, engineers, and IT/DevOps teams.&lt;/p&gt;
&lt;h4 id="3-communication-and-collaboration"&gt;3.  Communication and Collaboration&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for communication and collaboration within the team, including regular meetings and reporting requirements.&lt;/p&gt;
&lt;h4 id="4-training-and-development"&gt;4.  Training and Development&lt;/h4&gt;
&lt;p&gt;This section should outline the procedures for training and development of team members, including the types of training that should be provided and the procedures for ensuring that team members are up-to-date with the latest technologies and best practices.&lt;/p&gt;
&lt;p&gt;To prepare a complete Team Roles and Responsibilities document, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the structure of the MLOps team and what are the different roles and responsibilities within the team?&lt;/li&gt;
&lt;li&gt;What are the specific roles and responsibilities of each team member?&lt;/li&gt;
&lt;li&gt;How will team members communicate and collaborate with each other?&lt;/li&gt;
&lt;li&gt;What are the procedures for training and development of team members?&lt;/li&gt;
&lt;li&gt;How will team members stay up-to-date with the latest technologies and best practices in MLOps?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Team Roles and Responsibilities document covers all the necessary topics and provides clear guidelines for the roles and responsibilities of each team member in the process of developing, deploying, and maintaining machine learning models.&lt;/p&gt;
&lt;p&gt;&lt;a id="8-communication-and-collaboration-protocols"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="8-communication-and-collaboration-protocols"&gt;8.  Communication and Collaboration Protocols&lt;/h3&gt;
&lt;p&gt;Finally, effective communication and collaboration are essential for the success of MLOps. Communication and Collaboration protocols outline the procedures for communication and collaboration within the MLOps team, including regular meetings and reporting requirements. By establishing clear protocols for communication and collaboration, organizations can ensure that everyone is working together effectively and efficiently.&lt;/p&gt;
&lt;p&gt;Communication and Collaboration Protocols document should cover several key topics, including:&lt;/p&gt;
&lt;h4 id="1-communication-channels"&gt;1.  Communication Channels&lt;/h4&gt;
&lt;p&gt;This section should outline the different communication channels that should be used for different types of communications, such as email, instant messaging, video conferencing, and project management tools.&lt;/p&gt;
&lt;h4 id="2-communication-frequencies"&gt;2.  Communication Frequencies&lt;/h4&gt;
&lt;p&gt;This section should outline the communication frequencies, such as regular meetings, status updates, and reporting requirements.&lt;/p&gt;
&lt;h4 id="3-collaboration-tools"&gt;3.  Collaboration Tools&lt;/h4&gt;
&lt;p&gt;This section should outline the different collaboration tools that should be used for different types of collaborations, such as version control systems, issue tracking systems, and document management systems.&lt;/p&gt;
&lt;h4 id="4-decision-making-process"&gt;4.  Decision-making process&lt;/h4&gt;
&lt;p&gt;This section should outline the process for making decisions, such as who is involved in decision-making, how decisions are made, and how conflicts are resolved.&lt;/p&gt;
&lt;p&gt;To prepare a complete Communication and Collaboration Protocols document, organizations can ask themselves the following key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the different communication channels that should be used for different types of communications?&lt;/li&gt;
&lt;li&gt;How often should team members communicate with each other, such as regular meetings, status updates, and reporting requirements?&lt;/li&gt;
&lt;li&gt;What are the different collaboration tools that should be used for different types of collaborations?&lt;/li&gt;
&lt;li&gt;How will decisions be made and how conflicts will be resolved?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By answering these questions, organizations can ensure that their Communication and Collaboration Protocols document covers all the necessary topics and provides clear guidelines for effective communication and collaboration within the MLOps team.&lt;/p&gt;
&lt;p&gt;&lt;a id="we-have-all-policies-and-guidelines-ready---whats-next"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="steps-for-introducing-mlops-in-an-organization"&gt;Steps for introducing MLOps in an organization&lt;/h2&gt;
&lt;p&gt;We have all policies and guidelines ready - what's next?
Once all the policies and guidelines for MLOps are ready, the next steps for introducing MLOps in an organization can include:&lt;/p&gt;
&lt;h4 id="1-communication-and-training"&gt;1.  Communication and Training&lt;/h4&gt;
&lt;p&gt;Communicate the policies and guidelines to all relevant stakeholders, including data scientists, engineers, IT/DevOps teams, and management. Provide training to ensure that everyone understands their roles and responsibilities and is able to use the new tools and processes effectively.&lt;/p&gt;
&lt;h4 id="2-implementation"&gt;2.  Implementation&lt;/h4&gt;
&lt;p&gt;Begin implementing the policies and guidelines by setting up the infrastructure and tools, such as cloud environments, data storage solutions, and CI/CD pipelines.&lt;/p&gt;
&lt;h4 id="3-pilot-project"&gt;3.  Pilot Project&lt;/h4&gt;
&lt;p&gt;Start with a small pilot project to test the new MLOps processes and identify any issues or areas for improvement. Use the lessons learned from the pilot project to make adjustments and fine-tune the processes.&lt;/p&gt;
&lt;h4 id="4-scale-up"&gt;4.  Scale Up&lt;/h4&gt;
&lt;p&gt;Once the pilot project is successful, begin scaling up the MLOps processes and tools to the rest of the organization.&lt;/p&gt;
&lt;h4 id="5-continuous-improvement"&gt;5.  Continuous Improvement&lt;/h4&gt;
&lt;p&gt;Regularly review and update the policies and guidelines based on feedback and lessons learned. Continuously monitor and improve the MLOps processes to ensure they are efficient and effective.&lt;/p&gt;
&lt;h4 id="6-auditing-and-compliance"&gt;6.  Auditing and Compliance&lt;/h4&gt;
&lt;p&gt;Regularly conduct audits to ensure that the organization is following the policies and guidelines and is compliant with relevant regulations and laws.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Process for introducying MLOps" src="/images/mlops_policies/introducying_mlops.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2. Process for introducing MLOps when policies are ready&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is also important to establish a governance structure for MLOps, such as an MLOps team, who will be responsible for the overall management and execution of the MLOps process. This team should have a clear leadership, roles and responsibilities and should be accountable for creating and maintaining the MLOps policies and guidelines, as well as implementing and monitoring the MLOps processes.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-are-some-factors-that-can-block-or-slow-down-mlops-incubation-in-organization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-are-some-factors-that-can-block-or-slow-down-mlops-incubation-in-organization"&gt;What are some factors that can block or slow-down MLOps incubation in organization?&lt;/h2&gt;
&lt;p&gt;&lt;a id="blocking-factors"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="blocking-factors"&gt;Blocking factors&lt;/h3&gt;
&lt;p&gt;When introducing MLOps policies and guidelines in an organization that was operating without MLOps earlier, there are several potential problems that may be encountered, such as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Resistance to Change:&lt;/strong&gt; Introducing new policies and guidelines can be met with resistance from employees who are used to the legacy practices and may be hesitant to change the way they work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of Understanding:&lt;/strong&gt; Employees may not fully understand the new policies and guidelines or how they fit into the overall MLOps process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Technical Challenges:&lt;/strong&gt; The organization may not have the necessary infrastructure or tools in place to support the new MLOps processes, and there may be technical challenges in implementing the new policies and guidelines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficulty in Measuring Success:&lt;/strong&gt; It may be difficult to measure the success of the new MLOps processes, as there may not be established metrics or benchmarks in place.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Legacy systems and processes:&lt;/strong&gt; The organization may have legacy systems and processes in place that are not compatible with the new MLOps processes, which could lead to delays and inefficiencies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficulty in changing established culture:&lt;/strong&gt; If the organization has a culture that is not conducive to MLOps, it may be difficult to introduce the new policies and guidelines and to get employees to adopt the new processes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of resources:&lt;/strong&gt; The organization may not have the resources or expertise to implement the new policies and guidelines, which could lead to delays and inefficiencies.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="actions-to-avoid-or-overcome-blockers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="actions-to-avoid-or-overcome-blockers"&gt;Actions to avoid or overcome blockers&lt;/h3&gt;
&lt;p&gt;To overcome these problems, potential resistance, some strategies that can be used include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Communication and Training:&lt;/strong&gt; Clearly communicate the reasons for introducing the new policies and guidelines, and provide training to ensure that employees understand their roles and responsibilities and how the new processes will benefit the organization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Involvement and Empowerment:&lt;/strong&gt; Involve employees in the process of creating the new policies and guidelines, and empower them to take ownership of the new processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pilot Project:&lt;/strong&gt; Start with a small pilot project to test the new MLOps processes and to get buy-in from employees.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Change Management:&lt;/strong&gt; Use change management techniques to manage the transition to the new processes and to address any issues that arise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Continuous Improvement:&lt;/strong&gt; Regularly review and update the policies and guidelines based on feedback and lessons learned, and continuously monitor and improve the MLOps processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metrics and Benchmarking:&lt;/strong&gt; Establish metrics and benchmarks to measure the success of introducing MLOps.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="be-prepared-on-what-can-go-wrong"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="be-prepared-on-what-can-go-wrong"&gt;Be prepared on what can go wrong&lt;/h2&gt;
&lt;p&gt;When introducing MLOps policies and guidelines, there are several potential problems that may arise, such as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of buy-in:&lt;/strong&gt; Employees may not fully understand the value or need for the new policies and guidelines, and may be resistant to change. This could lead to a lack of buy-in and difficulty in getting employees to adopt the new processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implementation challenges:&lt;/strong&gt; The organization may not have the necessary infrastructure or tools in place to support the new MLOps processes, and there may be technical challenges in implementing the new policies and guidelines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Difficulty in measuring success:&lt;/strong&gt; It may be difficult to measure the success of the new MLOps processes, as there may not be established metrics or benchmarks in place.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of governance:&lt;/strong&gt; If the organization does not establish a governance structure for MLOps, such as an MLOps team, it can be difficult to ensure that the policies and guidelines are being followed, and that the MLOps processes are being executed effectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of Communication:&lt;/strong&gt; If there is a lack of communication between different teams, stakeholders, and departments it can lead to confusion, delays and problems with the implementation of the new policies and guidelines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compliance issues:&lt;/strong&gt; If the organization does not have a clear understanding of the compliance requirements for MLOps, it can lead to compliance issues and potential legal and financial consequences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Difficulty in adapting to new tools and technologies:&lt;/strong&gt; The organization may have difficulty in adapting to new tools and technologies that are required for the implementation of MLOps, which could lead to delays and inefficiencies.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Introducing MLOps involves developing a set of policies, guidelines, and good-practices that help to ensure the speed, quality, and reliability of machine learning models. By standardizing practices, organizations can improve the performance of their models, reduce costs and risks, and ensure compliance with relevant regulations and laws. With the right policies, guidelines, and good-practices in place, organizations can successfully implement MLOps and realize the full potential of machine learning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/></entry><entry><title>MLOps Scorecard - How Advanced Is Your Organization in Implementing MLOps Processes?</title><link href="http://127.0.0.1:8000/mlops-scorecard/" rel="alternate"/><published>2023-01-21T00:00:00+01:00</published><updated>2023-01-26T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-21:/mlops-scorecard/</id><summary type="html">&lt;p&gt;Use proposed scorecard to assess how advanced is your organization in implementing MLOps processes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this article I propose a sample MLOps scorecard that organizations can use to assess their current level of MLOps implementation. All what's needed is to count score (0-100) in each of the 6 layers of introducing  MLOps practices:&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3,4,5" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#analyze-organization-along-6-dimensions"&gt;Analyze organization along 6 dimensions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-governance"&gt;1.  Governance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-data-management"&gt;2.  Data Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-model-development"&gt;3.  Model Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-deployment"&gt;4.  Deployment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-monitoring-and-evaluation"&gt;5.  Monitoring and Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-continuous-improvement"&gt;6.  Continuous Improvement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visualize-it"&gt;Visualize it!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#limitations-that-you-should-be-aware-of"&gt;Limitations that you should be aware of&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#factors-limiting-potential-utility-of-such-analysis"&gt;Factors limiting potential utility of such analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-subjectivity"&gt;1.  Subjectivity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-lack-of-context"&gt;2.  Lack of context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-limited-scope"&gt;3.  Limited scope&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-limited-flexibility"&gt;4.  Limited flexibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-limited-actionability"&gt;5.  Limited actionability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#hints-for-overcoming-limitations"&gt;Hints for overcoming limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="analyze-organization-along-6-dimensions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="analyze-organization-along-6-dimensions"&gt;Analyze organization along 6 dimensions&lt;/h2&gt;
&lt;p&gt;&lt;a id="1-governance"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-governance"&gt;1.  Governance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ ]   Does the organization have a clear ML strategy and governance structure in place?&lt;/li&gt;
&lt;li&gt;[ ]   Are there policies and procedures for data management, model development, and deployment?&lt;/li&gt;
&lt;li&gt;[ ]   Is there a designated team responsible for MLOps?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional ideas for the questions in this category:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Do we have a clear and well-defined governance structure for our ML projects?&lt;/li&gt;
&lt;li&gt;Are there clearly defined roles and responsibilities for ML project team members?&lt;/li&gt;
&lt;li&gt;Do we have a process in place for managing and controlling access to data used in ML projects?&lt;/li&gt;
&lt;li&gt;Are there policies and procedures in place to ensure that ML models are developed and deployed ethically?&lt;/li&gt;
&lt;li&gt;Do we have a process in place for regularly reviewing and updating our ML governance framework?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="2-data-management"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-data-management"&gt;2.  Data Management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ ]   Is there a centralized repository for storing and managing data?&lt;/li&gt;
&lt;li&gt;[ ]   Is data preprocessing and feature engineering automated?&lt;/li&gt;
&lt;li&gt;[ ]   Are there mechanisms in place for data quality control and validation?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional ideas for the questions in this category:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Do we have a process in place for collecting, storing, and maintaining the integrity of data used in ML projects?&lt;/li&gt;
&lt;li&gt;Are there clear data quality standards and procedures in place to ensure that data used in ML projects is fit for purpose?&lt;/li&gt;
&lt;li&gt;Do we have a process in place for versioning and tracking changes to data used in ML projects?&lt;/li&gt;
&lt;li&gt;Are there procedures in place to ensure that data used in ML projects is kept secure and protected against unauthorized access?&lt;/li&gt;
&lt;li&gt;Do we have a process in place for monitoring data usage and ensuring compliance with relevant regulations and laws?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="3-model-development"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-model-development"&gt;3.  Model Development&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ ]  Is there a standard development process for creating and testing ML models?&lt;/li&gt;
&lt;li&gt;[ ]  Are there tools and frameworks in place to facilitate collaboration and version control?&lt;/li&gt;
&lt;li&gt;[ ]  Are there mechanisms in place to track the performance of models over time?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional ideas for the questions in this category:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Are there clear standards and guidelines in place for developing, testing, and evaluating models?&lt;/li&gt;
&lt;li&gt;Is there a process in place for version control and reproducibility of models?&lt;/li&gt;
&lt;li&gt;Are there testing and evaluation procedures in place to ensure the model's performance and accuracy?&lt;/li&gt;
&lt;li&gt;Are there processes in place for monitoring and addressing bias in the models?&lt;/li&gt;
&lt;li&gt;Is there a process in place for continuous improvement and updating of models in response to new data and changing requirements?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="4-deployment"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-deployment"&gt;4.  Deployment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ ] Are models deployed in a consistent and repeatable manner?&lt;/li&gt;
&lt;li&gt;[ ] Are there mechanisms in place for monitoring and maintaining deployed models?&lt;/li&gt;
&lt;li&gt;[ ] Are there processes for rolling out updates to models?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional ideas for the questions in this category:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there a process in place for deploying models to production systems?&lt;/li&gt;
&lt;li&gt;Are there procedures in place for monitoring and maintaining deployed models?&lt;/li&gt;
&lt;li&gt;Is there a process in place for rolling out updates and new versions of models?&lt;/li&gt;
&lt;li&gt;Are there procedures in place for monitoring and addressing model performance in production?&lt;/li&gt;
&lt;li&gt;Are there processes in place for managing and securing the data and resources used by deployed models?
&lt;a id="5-monitoring-and-evaluation"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="5-monitoring-and-evaluation"&gt;5.  Monitoring and Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ ]   Are there metrics in place to measure the performance of models in production?&lt;/li&gt;
&lt;li&gt;[ ]  Are there mechanisms for capturing and analyzing feedback from users?&lt;/li&gt;
&lt;li&gt;[ ]  Are there processes in place for conducting regular audits and evaluations of ML systems?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional ideas for the questions in this category:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there a process in place for monitoring model performance and accuracy in production?&lt;/li&gt;
&lt;li&gt;Are there procedures in place for evaluating model performance against business objectives and goals?&lt;/li&gt;
&lt;li&gt;Are there processes in place for identifying and addressing any issues or errors with deployed models?&lt;/li&gt;
&lt;li&gt;Are there procedures in place for collecting and analyzing feedback on deployed models from end-users?&lt;/li&gt;
&lt;li&gt;Are there processes in place for implementing and evaluating changes to improve model performance over time?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="6-continuous-improvement"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-continuous-improvement"&gt;6.  Continuous Improvement&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ ]  Are there regular reviews of the MLOps process in place?&lt;/li&gt;
&lt;li&gt;[ ]  Are there processes for continuous improvement and experimentation?&lt;/li&gt;
&lt;li&gt;[ ]  Is there a culture of learning and adaptation in the organization?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional ideas for the questions in this category:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there a process in place for regularly reviewing and improving deployed models?&lt;/li&gt;
&lt;li&gt;Are there procedures in place for incorporating feedback and new data into model development and deployment?&lt;/li&gt;
&lt;li&gt;Is there a process in place for regularly retraining and updating models?&lt;/li&gt;
&lt;li&gt;Are there procedures in place for monitoring and addressing shifts in data distribution and model drift?&lt;/li&gt;
&lt;li&gt;Are there processes in place for conducting impact analysis and A/B testing of new models or updates?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="visualize-it"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="visualize-it"&gt;Visualize it&lt;/h2&gt;
&lt;p&gt;Finally, you can use radar plot to present the results visually, as example see the radar chart below:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_scorecard/mlops_scorecard.jpg"  alt="MLOps impementation progress - radar chart"&gt;&lt;/p&gt;
&lt;p&gt;or similar Stellar plot as below:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_scorecard/mlops_scorecard_stellar_plot_2.jpg"  alt="MLOps impementation progress - stellar plot"&gt;&lt;/p&gt;
&lt;p&gt;You can use other type of chart that suit your needs to present the results in a visual manner.
&lt;a id="limitations-that-you-should-be-aware-of"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="limitations-that-you-should-be-aware-of"&gt;Limitations that you should be aware of&lt;/h2&gt;
&lt;p&gt;There are a few potential heads-up to keep in mind when using this kind of evaluation for measuring the maturity of an organization's MLOps processes.&lt;/p&gt;
&lt;p&gt;&lt;a id="factors-limiting-potential-utility-of-such-analysis"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="factors-limiting-potential-utility-of-such-analysis"&gt;Factors limiting potential utility of such analysis&lt;/h3&gt;
&lt;p&gt;&lt;a id="1-subjectivity"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="1-subjectivity"&gt;1.  Subjectivity&lt;/h4&gt;
&lt;p&gt;The evaluation process may be subject to personal biases and interpretations, making it difficult to obtain consistent and accurate results.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-lack-of-context"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="2-lack-of-context"&gt;2.  Lack of context&lt;/h4&gt;
&lt;p&gt;The evaluation process may not take into account the specific context and constraints of the organization, leading to results that may not be relevant or applicable.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-limited-scope"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="3-limited-scope"&gt;3.  Limited scope&lt;/h4&gt;
&lt;p&gt;The evaluation process may only assess a limited number of aspects of MLOps processes, leading to an incomplete understanding of the organization's MLOps maturity.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-limited-flexibility"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="4-limited-flexibility"&gt;4.  Limited flexibility&lt;/h4&gt;
&lt;p&gt;The evaluation process may be limited in its ability to adapt to changing conditions and requirements, making it difficult to maintain its relevance over time.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-limited-actionability"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="5-limited-actionability"&gt;5.  Limited actionability&lt;/h4&gt;
&lt;p&gt;The results of the evaluation process may not be actionable, meaning that they may not provide clear guidance on how to improve the organization's MLOps processes.&lt;/p&gt;
&lt;p&gt;&lt;a id="hints-for-overcoming-limitations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="hints-for-overcoming-limitations"&gt;Hints for overcoming limitations&lt;/h3&gt;
&lt;p&gt;In order to overcome these limitations, it's advisable to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensure that the evaluation process is carried out by a diverse group of individuals with relevant expertise, to reduce the influence of personal biases.&lt;/li&gt;
&lt;li&gt;Tailor the evaluation process to the specific context and constraints of the organization.&lt;/li&gt;
&lt;li&gt;Continuously update the evaluation process to reflect new trends, best practices, and regulations.&lt;/li&gt;
&lt;li&gt;Use the evaluation results to develop a clear plan for improving the organization's MLOps processes and track progress over time.&lt;/li&gt;
&lt;li&gt;Consider using a combination of quantitative and qualitative methods for evaluating MLOps maturity, in order to have a more comprehensive understanding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To have more reliable insights in  progress of introducing MLOps try to use this method in conjunction with other methods such as interviews, surveys, and data analysis.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/><category term="scorecard"/><category term="checklist"/><category term="mlops-maturity"/><category term="mlops-readines"/><category term="organization"/><category term="governance"/><category term="data-management"/><category term="model-deployment"/><category term="deployment"/><category term="model-monitoring"/><category term="model-evaluation"/></entry><entry><title>Common Types of Data Science Projects</title><link href="http://127.0.0.1:8000/common-types-of-data-science-projects/" rel="alternate"/><published>2023-01-19T00:00:00+01:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-19:/common-types-of-data-science-projects/</id><summary type="html">&lt;p&gt;Learn about common types of data science projects and best practices for approaching them. From end-to-end individual work to production-ready projects, this guide covers it all.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data science is a rapidly growing field that encompasses a wide range of activities and applications. As a data scientist, you may find yourself working on a variety of different types of projects, each with their own unique challenges and requirements. In this article, we'll explore some of the most common types of data science projects and discuss best practices for approaching them.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-end-to-end-individual-work"&gt;1. End-to-end Individual Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-collaborative-project"&gt;2. Collaborative Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-individual-work-but-final-notebook-shares-as-result"&gt;3. Individual Work but Final Notebook Shares as Result&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-production-ready-projects"&gt;4. Production-Ready Projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-research-projects"&gt;5. Research Projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="1-end-to-end-individual-work"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-end-to-end-individual-work"&gt;1. End-to-end Individual Work&lt;/h3&gt;
&lt;p&gt;&lt;img alt="End-to-end Individual Work - graphics" src="/images/ml_project_types/individual_e2e_project.jpg"&gt;&lt;/p&gt;
&lt;p&gt;End-to-end individual work is a type of data science project that is typically &lt;strong&gt;short-term&lt;/strong&gt; and has &lt;strong&gt;low complexity&lt;/strong&gt;. These projects may consist of a &lt;strong&gt;single Jupyter notebook&lt;/strong&gt; and may be completed by a single person. Examples of end-to-end individual work include analyzing customer data for a retail store or building a simple machine learning model for personal use.&lt;/p&gt;
&lt;p&gt;When working on an end-to-end individual project, it's important to keep the project organized and &lt;strong&gt;structured&lt;/strong&gt;. A good way to do this is to use a template or a framework such as &lt;a href="https://github.com/drivendata/cookiecutter-data-science"&gt;Cookiecutter Data Science&lt;/a&gt;, which provides a standardized directory structure and files for data science projects. Additionally, it is recommended to use version control tools such as &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; to track changes in the project over time.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-collaborative-project"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-collaborative-project"&gt;2. Collaborative Project&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Collaborative Project - graphics" src="/images/ml_project_types/team_project.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Collaborative projects are a type of data science project that involve multiple people working together to achieve a common goal. These projects may be of &lt;strong&gt;medium complexity&lt;/strong&gt; and may involve &lt;strong&gt;multiple Jupyter notebooks&lt;/strong&gt;. Examples of collaborative projects include predicting customer churn for a client or building a recommendation engine for a streaming service.&lt;/p&gt;
&lt;p&gt;When working on a collaborative project, it's important to establish clear communication and collaboration practices. &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; is a powerful version control system that allows multiple people to work on the same project at the same time. Additionally, it is important to extract some mature code into a python module that is imported into a notebook and take some effort to clean up and test the code. Tools such as &lt;a href="https://docs.pytest.org/en/latest/"&gt;pytest&lt;/a&gt; and &lt;a href="https://flake8.pycqa.org/en/latest/"&gt;flake8&lt;/a&gt; can help to ensure that the code is of high quality.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-individual-work-but-final-notebook-shares-as-result"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-individual-work-but-final-notebook-shares-as-result"&gt;3. Individual Work but Final Notebook Shares as Result&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Individual Work but Final Notebook Shares as Result - graphics" src="/images/ml_project_types/individual_but_presented.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Individual work but final notebook shares as result is a type of data science project that involves a single person working on a project, but the final results are shared with others. These projects may involve a single Jupyter notebook and may be completed by a single person. Examples of this type of project include an analysis on demand for a company's management team or a side-project/tutorial that is published on a blog.&lt;/p&gt;
&lt;p&gt;When working on an individual work but final notebook shares as result project, it's important to keep the project organized and structured. A good way to do this is to use a template or a framework such as &lt;a href="https://github.com/drivendata/cookiecutter-data-science"&gt;Cookiecutter Data Science&lt;/a&gt;, which provides a standardized directory structure and files for data science projects. Additionally, it is recommended to use version control tools such as &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; to track changes in the project over time.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-production-ready-projects"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-production-ready-projects"&gt;4. Production-Ready Projects&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Production-Ready Projects - graphics" src="/images/ml_project_types/production_ready.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Production-ready projects are a type of data science project that involve developing a model or algorithm that will be deployed in a production environment. These projects may be of high complexity and may involve multiple Jupyter notebooks. Examples of production
-ready projects include building a recommendation engine for a streaming service or developing a predictive model for financial forecasting.&lt;/p&gt;
&lt;p&gt;When working on a production-ready project, it's important to consider the scalability and performance of the final model. Tools such as &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; and &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; can be used to deploy the model in a production environment. Additionally, it is important to extract some mature code into a python module that is imported into a notebook and take some effort to clean up and test the code using tools such as &lt;a href="https://docs.pytest.org/en/latest/"&gt;pytest&lt;/a&gt; and &lt;a href="https://flake8.pycqa.org/en/latest/"&gt;flake8&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-research-projects"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-research-projects"&gt;5. Research Projects&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Research Projects - graphics" src="/images/ml_project_types/scientific.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Research projects are a type of data science project that involve developing new algorithms or techniques in the field of machine learning and artificial intelligence. These projects may involve a lot of experimentation and iteration, and the final results would be shared in a research paper or conference presentation. Examples of research projects include developing a new reinforcement learning algorithm or exploring the use of generative models for natural language processing.&lt;/p&gt;
&lt;p&gt;When working on a research project, it's important to keep detailed records of the experimentation process. Tools such as &lt;a href="https://www.tensorflow.org/tensorboard"&gt;TensorBoard&lt;/a&gt; and &lt;a href="https://mlflow.org/"&gt;MLflow&lt;/a&gt; can be used to track the progress of the project and visualize the results. Additionally, it is important to use version control tools such as &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; to track changes in the project over time.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Data science projects can take many forms, each with its own unique challenges and requirements. By understanding the &lt;strong&gt;different types of projects&lt;/strong&gt; and &lt;strong&gt;best practices for approaching them&lt;/strong&gt;, data scientists can work more efficiently and effectively to deliver valuable insights and solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;
Graphics created with openjourney model&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="project"/><category term="software-maintenence"/><category term="software-development"/><category term="software-project"/><category term="jupyter"/></entry><entry><title>Don't Just Create Backups, Verify Them - How Restic Can Help?</title><link href="http://127.0.0.1:8000/verify-backups-restic-example/" rel="alternate"/><published>2023-01-15T00:00:00+01:00</published><updated>2023-01-25T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-15:/verify-backups-restic-example/</id><summary type="html">&lt;p&gt;Learn how to verify your backups with Restic, ensure completeness, integrity, and recoverability. Automate the process for peace of mind. Read now&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Creating regular backups of your data is an important step in protecting against data loss due to hardware failure, human error, or malicious attacks. However, it is not enough to simply create backups - it is also crucial to ensure that the backups are done correctly and can be successfully restored in the event of a disaster. In this article, we will discuss how to verify that backups created with the open-source backup tool Restic are done correctly.&lt;/p&gt;
&lt;h2 id="general-principles-of-backup-verification"&gt;general principles of backup verification&lt;/h2&gt;
&lt;p&gt;Before diving into the specifics of Restic, it's important to understand the general principles of backup verification. When verifying backups, you should ensure that the following three elements are present:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Completeness&lt;/strong&gt;: The backup should contain all of the files and data that you expect it to contain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrity&lt;/strong&gt;: The backup should be free from errors, such as corrupted files or missing data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recoverability&lt;/strong&gt;: The backup should be able to be restored to a usable state in the event of a disaster.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="how-to-verify-backups-created-with-restic"&gt;How to verify backups created with Restic?&lt;/h2&gt;
&lt;p&gt;With these principles in mind, let's take a look at how to verify backups created with Restic.&lt;/p&gt;
&lt;h3 id="1-verify-completeness-of-backups"&gt;1.  Verify completeness of backups&lt;/h3&gt;
&lt;p&gt;To ensure that your backups contain all of the files and data that you expect, you can use the Restic command &lt;code&gt;restic check&lt;/code&gt;. This command will check the integrity of the data stored in the backup, as well as verify that all files are present and have the correct permissions. If there are any issues with the backup, Restic will print a message indicating the problem and the file that is affected.&lt;/p&gt;
&lt;h3 id="verify-integrity-of-backups"&gt;Verify integrity of backups&lt;/h3&gt;
&lt;p&gt;Another important aspect of backup verification is ensuring that the backup is free from errors, such as corrupted files or missing data. Restic provides the command &lt;code&gt;restic check --read-data&lt;/code&gt; to verify the integrity of data stored in the backup. This command will read all files in the backup and compare their checksums to the original files. If the checksums do not match, this indicates that the file is corrupted or has been modified.&lt;/p&gt;
&lt;h3 id="3-verify-recoverability-of-backups"&gt;3.  Verify recoverability of backups&lt;/h3&gt;
&lt;p&gt;The most important aspect of backup verification is the ability to restore the data in the event of a disaster. To test the recoverability of a Restic backup, you can use the command &lt;code&gt;restic restore&lt;/code&gt;. This command will allow you to specify a specific snapshot or set of files to restore. You can also specify a directory where the files should be restored. Once the restore is complete, you should carefully review the restored files to ensure that they are complete, accurate, and usable.&lt;/p&gt;
&lt;h3 id="4-automate-the-process"&gt;4.  Automate the process&lt;/h3&gt;
&lt;p&gt;It can be time-consuming to manually check backups for completeness, integrity and recoverability. To automate this process, you can schedule Restic's commands &lt;code&gt;restic check&lt;/code&gt;, &lt;code&gt;restic check --read-data&lt;/code&gt; and &lt;code&gt;restic restore&lt;/code&gt; to run at specific intervals using a cron job or a task scheduler.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Backup verification is an essential step in protecting your data. By using the open-source tool Restic and following the steps outlined in this article, you can ensure that your backups are done correctly and can be successfully restored in the event of a disaster. Remember to schedule the verification process to run at specific intervals and to regularly review the backups to ensure that they are complete, accurate, and usable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading image from &lt;a href="https://unsplash.com/photos/GNyjCePVRs8"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@benjaminlehman"&gt;benjamin lehman&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;X::[[my_system_for_backups]]&lt;/p&gt;</content><category term="Howto"/><category term="linux"/><category term="backup"/><category term="good-practices"/><category term="restic"/></entry><entry><title>How to Detect ChatGPT-Generated Text?</title><link href="http://127.0.0.1:8000/detection-of-texts-generated-by-large-language-models-like-chatgpt/" rel="alternate"/><published>2023-01-11T00:00:00+01:00</published><updated>2023-01-20T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-11:/detection-of-texts-generated-by-large-language-models-like-chatgpt/</id><summary type="html">&lt;p&gt;Discover the latest methods for distinguishing machine-generated text from the human-written text. Learn about statistical, syntactic, semantic, and neural network-based approaches. Stay up-to-date with the latest research in NLP and AI.&lt;/p&gt;</summary><content type="html">&lt;h4 id="survey-of-methods-for-detecting-text-generated-by-large-language-models-llms"&gt;Survey of Methods for Detecting Text Generated by Large Language Models (LLMs)&lt;/h4&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The detection of whether a given text is organic or synthetic refers to the task of determining whether a text is written by a human or generated by a machine, such as a language model. This is an important problem in natural language processing (NLP) as the use of language models for text generation is becoming increasingly common, and it is important to be able to distinguish between machine-generated text and human-written text. Detection of synthetic text is important in many applications such as plagiarism detection, authorship identification, and many more.&lt;/p&gt;
&lt;p&gt;There are several approaches that have been proposed for this task, including statistical, syntactic, semantic, and neural network-based methods. These methods use various features of the text, such as n-gram frequencies, syntactic structure, and semantic coherence, to distinguish between machine-generated and human-generated text. More recent methods have also started to use pre-trained models and adversarial training to improve the performance of text classifiers.&lt;/p&gt;
&lt;p&gt;Overall, the detection of synthetic text is a challenging problem that requires the integration of multiple techniques to achieve high accuracy. It is an active area of research in NLP, and new methods and approaches are constantly being proposed to improve the performance of synthetic text detection.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#non-neural-network-approaches"&gt;Non-Neural Network Approaches&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#statistical-approaches"&gt;Statistical approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#syntactic-approaches"&gt;Syntactic approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#semantic-approaches"&gt;Semantic approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interaction-based-approaches"&gt;Interaction-based approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hybrid-approaches"&gt;Hybrid approaches&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning-approaches"&gt;Deep Learning approaches&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#neural-network-based-methods"&gt;Neural Network-based methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-methods"&gt;Supervised methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-methods"&gt;Unsupervised methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary-of-nn-based-methods"&gt;Summary of NN-based methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#adversarial-training"&gt;Adversarial training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#attention-based-methods"&gt;Attention-based methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pre-trained-models"&gt;Pre-trained models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="non-neural-network-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="non-neural-network-approaches"&gt;Non-Neural Network Approaches&lt;/h2&gt;
&lt;p&gt;&lt;a id="statistical-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="statistical-approaches"&gt;Statistical approaches&lt;/h3&gt;
&lt;p&gt;These methods use various statistical features, such as n-gram frequencies, to distinguish between machine-generated and human-generated text. For example &lt;a href="#r1"&gt;1&lt;/a&gt; uses a statistical model to identify machine-translated text.&lt;/p&gt;
&lt;p&gt;&lt;a id="syntactic-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="syntactic-approaches"&gt;Syntactic approaches&lt;/h3&gt;
&lt;p&gt;These methods rely on the syntactic structure of the text, such as the length of sentences, the use of punctuation, and the presence of certain grammatical constructions.&lt;/p&gt;
&lt;p&gt;&lt;a id="semantic-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="semantic-approaches"&gt;Semantic approaches&lt;/h3&gt;
&lt;p&gt;These methods rely on the meaning of the text, such as the coherence of the content and the presence of certain semantic patterns. For example, &lt;a href="#r2"&gt;2&lt;/a&gt; uses semantic features to identify machine-generated text.&lt;/p&gt;
&lt;p&gt;&lt;a id="interaction-based-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="interaction-based-approaches"&gt;Interaction-based approaches&lt;/h3&gt;
&lt;p&gt;These methods rely on the interaction between the language model and the human user. For example, use human-written stories to evaluate the language generation models.&lt;/p&gt;
&lt;p&gt;&lt;a id="hybrid-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="hybrid-approaches"&gt;Hybrid approaches&lt;/h3&gt;
&lt;p&gt;These methods use a combination of the above approaches, such as &lt;a href="#r3"&gt;3&lt;/a&gt; uses a combination of statistical, syntactic, and semantic features to identify machine-generated text.&lt;/p&gt;
&lt;p&gt;&lt;a id="deep-learning-approaches"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="deep-learning-approaches"&gt;Deep Learning approaches&lt;/h2&gt;
&lt;p&gt;&lt;a id="neural-network-based-methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="neural-network-based-methods"&gt;Neural Network-based methods&lt;/h3&gt;
&lt;p&gt;Neural network-based methods use deep learning techniques to learn the representations of human and machine-generated text and use them to classify new text. These methods can be divided into two main categories:&lt;/p&gt;
&lt;h4 id="supervised-methods"&gt;Supervised methods&lt;/h4&gt;
&lt;p&gt;These methods use a dataset of labeled text, where the text is labeled as human-generated or machine-generated, to train a neural network to classify new text. The neural network is typically composed of an encoder and a classifier. The encoder is used to convert the input text into a fixed-length vector representation, and the classifier is used to make the final decision about whether the text is human-generated or machine-generated. The encoder can be a pre-trained model such as BERT or GPT-2, or it can be trained from scratch. The classifier is typically a fully connected neural network with one or more hidden layers.&lt;/p&gt;
&lt;h4 id="unsupervised-methods"&gt;Unsupervised methods&lt;/h4&gt;
&lt;p&gt;These methods do not require labeled text, and instead, use unsupervised techniques such as clustering or autoencoders to learn the representations of human and machine-generated text. The neural network is typically an autoencoder, which is trained to reconstruct the input text. The network learns to extract the features of the text that are important for reconstruction, and these features can then be used to classify new text as human-generated or machine-generated.&lt;/p&gt;
&lt;h4 id="summary-of-nn-based-methods"&gt;Summary of NN-based methods&lt;/h4&gt;
&lt;p&gt;In both cases, during the training phase, the neural network learns to extract the features from the text that are indicative of whether it was generated by a machine or a human. These features can be syntactic, semantic, or even statistical based on the architecture and the training data. In the testing phase, the neural network can classify new text by extracting the features and making a decision based on the learned representations.&lt;/p&gt;
&lt;p&gt;One of the advantages of neural network-based methods is their ability to learn complex representations of the text, which can capture both syntactic and semantic features of the text. They also have the ability to handle large amounts of data and generalize well to new text. However, they can be computationally expensive, and they require large amounts of labeled data to train effectively.&lt;/p&gt;
&lt;p&gt;&lt;a id="adversarial-training"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="adversarial-training"&gt;Adversarial training&lt;/h3&gt;
&lt;p&gt;This approach trains a classifier by generating machine-generated text that is similar to human-written text, and then fine-tuning the classifier to distinguish between the two.&lt;/p&gt;
&lt;p&gt;&lt;a id="attention-based-methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="attention-based-methods"&gt;Attention-based methods&lt;/h3&gt;
&lt;p&gt;These methods use attention mechanisms to identify the key parts of the text that are indicative of whether it was generated by a machine or a human. &lt;a href="#r4"&gt;4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="pre-trained-models"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="pre-trained-models"&gt;Pre-trained models&lt;/h3&gt;
&lt;p&gt;These methods use pre-trained models, such as BERT or GPT-2, to extract features from the text and use them to classify the text as human-generated or machine-generated. For example, "Pre-trained Language Models for Discriminating Human and Machine-Generated Text" (2021) by J. Wang et al. &lt;a href="#r5"&gt;5&lt;/a&gt; uses pre-trained models to extract features and classify text.&lt;/p&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a id="r1"&gt;[1]&lt;/a&gt; Jean Senellart et al. &lt;strong&gt;&lt;em&gt;"Achieving Open Vocabulary Neural Machine Translation"&lt;/em&gt;&lt;/strong&gt; (2014), &lt;a href="https://arxiv.org/abs/1604.00788"&gt;arXiv:1604.00788&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id="r2"&gt;[2]&lt;/a&gt; Mengjiao Bao, Jianxin Li et al. &lt;strong&gt;&lt;em&gt;"Learning Semantic Coherence for Machine Generated Spam Text Detection"&lt;/em&gt;&lt;/strong&gt; (2019) &lt;a href="https://www.semanticscholar.org/paper/Learning-Semantic-Coherence-for-Machine-Generated-Bao-Li/5de7dca75e9846fcbb7d6c9b4c8ab5aaf6cfbd43"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id="r3"&gt;[3]&lt;/a&gt; Nirav Diwan, Tanmoy Chakravorty, Zubair Shafiq &lt;strong&gt;&lt;em&gt;"Fingerprinting Fine-tuned Language Models in the Wild"&lt;/em&gt;&lt;/strong&gt; (2021) &lt;a href="https://arxiv.org/abs/2106.01703"&gt;https://arxiv.org/abs/2106.01703&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a id="r4"&gt;[4]&lt;/a&gt; Tiziano Fagni et al. &lt;strong&gt;&lt;em&gt;"TweepFake: about detecting deepfake tweets"&lt;/em&gt;&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a id="r5"&gt;[5]&lt;/a&gt; J. Wang et al. &lt;strong&gt;&lt;em&gt;"Pre-trained Language Models for Discriminating Human and Machine-Generated Text"&lt;/em&gt;&lt;/strong&gt; (2021) &lt;a href="https://arxiv.org/abs/2105.10311"&gt;https://arxiv.org/abs/2105.10311&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[6] [Real or Fake? Learning to Discriminate Machine from Human Generated Text | DeepAI]&lt;a href="https://deepai.org/publication/real-or-fake-learning-to-discriminate-machine-from-human-generated-text"&gt;Real or Fake? Learning to Discriminate Machine from Human Generated Text | DeepAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openai.com/blog/forecasting-misuse/"&gt;Forecasting Potential Misuses of Language Models for Disinformation Campaigns - and How to Reduce Risk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="NLP"/><category term="machine-learning"/><category term="ChatGPT"/><category term="BERT"/><category term="Adversarial-networks"/><category term="GAN"/></entry><entry><title>Maximizing Efficiency in MLOps - How Blue/Green Deployment Can Help?</title><link href="http://127.0.0.1:8000/blue-green-deployment-in-mlops/" rel="alternate"/><published>2023-01-03T00:00:00+01:00</published><updated>2023-01-17T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2023-01-03:/blue-green-deployment-in-mlops/</id><summary type="html">&lt;p&gt;Learn about blue/green deployment in MLOps, its usefulness and when to use it, and the cost and complexity of maintaining two separate environments&lt;/p&gt;</summary><content type="html">&lt;p&gt;X::[[mlops]]&lt;/p&gt;
&lt;p&gt;MLOps (Machine Learning Operations) is a rapidly growing field that aims to bridge the gap between data science and production. One of the key practices in MLOps is the deployment of machine learning models. Blue/Green deployment is a deployment strategy that can be used to minimize downtime and reduce the risk of introducing new bugs when deploying machine learning models. In this article, we will explore the concept of Blue/Green deployment and its usefulness in MLOps.&lt;/p&gt;
&lt;h2 id="what-is-bluegreen-deployment"&gt;What is Blue/Green Deployment?&lt;/h2&gt;
&lt;p&gt;Blue/Green deployment is a technique for deploying a new version of a service by running two identical production environments, called Blue and Green. The current live environment, or Blue, continues to serve live traffic while the new version, or Green, is deployed. Once the new version is deployed and validated, traffic is switched over to the new environment, or Green. The previous environment, or Blue, is then decommissioned or repurposed for future deployments.&lt;/p&gt;
&lt;p&gt;The main advantage of Blue/Green deployment is that it allows for minimal downtime during deployments and easy rollbacks in case of issues. In addition, it allows for testing the new version of the service in a live environment before making it available to the users.&lt;/p&gt;
&lt;h2 id="is-bluegreen-deployment-useful-in-mlops"&gt;Is Blue/Green Deployment useful in MLOps?&lt;/h2&gt;
&lt;p&gt;Blue/Green deployment can be a useful practice in MLOps, especially in cases where the machine learning model is critical to the business and downtime is not acceptable. However, it's important to note that Blue/Green deployment is not always necessary or appropriate for every MLOps scenario. The usefulness of Blue/Green deployment depends on a variety of factors, such as the complexity of the machine learning model, the size and scale of the deployment, and the level of risk involved.&lt;/p&gt;
&lt;p&gt;In cases where the machine learning model is relatively simple, and the deployment is small in scale, Blue/Green deployment may not be necessary. In these cases, a simpler deployment strategy such as rolling updates may be more appropriate.&lt;/p&gt;
&lt;p&gt;On the other hand, in cases where the machine learning model is complex, and the deployment is large in scale, Blue/Green deployment may be a more appropriate strategy. This is because it allows for testing the new version of the service in a live environment before making it available to the users, minimizing the risk of introducing new bugs and providing an easy rollback option in case of issues.&lt;/p&gt;
&lt;p&gt;When considering Blue/Green deployment in MLOps, it's important to also consider the cost and complexity of maintaining two separate environments. It's also important to have a clear rollback strategy in case of issues with the new version of the service.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Blue/Green deployment is a useful technique for deploying machine learning models in cases where the model is critical to the business and downtime is not acceptable. However, it's important to consider the complexity and scale of the deployment, as well as the cost and complexity of maintaining two separate environments. It's also important to have a clear rollback strategy in case of issues with the new version of the service.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;
Header image from &lt;a href="https://unsplash.com/photos/pfX-GsJMtDY"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@joelfilip"&gt;Joel Filipe&lt;/a&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/><category term="devops"/><category term="machine-learning"/><category term="deployment"/><category term="production"/></entry><entry><title>MLOps Certifications - A Comprehensive Guide</title><link href="http://127.0.0.1:8000/mlops-certifications-a-comprehensive-guide/" rel="alternate"/><published>2022-11-27T00:00:00+01:00</published><updated>2023-01-13T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-11-27:/mlops-certifications-a-comprehensive-guide/</id><summary type="html">&lt;p&gt;Learn about popular MLOps certifications offered by Amazon, Google, and Microsoft. Understand if they are worth the effort and money and discover free MLOps certifications available to gain knowledge and understanding of the field.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mlops-certificates---are-they-worth-your-effort-and-money"&gt;MLOps Certificates - Are They Worth Your Effort and Money?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#certifications"&gt;Certifications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#aws-certified-machine-learning---specialty"&gt;AWS Certified Machine Learning - Specialty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#google-cloud-professional-machine-learning-engineer"&gt;Google Cloud Professional Machine Learning Engineer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#free-mlops-certifications"&gt;Free MLOps Certifications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Machine learning operations (MLOps) is a rapidly growing field that combines the expertise of data science and software engineering to streamline and automate the deployment of machine learning models in production. As the demand for MLOps professionals increases, more and more organizations are offering certifications to validate the skills and knowledge of individuals in this field. In this blog post, we'll take a look at some of the most popular MLOps certifications available and discuss their relevance to the industry.&lt;/p&gt;
&lt;p&gt;&lt;a id="mlops-certificates---are-they-worth-your-effort-and-money"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mlops-certificates-are-they-worth-your-effort-and-money"&gt;MLOps Certificates - Are They Worth Your Effort and Money?&lt;/h2&gt;
&lt;p&gt;Before diving into the specific certifications, it's important to consider whether or not investing in an MLOps certification is worth your time and money. While certifications can be a valuable addition to your resume and demonstrate your knowledge and skills to potential employers, they are not a substitute for real-world experience. In the fast-paced field of MLOps, the most important thing is to have hands-on experience with the tools and techniques used in the industry.&lt;/p&gt;
&lt;p&gt;That being said, a certification can be a great way to gain a deeper understanding of the field and to stay up-to-date with the latest trends and best practices. Additionally, certifications can also help you to stand out in a crowded job market and may be preferred or required by certain employers.&lt;/p&gt;
&lt;p&gt;&lt;a id="certifications"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="certifications"&gt;Certifications&lt;/h2&gt;
&lt;p&gt;&lt;a id="aws-certified-machine-learning---specialty"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="aws-certified-machine-learning-specialty"&gt;AWS Certified Machine Learning - Specialty&lt;/h3&gt;
&lt;p&gt;The AWS Certified Machine Learning - Specialty certification is offered by Amazon Web Services (AWS) and is designed for individuals who want to demonstrate their expertise in using AWS for machine learning. To earn this certification, candidates must pass an exam that covers topics such as data preparation, model selection, and deployment.&lt;/p&gt;
&lt;p&gt;This certification is relevant for those who work with AWS and want to demonstrate their knowledge of the platform's capabilities for machine learning. It is also useful for those who want to learn more about how to use AWS for MLOps.&lt;/p&gt;
&lt;p&gt;Link to certification: &lt;a href="https://aws.amazon.com/certification/certified-machine-learning-specialty/"&gt;AWS Certified Machine Learning - Specialty Certification | AWS Certification | AWS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="google-cloud-professional-machine-learning-engineer"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="google-cloud-professional-machine-learning-engineer"&gt;Google Cloud Professional Machine Learning Engineer&lt;/h3&gt;
&lt;p&gt;The Google Cloud Professional Machine Learning Engineer certification is offered by Google Cloud and is designed for individuals who want to demonstrate their expertise in designing and implementing machine learning solutions on the Google Cloud platform. To earn this certification, candidates must pass an exam that covers topics such as data preparation, model selection, and deployment.&lt;/p&gt;
&lt;p&gt;This certification is relevant for those who work with Google Cloud and want to demonstrate their knowledge of the platform's capabilities for machine learning. It is also useful for those who want to learn more about how to use Google Cloud for MLOps.&lt;/p&gt;
&lt;p&gt;Link to certification: &lt;a href="https://cloud.google.com/certification/machine-learning-engineer"&gt;Professional ML Engineer Certification  |  Google Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="free-mlops-certifications"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="free-mlops-certifications"&gt;Free MLOps Certifications&lt;/h2&gt;
&lt;p&gt;While many MLOps certifications are offered by organizations and require payment to take the exam, there are also a number of free certifications available. These certifications may not carry as much weight as paid certifications, but they can still be a great way to gain knowledge and understanding of the field.&lt;/p&gt;
&lt;p&gt;Here are a few examples of free MLOps certifications, first one is specialization from deeplearning.ai with Andrew Ng as one of the instructors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The one from Coursera:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops"&gt;Machine Learning Engineering for Production (MLOps) | Coursera&lt;/a&gt;. There are 4 Courses in this Specialization:&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/introduction-to-machine-learning-in-production?specialization=machine-learning-engineering-for-production-mlops"&gt;Introduction to Machine Learning in Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production?specialization=machine-learning-engineering-for-production-mlops"&gt;Machine Learning Data Lifecycle in Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production?specialization=machine-learning-engineering-for-production-mlops"&gt;Machine Learning Modeling Pipelines in Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/deploying-machine-learning-models-in-production?specialization=machine-learning-engineering-for-production-mlops"&gt;Deploying Machine Learning Models in Production&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Offering from edEx:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.edx.org/course/mlops-for-scaling-tinyml?index=product&amp;amp;queryID=39e1fe1817b7fd2d6b7935cb43dd9523&amp;amp;position=1"&gt;MLOps for Scaling TinyML | edX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.edx.org/course/machine-learning-ops-ml-ops-building-a-pipeline-using-azure?index=product&amp;amp;queryID=3f012e02f235f9d3cd16f39a24db063b&amp;amp;position=2"&gt;MLOps1 (Azure): Deploying AI and ML Models using Microsoft Azure Machine Learning | edX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.edx.org/course/machine-learning-ops-ml-ops1-deploying-ai-ml-models-in-production-using-aws?index=product&amp;amp;queryID=3f012e02f235f9d3cd16f39a24db063b&amp;amp;position=3"&gt;MLOps1 (AWS): Deploying AI &amp;amp; ML Models in Production using Amazon Web Services | edX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.edx.org/course/machine-learning-ops-ml-ops1-deploying-ai-ml-models-in-production-using-gcp?index=product&amp;amp;queryID=3f012e02f235f9d3cd16f39a24db063b&amp;amp;position=4"&gt;MLOps1 (GCP): Deploying AI and ML Models using Google Cloud Platform | edX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While these free certifications do not carry as much weight as paid certifications, they can still be a good way to gain knowledge and understanding of the field. They can also be a good starting point for those considering a paid certification in the future. Additionally, the experience and knowledge gained through such certifications can be added to the resume and can be used in the job interview.&lt;/p&gt;
&lt;p&gt;It's worth noting that, Even though these certifications are free, they still require effort and dedication to complete, and can be a great way to demonstrate your commitment to learning and growing in the field of MLOps.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;MLOps is a rapidly growing field that combines the expertise of data science and software engineering to streamline and automate the deployment of machine learning models in production. As the demand for MLOps professionals increases, more and more organizations are offering certifications to validate the skills and knowledge of individuals in this field.&lt;/p&gt;
&lt;p&gt;In this article, we've looked at some of the most popular MLOps certifications available, including those offered by Amazon Web Services, Google Cloud, and Microsoft. We've also discussed the relevance of these certifications to the industry and whether or not they are worth your effort and money.&lt;/p&gt;
&lt;p&gt;While certifications can be a valuable addition to your resume and demonstrate your knowledge and skills to potential employers, it's important to remember that they are not a substitute for real-world experience. The most important thing is to have hands-on experience with the tools and techniques used in the industry.&lt;/p&gt;
&lt;p&gt;Additionally, we have also discussed the availability of free MLOps certifications. They may not carry as much weight as paid certifications, but they can still be a great way to gain knowledge and understanding of the field. They can also be a good starting point for those considering a paid certification in the future.&lt;/p&gt;
&lt;p&gt;Ultimately, whether or not you decide to pursue an MLOps certification will depend on your goals and career aspirations. But, regardless of your decision, it is essential to continuously learn and stay up-to-date with the latest trends and best practices in the field.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading image from &lt;a href="https://unsplash.com/photos/hvL7qlvZ5T4"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@city_child"&gt;Joan Kwamboka&lt;/a&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/><category term="certificates"/><category term="job-search"/><category term="interview"/><category term="machine-learning"/></entry><entry><title>Roles in MLOps</title><link href="http://127.0.0.1:8000/roles-in-mlops/" rel="alternate"/><published>2022-11-12T00:00:00+01:00</published><updated>2023-01-12T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-11-12:/roles-in-mlops/</id><summary type="html">&lt;p&gt;Learn about the different roles in MLOps and the responsibilities of each role, including Model Deployment Engineer, Data pipeline Engineer, Model Monitoring Engineer, Model Governance Engineer, Machine Learning Infra Engineer and Machine Learning Platform Engineer.&lt;/p&gt;</summary><content type="html">&lt;p&gt;MLOps (Machine Learning Operations) is a practice that combines the principles of DevOps with the unique requirements of machine learning to improve the speed, quality, and reliability of machine learning models in production. In this practice, several roles are defined to manage the end-to-end machine learning pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Engineer&lt;/strong&gt;
Data Engineer is responsible for data collection, storage, and processing. They ensure that data is properly labeled, annotated, and cleaned before it is used for training models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning Engineer&lt;/strong&gt;
Machine Learning Engineer is responsible for designing, developing, and deploying machine learning models. They work closely with data engineers and data scientists to ensure that models are properly trained and optimized.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Scientist&lt;/strong&gt;
Data Scientist is responsible for exploring and analyzing data, developing models and algorithms, and interpreting the results. They work closely with machine learning engineers to ensure that models are properly trained and validated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DevOps Engineer&lt;/strong&gt;
DevOps Engineer is responsible for automating and streamlining the deployment, scaling, and management of machine learning models in production. They work closely with machine learning engineers and data engineers to ensure that models are properly deployed, monitored, and maintained.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Governance&lt;/strong&gt;
Model Governance person is responsible for managing and controlling the lifecycle of machine learning models. They work closely with data scientists, machine learning engineers, and devops engineers to ensure that models are properly versioned, tracked, and audited.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MLOps Engineer&lt;/strong&gt;
MLOps Enginner is responsible for coordinating and managing the end-to-end machine learning pipeline. They work closely with data engineers, machine learning engineers, data scientists, devops engineers, and model governance to ensure that models are properly integrated, deployed, and maintained in a production environment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="specialisations-in-mlops-engineering"&gt;Specialisations in MLOps Engineering&lt;/h2&gt;
&lt;p&gt;The generic &lt;strong&gt;MLOps Engineer&lt;/strong&gt; role can be broken down into several more specific roles:&lt;/p&gt;
&lt;h3 id="1-model-deployment-engineer"&gt;1.  Model Deployment Engineer&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model Deployment Engineer&lt;/strong&gt; is responsible for deploying and managing machine learning models in production. They work closely with machine learning engineers and DevOps engineers to ensure that models are properly deployed, scaled, and maintained in a production environment.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_roles/model_deployment_engineer.png" height=150 alt="Model Deployment Engineer"&gt;&lt;/p&gt;
&lt;h3 id="2-data-pipeline-engineer"&gt;2. Data Pipeline Engineer&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Data pipeline Engineer&lt;/strong&gt; is responsible for building and maintaining data pipeline that feeds the machine learning models. They work closely with data engineers to ensure that data is properly collected, stored, and processed before it is used for training models.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_roles/data_pipeline_engineer.png" height=150 alt="Data Pipeline Engineer"&gt;&lt;/p&gt;
&lt;h3 id="3-model-monitoring-engineer"&gt;3.  Model Monitoring Engineer&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model Monitoring Engineer&lt;/strong&gt; responsible for monitoring the performance and health of machine learning models in production. They work closely with machine learning engineers, data engineers, and DevOps engineers to ensure that models are properly monitored, troubleshot, and maintained in a production environment.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_roles/model_monitoring_engineer.png" height=150 alt="Model Monitoring Engineer"&gt;  &lt;/p&gt;
&lt;h3 id="4-model-governance-engineer"&gt;4.  Model Governance Engineer&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model Governance Engineer&lt;/strong&gt; is responsible for managing and controlling the lifecycle of machine learning models. They work closely with data scientists, machine learning engineers, and DevOps engineers to ensure that models are properly versioned, tracked, and audited.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_roles/model_governance_engineer.png" height=150 alt="Model Governance Engineer"&gt;  &lt;/p&gt;
&lt;h3 id="5-machine-learning-infra-engineer"&gt;5.  Machine Learning Infra Engineer&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning Infra Engineer&lt;/strong&gt; is responsible for building and maintaining the infrastructure that supports machine learning models in production. They work closely with machine learning engineers, data engineers, and DevOps engineers to ensure that the infrastructure is properly configured, scaled, and maintained in a production environment.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_roles/machine_learning_infra_engineer.png" height=150 alt="Machine Learning Infra Engineer"&gt;  &lt;/p&gt;
&lt;h3 id="6-machine-learning-platform-engineer"&gt;6.  Machine Learning Platform Engineer&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning Platform Engineer&lt;/strong&gt; is responsible for building and maintaining the platform that supports machine learning models in production. They work closely with machine learning engineers, data engineers, and DevOps engineers to ensure that the platform is properly configured, scaled, and maintained in a production environment.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/mlops_roles/machine_learning_platform_engineer.png" height=150 alt="Machine Learning Platform Engineer"&gt;  &lt;/p&gt;
&lt;p&gt;Some of the roles and responsibilities may overlap depending on the organization and the specific requirements of the project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Header image from &lt;a href="https://unsplash.com/photos/wJK9eTiEZHY"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@nataliepedigo"&gt;Natalie Pedigo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/><category term="devops"/><category term="machine-learning"/><category term="interview"/><category term="job"/><category term="job-search"/><category term="roles"/></entry><entry><title>50 Questions for MLOps Engineer Job Interview</title><link href="http://127.0.0.1:8000/50-questions-for-MLOps-engineer-job-interview/" rel="alternate"/><published>2022-11-02T00:00:00+01:00</published><updated>2023-01-12T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-11-02:/50-questions-for-MLOps-engineer-job-interview/</id><summary type="html">&lt;p&gt;Get ready for your next MLOps Engineer interview with our comprehensive list of 50+ questions. Covering topics like deployment, management, data pipeline, monitoring, and more.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://gumroad.com/"&gt;&lt;img alt="MLOps interview questions" src="https://safjan.com/images/mlop_interview_book_cover_3D_300px.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ksafjanuser.gumroad.com/l/mlops"&gt;Get for $2.99&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PDF, ePUB format EBook, no DRM&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;50 questions and answers&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stories from real projects&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;92 multiple choice quiz questions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;80 pages&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Machine learning has become an integral part of many industries, and as the use of machine learning models in production environments has increased, so has the need for MLOps (Machine Learning Operations). MLOps is a practice that combines the principles of DevOps with the unique requirements of machine learning to improve the speed, quality, and reliability of machine learning models in production.&lt;/p&gt;
&lt;p&gt;As the demand for MLOps engineers continues to grow, it's essential for companies to have a solid understanding of the skills and knowledge required for this role. This post will provide a set of interview questions that can help you evaluate the qualifications of potential MLOps engineers. The questions cover a wide range of topics, including MLOps best practices, model deployment and management, data management and pipeline, monitoring and troubleshooting, and more.&lt;/p&gt;
&lt;p&gt;Whether you're a hiring manager, a team lead, or an interviewer, these questions will help you identify the right candidates for your MLOps team and ensure that they have the skills and experience needed to succeed in this critical role.&lt;/p&gt;
&lt;p&gt;Please find below a set of 50 questions that can be used in the MLOps engineer job interview and 10 more for senior candidates.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can you explain the concept of MLOps and its importance in the industry?&lt;/li&gt;
&lt;li&gt;How do you approach the integration of machine learning models into a production environment?&lt;/li&gt;
&lt;li&gt;Can you walk me through a recent project you worked on that involved MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle version control for machine learning models?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with A/B testing or multi-armed bandit approaches?&lt;/li&gt;
&lt;li&gt;How do you monitor and troubleshoot machine learning models in production?&lt;/li&gt;
&lt;li&gt;Have you worked with any tools or platforms for MLOps, such as TensorFlow Serving, Kubernetes, or SageMaker?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with data drift and how you addressed it?&lt;/li&gt;
&lt;li&gt;How do you handle data privacy and security in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with hyperparameter tuning and optimization?&lt;/li&gt;
&lt;li&gt;How do you measure and improve the performance of machine learning models in production?&lt;/li&gt;
&lt;li&gt;Have you worked with any model interpretability or explainability tools?&lt;/li&gt;
&lt;li&gt;Can you walk me through your approach to testing and validation for machine learning models?&lt;/li&gt;
&lt;li&gt;How do you ensure the reproducibility of machine learning experiments?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with deploying machine learning models at scale?&lt;/li&gt;
&lt;li&gt;How do you handle rollbacks and roll forwards in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Have you worked with any automated machine learning (AutoML) tools?&lt;/li&gt;
&lt;li&gt;How do you manage the performance and resource usage of machine learning models in production?&lt;/li&gt;
&lt;li&gt;Can you discuss your experience with using containerization and virtualization technologies in MLOps?&lt;/li&gt;
&lt;li&gt;How do you stay current with the latest developments and trends in MLOps?&lt;/li&gt;
&lt;li&gt;Can you explain the concept of "feature store" and its role in MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle data labeling and annotation in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with deploying machine learning models on edge devices?&lt;/li&gt;
&lt;li&gt;How do you handle versioning and rollback of data sets in MLOps?&lt;/li&gt;
&lt;li&gt;Can you discuss a experience you have had with implementing continuous integration and delivery for machine learning models?&lt;/li&gt;
&lt;li&gt;How do you monitor and alert on machine learning model performance?&lt;/li&gt;
&lt;li&gt;Have you worked with any tools or platforms for model governance, such as MLFlow or ModelDB?&lt;/li&gt;
&lt;li&gt;Can you explain the concept of "canary deployment" and how it can be used in MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle model drift and retraining in production?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with using cloud-based platforms for MLOps, such as AWS SageMaker, GCP ML Engine, or Azure ML?&lt;/li&gt;
&lt;li&gt;How do you ensure the transparency and accountability of machine learning models in production?&lt;/li&gt;
&lt;li&gt;Can you discuss your experience with using Kubernetes or other container orchestration platforms in MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle data pipeline and feature engineering in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Have you worked with any tools or platforms for model explainability, such as SHAP or LIME?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with implementing A/B testing or multi-armed bandit approaches in production?&lt;/li&gt;
&lt;li&gt;How do you handle model deployments in multi-cloud or hybrid environments?&lt;/li&gt;
&lt;li&gt;Have you worked with any tools or platforms for model tracking and management, such as DataRobot or Algorithmia?&lt;/li&gt;
&lt;li&gt;Can you explain the concept of "dark launching" and how it can be used in MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle data lineage and traceability in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with implementing model monitoring and feedback loops?&lt;/li&gt;
&lt;li&gt;How do you handle model performance and scalability in production?&lt;/li&gt;
&lt;li&gt;Have you worked with any tools or platforms for model auditing and compliance, such as IBM AI Fairness 360 or Google What-If Tool?&lt;/li&gt;
&lt;li&gt;Can you discuss your experience with using serverless or FaaS (Function as a Service) in MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle data bias and fairness in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with using MLOps in regulated industries or environments?&lt;/li&gt;
&lt;li&gt;How do you handle model explainability and interpretability in production?&lt;/li&gt;
&lt;li&gt;Have you worked with any tools or platforms for model deployment and serving, such as TensorFlow Serving, Seldon, or Clipper?&lt;/li&gt;
&lt;li&gt;Can you explain the concept of "blue-green deployment" and how it can be used in MLOps?&lt;/li&gt;
&lt;li&gt;How do you handle data drift and concept drift in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Can you discuss a experience you have had with using MLOps in an Agile or DevOps environment?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are examples of questions for more advanced candidates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How do you handle distributed training and deployment of machine learning models in a multi-cloud environment?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with implementing auto-scaling for machine learning models in production?&lt;/li&gt;
&lt;li&gt;How do you handle model interpretability and explainability in an ensemble or multi-model setting?&lt;/li&gt;
&lt;li&gt;Can you discuss your experience with using machine learning on time-series data in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;How do you handle security and compliance for machine learning models in a regulated industry?&lt;/li&gt;
&lt;li&gt;Can you discuss an experience you have had with implementing reinforcement learning in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;How do you handle model interpretability and explainability for deep learning models?&lt;/li&gt;
&lt;li&gt;Can you discuss your experience with using machine learning in a distributed or edge computing environment?&lt;/li&gt;
&lt;li&gt;How do you handle data pipeline and feature engineering for time-series data in an MLOps pipeline?&lt;/li&gt;
&lt;li&gt;Can you discuss your experience with implementing federated learning in an MLOps pipeline?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="ebook-with-questions-and-answers"&gt;EBook with questions and answers&lt;/h2&gt;
&lt;p&gt;If you would like to prepare to answer these questions take look on my ebook that helps job-candidates to get MLOps job.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ksafjanuser.gumroad.com/l/mlops"&gt;&lt;img alt="MLOps interview questions" src="https://safjan.com/images/mlop_interview_book_cover_3D_300px.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gumroad.com/"&gt;Get for $2.99&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PDF, ePUB format EBook, no DRM&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;50 questions and answers&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stories from real projects&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;92 multiple choice quiz questions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;80 pages&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading image from &lt;a href="https://unsplash.com/photos/JaoVGh5aJ3E"&gt;unsplash&lt;/a&gt; @&lt;a href="https://unsplash.com/@amyhirschi"&gt;Amy Hirschi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/><category term="job"/><category term="job-search"/><category term="machine-learning"/><category term="interview"/></entry><entry><title>MLOps Roles of the Future</title><link href="http://127.0.0.1:8000/mlops_roles_of_the_future/" rel="alternate"/><published>2022-10-28T00:00:00+02:00</published><updated>2023-01-12T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-10-28:/mlops_roles_of_the_future/</id><summary type="html">&lt;p&gt;Discover the future of MLOps specializations, including Explainable AI/MLOps, Federated Learning/Edge MLOps, Reinforcement Learning/MLOps, AI/ML in IoT and IIoT, Model Explainability and Fairness.&lt;/p&gt;</summary><content type="html">&lt;p&gt;up::[[mlops]]&lt;/p&gt;
&lt;p&gt;As the field of MLOps is still relatively new and evolving, there are likely to be new specializations that will emerge in the future. Here are a few potential areas of specialization that may become more prominent in the future:&lt;/p&gt;
&lt;!-- MarkdownTOC levels='2,3' autolink=True autoanchor=True --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-explainable-aimlops"&gt;1.  Explainable AI/MLOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-federated-learningedge-mlops"&gt;2.  Federated Learning/Edge MLOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-reinforcement-learningmlops"&gt;3.  Reinforcement Learning/MLOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-aiml-in-iot-and-iiot"&gt;4.  AI/ML in IoT and IIoT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-model-explainability-and-fairness"&gt;5.  Model Explainability and Fairness&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="1-explainable-aimlops"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="1-explainable-aimlops"&gt;1.  Explainable AI/MLOps&lt;/h2&gt;
&lt;p&gt;As the use of machine learning models in critical decision-making applications increases, the need for explainable AI and interpretability will become more important. MLOps engineers with expertise in this area will be responsible for ensuring that models are transparent and accountable, and that their decisions can be understood and explained.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-federated-learningedge-mlops"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="2-federated-learningedge-mlops"&gt;2.  Federated Learning/Edge MLOps&lt;/h2&gt;
&lt;p&gt;Federated learning is a technique that allows multiple devices to train a model simultaneously while keeping data on device. Edge computing is becoming more prevalent, and as a result, MLOps engineers with expertise in federated learning and edge computing will be in high demand to ensure that models can be deployed and managed in these environments.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-reinforcement-learningmlops"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="3-reinforcement-learningmlops"&gt;3.  Reinforcement Learning/MLOps&lt;/h2&gt;
&lt;p&gt;Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions and observing the rewards/consequences. It is likely that more companies will start to use reinforcement learning in their products, so MLOps engineers with expertise in this area will be needed to ensure that these models can be deployed, monitored and maintained in a production environment.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-aiml-in-iot-and-iiot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="4-aiml-in-iot-and-iiot"&gt;4.  AI/ML in IoT and IIoT&lt;/h2&gt;
&lt;p&gt;Internet of things (IoT) and Industrial internet of things (IIoT) are growing rapidly. With this, there is a growing need to deploy and manage machine learning models on edge devices and gateways in order to analyze and act on data generated by connected devices. MLOps engineers with expertise in this area will be needed to ensure that models can be deployed and managed in these environments.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-model-explainability-and-fairness"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="5-model-explainability-and-fairness"&gt;5.  Model Explainability and Fairness&lt;/h2&gt;
&lt;p&gt;With the growing concern about bias and fairness in AI models, there is an increasing need for MLOps engineers with expertise in model explainability and fairness. They will be responsible for developing and implementing techniques to ensure that models are fair and transparent, and for identifying and mitigating sources of bias in the data and model.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="mlops"/><category term="devops"/><category term="machine-learning"/><category term="interview"/><category term="job"/><category term="job-search"/><category term="roles"/></entry><entry><title>Is MLOps a Good Career?</title><link href="http://127.0.0.1:8000/is-mlops-good-career/" rel="alternate"/><published>2022-10-22T00:00:00+02:00</published><updated>2023-01-12T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-10-22:/is-mlops-good-career/</id><summary type="html">&lt;p&gt;Is a career in MLOps right for you? Learn about the pros and cons of this growing field, including high demand, high earning potential, exciting work, and career growth opportunities.&lt;/p&gt;</summary><content type="html">&lt;h2 id="mlops-career"&gt;MLOps career&lt;/h2&gt;
&lt;p&gt;Machine learning is becoming an increasingly important field, and as a result, the role of MLOps is becoming more critical for organizations that are using machine learning in production environments. MLOps is a relatively new field, and as such, there is a high demand for skilled professionals in this area.&lt;/p&gt;
&lt;p&gt;A career in MLOps can be rewarding, as you will be working at the forefront of technology and will have the opportunity to work on cutting-edge projects. MLOps professionals are in high demand and can expect to have a lot of job opportunities available to them.&lt;/p&gt;
&lt;p&gt;Additionally, MLOps Engineers are typically well compensated and can expect to earn high salaries. According to Glassdoor, the average salary for an MLOps Engineer is around $120,000 per year.&lt;/p&gt;
&lt;p&gt;Overall, a career in MLOps can be a good choice if you have a strong background in machine learning and programming, and are interested in working in a fast-paced, dynamic environment. It’s a field that is growing quickly and offers many opportunities for growth and development.&lt;/p&gt;
&lt;!-- MarkdownTOC levels='2,3' autolink=True autoanchor=True --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-are-the-good-aspects-of-mlops-career"&gt;What are the good aspects of MLOps career?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-high-demand"&gt;1.  High demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-high-earning-potential"&gt;2. High earning potential&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-exciting-and-dynamic-work"&gt;3. Exciting and dynamic work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-impactful-work"&gt;4. Impactful work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-variety-of-roles-and-industries"&gt;5. Variety of roles and industries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-career-growth-opportunities"&gt;6.  Career growth opportunities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-continual-learning"&gt;7.  Continual learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-can-be-the-downsides-of-the-mlops-career"&gt;What can be the downsides of the MLops career?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-high-level-of-responsibility"&gt;1.  High level of responsibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-complexity"&gt;2.  Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-high-stress"&gt;3.  High stress&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-lack-of-standardization"&gt;4.  Lack of standardization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-difficulty-in-finding-the-right-team"&gt;5.  Difficulty in finding the right team&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-limited-career-mobility"&gt;6.  Limited career mobility&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="what-are-the-good-aspects-of-mlops-career"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-are-the-good-aspects-of-mlops-career"&gt;What are the good aspects of MLOps career?&lt;/h2&gt;
&lt;p&gt;There are multiple upsides of MLOps career, below, there are 7 of them listed and supported by arguments.&lt;/p&gt;
&lt;p&gt;&lt;a id="1-high-demand"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-high-demand"&gt;1.  High demand&lt;/h3&gt;
&lt;p&gt;As the use of machine learning in production environments increases, so does the need for MLOps professionals. This high demand means that MLOps professionals can expect to have a lot of job opportunities available to them.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-high-earning-potential"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-high-earning-potential"&gt;2. High earning potential&lt;/h3&gt;
&lt;p&gt;MLOps professionals are typically well compensated and can expect to earn high salaries. According to Glassdoor, the average salary for an MLOps Engineer is around $120,000 per year.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-exciting-and-dynamic-work"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-exciting-and-dynamic-work"&gt;3. Exciting and dynamic work&lt;/h3&gt;
&lt;p&gt;MLOps is a relatively new field and is constantly evolving, so you will have the opportunity to work on cutting-edge projects and technologies.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-impactful-work"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-impactful-work"&gt;4. Impactful work&lt;/h3&gt;
&lt;p&gt;MLOps plays a crucial role in ensuring the success of machine learning projects in production environments. By working in MLOps, you will be able to make a real impact by ensuring that machine learning models are deployed, monitored, and maintained effectively.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-variety-of-roles-and-industries"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-variety-of-roles-and-industries"&gt;5. Variety of roles and industries&lt;/h3&gt;
&lt;p&gt;MLOps can be applied in a wide range of industries such as healthcare, finance, manufacturing, retail, and more. This means that you will have a wide range of options for where you can work and what types of projects you can work on.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-career-growth-opportunities"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-career-growth-opportunities"&gt;6.  Career growth opportunities&lt;/h3&gt;
&lt;p&gt;MLOps is a rapidly growing field, and as such, there are many opportunities for career growth and development. As you gain more experience, you may be able to advance to higher-level roles such as team lead or manager.&lt;/p&gt;
&lt;p&gt;&lt;a id="7-continual-learning"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="7-continual-learning"&gt;7.  Continual learning&lt;/h3&gt;
&lt;p&gt;As the field of MLOps is continuously evolving and new technologies are emerging, MLOps professionals will have opportunities to continue learning and expanding their skills.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-can-be-the-downsides-of-the-mlops-career"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-can-be-the-downsides-of-the-mlops-career"&gt;What can be the downsides of the MLops career?&lt;/h2&gt;
&lt;p&gt;While a career in MLOps can be very rewarding, there are also some downsides to consider. Please note that these are just potential downsides or challenges and you don't need to face them in your MLOps career.&lt;/p&gt;
&lt;p&gt;&lt;a id="1-high-level-of-responsibility"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-high-level-of-responsibility"&gt;1.  High level of responsibility&lt;/h3&gt;
&lt;p&gt;MLOps professionals are responsible for ensuring that machine learning models are deployed, monitored, and maintained effectively in production environments. This can be a high-pressure role, as the success of the machine learning project relies heavily on the work of the MLOps team.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-complexity"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-complexity"&gt;2.  Complexity&lt;/h3&gt;
&lt;p&gt;MLOps is a complex field that requires a strong background in machine learning and programming, as well as experience with deployment, scaling, and monitoring. This complexity can make the field difficult to navigate for those without the right background or experience.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-high-stress"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-high-stress"&gt;3.  High stress&lt;/h3&gt;
&lt;p&gt;MLOps is a high-pressure field with tight deadlines and high stakes. This can be stressful for some professionals and may not be suitable for those who don't enjoy working under pressure.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-lack-of-standardization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-lack-of-standardization"&gt;4.  Lack of standardization&lt;/h3&gt;
&lt;p&gt;MLOps is still a relatively new field and there is a lack of standardization in terms of best practices, tools, and methodologies. This can make it challenging to know how to approach specific problems and can make it hard to compare the performance of different teams.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-difficulty-in-finding-the-right-team"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-difficulty-in-finding-the-right-team"&gt;5.  Difficulty in finding the right team&lt;/h3&gt;
&lt;p&gt;MLOps requires collaboration between different teams such as machine learning engineers, data engineers, data scientists, DevOps engineers, and model governance. Finding the right team can be difficult and may take time.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-limited-career-mobility"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-limited-career-mobility"&gt;6.  Limited career mobility&lt;/h3&gt;
&lt;p&gt;MLOps is a highly specialized field, and as such, it can be difficult to transition to other roles or industries if you decide to leave the field.&lt;/p&gt;
&lt;p&gt;It's important to keep in mind that while these downsides exist, they do not necessarily mean that a career in MLOps is not right for you. It's always recommended to weigh the pros and cons and see if it's the right fit for you and your goals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading image from &lt;a href="https://unsplash.com/photos/dmLIDt7xZNA"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@allecgomes"&gt;Allec Gomes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="MLOps"/><category term="mlops"/><category term="devops"/><category term="machine-learning"/><category term="interview"/><category term="job"/><category term="job-search"/><category term="roles"/><category term="career"/></entry><entry><title>Visual Text Exploration as Part of Preprocessing Before Classification</title><link href="http://127.0.0.1:8000/visual-text-exploration-as-part-of-preprocessing-before-classification/" rel="alternate"/><published>2022-10-11T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-10-11:/visual-text-exploration-as-part-of-preprocessing-before-classification/</id><summary type="html">&lt;p&gt;This post discusses importance of visual text exploration in preprocessing for classification, covers techniques (wordcloud, Sentiment Analysis, topic modeling, data cleaning) &amp;amp; how to use them with popular libraries. Encourages readers to try for own projects.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The process of text classification is a common task in natural language processing and machine learning. In order to classify text data, it's essential to preprocess the data first. One of the key aspects of preprocessing is to explore the data in order to understand its characteristics, identify patterns and outliers, and determine which techniques and methods should be used to clean and prepare the data for classification.&lt;/p&gt;
&lt;p&gt;Visual text exploration is a powerful tool that can be used to gain insights and understanding of text data. By visualizing the data in different ways, it's possible to detect patterns and identify trends that might not be immediately apparent from just reading through the data. Additionally, visual text exploration can also bring ideas for processing actions that need to be taken on the dataset.&lt;/p&gt;
&lt;p&gt;In this blog post, we will analyze modern tools that can be used for visual exploration of text data and provide generic ideas for textual data inspection that can be applied in many text-exploration tasks. We will cover techniques such as wordcloud, Sentiment Analysis and topic modeling which are widely used by researchers to analyze text data and extract insights. Additionally, we will also cover textual data cleaning and it's role in preprocessing of text data.&lt;/p&gt;
&lt;h2 id="textual-data-cleaning"&gt;Textual data cleaning&lt;/h2&gt;
&lt;p&gt;Textual data cleaning is an essential step in the preprocessing of text data before classification. The goal of data cleaning is to prepare the data so that it can be easily understood and analyzed by machine learning algorithms.&lt;/p&gt;
&lt;p&gt;There are multiple techniques such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removing stop-words&lt;/li&gt;
&lt;li&gt;stemming and lematization&lt;/li&gt;
&lt;li&gt;Regular expressions to clean text data by removing special characters and unwanted elements, such as URLs or email addresses.&lt;/li&gt;
&lt;li&gt;Text normalization - converting all the text to lowercase, removing punctuation and converting numbers to words.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Textual data cleaning will be not discussed in details in this article.&lt;/p&gt;
&lt;h2 id="textual-data-exploration-with-sentiment-analysis"&gt;Textual Data Exploration with Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is a method of determining the emotional tone of a piece of text, which can be useful for identifying patterns in text data. Sentiment analysis can be used to classify text as positive, negative, or neutral based on the words and phrases used in the text.&lt;/p&gt;
&lt;p&gt;The process of sentiment analysis typically involves analyzing a piece of text and determining the presence and strength of various sentiment-bearing words or phrases. The text is then classified as having a positive, negative or neutral sentiment.&lt;/p&gt;
&lt;p&gt;Sentiment analysis can be applied in a wide range of applications, such as customer feedback analysis, social media monitoring, and opinion mining. The output of sentiment analysis can help in identifying patterns in the data such as customer preferences, brand reputation and product feedback.&lt;/p&gt;
&lt;p&gt;There are a variety of libraries and packages available for performing sentiment analysis. Some of the popular libraries are &lt;code&gt;NLTK&lt;/code&gt;, &lt;code&gt;TextBlob&lt;/code&gt;, &lt;code&gt;VaderSentiment&lt;/code&gt;, and &lt;code&gt;Pattern&lt;/code&gt;. Each library has its own unique features, such as NLTK has a comprehensive list of common english words to remove and TextBlob has a built in sentiment analysis function.&lt;/p&gt;
&lt;p&gt;For example, using NLTK library, you can perform sentiment analysis using the &lt;code&gt;VaderSentimentIntensityAnalyzer&lt;/code&gt; by creating an object of it and passing your text to the polarity_scores method, which returns a dictionary containing scores for each sentiment-negative, neutral and positive. With TextBlob, you can directly use the sentiment method, which returns a named tuple of form (polarity, subjectivity ) where polarity is a float within the range [-1.0, 1.0] and subjectivity is a float within the range [0.0, 1.0].&lt;/p&gt;
&lt;p&gt;Note that sentiment analysis is a complex task and the results can be affected by the language and context of the text. However, with the right tools and techniques, sentiment analysis can be a powerful way to gain insights and identify patterns in text data.&lt;/p&gt;
&lt;h2 id="textual-data-exploration-with-topic-modeling"&gt;Textual Data Exploration with topic modeling&lt;/h2&gt;
&lt;p&gt;Topic modeling is a method of identifying the main topics or themes present in a large collection of text data. It can be used to uncover the hidden structure in the data and to understand the main concepts or ideas that are being discussed. Topic modeling can be used to classify text into predefined topics or to discover new and previously unknown topics.&lt;/p&gt;
&lt;p&gt;The most common technique used in topic modeling is Latent Dirichlet Allocation (LDA). LDA is a probabilistic model that aims to uncover the underlying topics present in a corpus of text data by assuming that each document is a mixture of a small number of latent topics. It is a generative model that describes how a set of observations is generated by a set of latent variables.&lt;/p&gt;
&lt;p&gt;There are many libraries and packages available that can be used to perform topic modeling, some of the most popular ones are &lt;a href="https://radimrehurek.com/gensim/"&gt;Gensim&lt;/a&gt;, &lt;a href="http://mallet.cs.umass.edu/index.php"&gt;Mallet&lt;/a&gt; and &lt;a href="https://cran.r-project.org/web/packages/LDAvis/index.html"&gt;LDAvis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, Gensim is a python library that provides an implementation of LDA. It allows to create a corpus of text data, create a bag of words and then generate a set of topics using the LDA model. Gensim also provides functionality to evaluate the quality of the generated topics and the ability to visualize the topics using pyLDAvis.&lt;/p&gt;
&lt;p&gt;Mallet, which is a Java-based library, can also be used for topic modeling. It provides additional functionality for visualizing topics as well as tools for evaluating the quality of the generated topics.&lt;/p&gt;
&lt;p&gt;LDAvis is a R package that provides interactive visualizations of LDA models. It can be used to extract insights from the model, explore the generated topics and get a sense of the overall structure of the data.&lt;/p&gt;
&lt;p&gt;Topic modeling can be a powerful tool for uncovering the hidden structure in large collections of text data. It can be used to classify text into predefined topics or to discover new and previously unknown topics. With the right tools and techniques, topic modeling can be a powerful way to gain insights and identify patterns in text data.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Visual text exploration is a powerful tool that can be used to gain insights and understanding of text data as part of preprocessing before classification. We have seen how techniques such as wordcloud, Sentiment Analysis, topic modeling, and textual data cleaning can be used to explore text data and extract insights.&lt;/p&gt;
&lt;p&gt;Visual text exploration allows us to detect patterns and identify trends that might not be immediately apparent from just reading through the data. Additionally, these techniques can also bring ideas for processing actions that need to be taken on the dataset.&lt;/p&gt;
&lt;p&gt;Note that the techniques discussed in this post are just a starting point and there are many other tools and techniques that can be used for visual text exploration. Furthermore, the best approach will depend on the specific dataset and problem you are working on.&lt;/p&gt;
&lt;p&gt;I encourage readers to try out the tools and techniques covered in this post and to use them in their own text exploration projects. With the right tools and techniques, you can gain valuable insights and improve the performance of text classification models.&lt;/p&gt;
&lt;h2 id="references-and-additional-resources"&gt;References and Additional Resources&lt;/h2&gt;
&lt;p&gt;Here are some references and additional resources that readers can use to learn more about visual text exploration and related topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;"Text Mining and Analysis: Practical Methods, Examples, and Case Studies Using SAS" by Michael J. A. Berry and Gordon S. Linoff&lt;/li&gt;
&lt;li&gt;"Python Text Processing with NLTK 2.0 Cookbook" by Jacob Perkins&lt;/li&gt;
&lt;li&gt;"Topic Modeling with Gensim (Python)" by Shilpa Arora, &lt;a href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"&gt;https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;"Topic Modeling with Mallet" by J. Stephen Downie, &lt;a href="http://mallet.cs.umass.edu/topics.php"&gt;http://mallet.cs.umass.edu/topics.php&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;"Interactive topic model visualization" by Carson Sievert, &lt;a href="https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf"&gt;https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;"NLTK Sentiment Analysis" by "Natural Language Processing with Python" by Steven Bird, Ewan Klein, and Edward Loper, &lt;a href="http://www.nltk.org/howto/sentiment.html"&gt;http://www.nltk.org/howto/sentiment.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;"TextBlob: Simplified Text Processing" by Steven Loria, &lt;a href="http://textblob.readthedocs.io/en/dev/"&gt;http://textblob.readthedocs.io/en/dev/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you have any further questions or would like to learn more about visual text exploration, I recommend consulting these resources as they provide detailed information on various techniques and tools that can be used to explore text data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Header graphics from &lt;www.wordle.net&gt;, &lt;a href="https://commons.wikimedia.org/wiki/User:Ragettho" title="User:Ragettho"&gt;User:Ragettho&lt;/a&gt; found on &lt;a href="https://commons.wikimedia.org/wiki/File:Wikinews_word_cloud.jpg"&gt;wikimedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="NLP"/><category term="python"/></entry><entry><title>10 Lesser-Known, Yet Powerful Python Plotting Libraries</title><link href="http://127.0.0.1:8000/lesser-known-yet-powerful-python-plotting-libraries/" rel="alternate"/><published>2022-09-30T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-09-30:/lesser-known-yet-powerful-python-plotting-libraries/</id><summary type="html">&lt;p&gt;The most widely used libraries for plotting in python are matplotlib, Plotly, seaborn, and bokeh. This article describes 10 other powerful plotting libraries available in Python that can be used to create high-quality plots and visualizations.&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are a few widely used plotting libraries used in python projects: &lt;a href="https://matplotlib.org/"&gt;matplotlib&lt;/a&gt;, &lt;a href="https://plotly.com/python/"&gt;plotly&lt;/a&gt;, &lt;a href="https://seaborn.pydata.org/"&gt;seaborn&lt;/a&gt; , and &lt;a href="https://docs.bokeh.org/en/latest/index.html"&gt;Bokeh&lt;/a&gt;. However, the python ecosystem is not limited to these libraries. Many powerful and lesser-known plotting libraries in Python can be used to create high-quality plots and visualizations.&lt;/p&gt;
&lt;!-- MarkdownTOC levels='2,3' autolink=True --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-holoviews-"&gt;1. HoloViews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-bqplot-"&gt;2. Bqplot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-plotnine"&gt;3. Plotnine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-geoplotlib-"&gt;4. Geoplotlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-pygal-"&gt;5. Pygal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-vincent-"&gt;6. Vincent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-pyngl"&gt;7. PyNGL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-ggplot-"&gt;8. Ggplot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-pyqtgraph-"&gt;9. PyQtGraph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#10-pyqwt"&gt;10. PyQwt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#11-lets-plot"&gt;11. Lets-plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#plotly-express-"&gt;Plotly Express&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;h3 id="1-holoviews"&gt;1. &lt;a href="https://holoviews.org/"&gt;HoloViews&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/ioam/holoviews.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://holoviews.org/_static/logo_horizontal.png" style="max-height: 100px;" alt="HoloViews logo"&gt;&lt;/p&gt;
&lt;p&gt;A library that makes it easy to create and interact with data visualizations in Python, using a high-level interface. The high-level interface refers to the library's ease of use and user-friendly design for creating and interacting with data visualizations in Python. This interface is designed to abstract away some of the complexities of the underlying plotting and data manipulation, making it simpler for users to create and customize their visualizations with minimal code.
By default, HoloViews plots with Bokeh but has extensions for matplotlib and plotly.
&lt;a href="https://holoviews.org/gallery/index.html"&gt;Gallery - HoloViews v1.15.3&lt;/a&gt;
&lt;a href="https://holoviews.org/reference/index.html"&gt;Reference Gallery - HoloViews v1.15.3&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-bqplot"&gt;2. &lt;a href="https://github.com/bqplot/bqplot"&gt;Bqplot&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/bloomberg/bqplot.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;Bqplot is a Python interactive data visualization library developed by researchers at Bloomberg. This is a 2D data visualization library based on the Jupyter interactive widget framework, which allows you to create interactive plots that can be easily embedded in notebooks.&lt;/p&gt;
&lt;h3 id="3-plotnine"&gt;3. &lt;a href="https://plotnine.readthedocs.io/en/stable/"&gt;Plotnine&lt;/a&gt;&lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/has2k1/plotnine.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://plotnine.readthedocs.io/en/stable/_images/logo-540.png" style="max-height: 100px;" alt="Plotnine logo"&gt;&lt;/p&gt;
&lt;p&gt;Plotnine is a data visualization library for Python, based on the popular data visualization package ggplot2 for R programming language.&lt;/p&gt;
&lt;p&gt;It is designed to mimic the syntax and structure of ggplot2, making it easy for R users to transition to using Python for data visualization. It provides a powerful and flexible way to create plots and charts by using a "grammar of graphics" approach, where the user can build plots by combining different elements such as layers, scales, and geoms.&lt;/p&gt;
&lt;p&gt;The library provides a wide range of visualization options and customization capabilities for creating plots and charts, including bar plots, line plots, scatter plots, box plots, heat maps, and many more.&lt;/p&gt;
&lt;p&gt;It is built on top of other libraries such as pandas and matplotlib, it allow to make complexe graphics with simple code. Additionally, plotnine supports a variety of input data formats, such as pandas dataframe, numpy arrays, and more. It also supports different types of output formats such as static image files and interactive web-based plots.&lt;/p&gt;
&lt;p&gt;In short, plotnine is a great library for creating powerful, flexible, and customizable data visualizations in Python and it is designed for those familiar with R ggplot2 package.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://plotnine.readthedocs.io/en/stable/gallery.html"&gt;Gallery - plotnine 0.10.1 documentation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-geoplotlib"&gt;4. &lt;a href="https://github.com/andrea-cuttone/geoplotlib"&gt;Geoplotlib&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/andrea-cuttone/geoplotlib.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;A library for creating geospatial visualizations, which can be used to create maps, choropleths, and other types of geospatial plots.&lt;/p&gt;
&lt;h3 id="5-pygal"&gt;5. &lt;a href="https://www.pygal.org/en/stable/"&gt;Pygal&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/Kozea/pygal.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;Pygal is a library for creating SVG charts and graphs, which provides a wide range of chart types and a simple interface for creating interactive plots.&lt;/p&gt;
&lt;h3 id="6-vincent"&gt;6. &lt;a href="https://vincent.readthedocs.io/en/latest/index.html"&gt;Vincent&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/wrobstory/vincent.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;The concept behind this library is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The data capabilities of Python. The visualization capabilities of JavaScript.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Vincent allows you to build Vega specifications in a Pythonic way and performs type-checking to help ensure that your specifications are correct. It also has a number of convenience chart-building methods that quickly turn Python data structures into Vega visualization grammar, enabling graphical exploration. It allows for quick iteration of visualization designs via getters and setters on grammar elements, and outputs the final visualization to JSON.&lt;/p&gt;
&lt;p&gt;Perhaps most importantly, Vincent has Pandas-Fu, and is built specifically to allow for quick plotting of DataFrames and Series.
&lt;a href="https://vincent.readthedocs.io/en/latest/charts_library.html"&gt;Charts Library - Vincent 0.4 documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;NOTE: The repository for vincent has been archived by the owner. It is now read-only. There is no active development ongoing on original repo.&lt;/p&gt;
&lt;h3 id="7-pyngl"&gt;7. &lt;a href="https://www.pyngl.ucar.edu/Examples/"&gt;PyNGL&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A library for creating visualizations of scientific data, which provides a wide range of plotting and visualization options for working with multidimensional data.
&lt;a href="https://www.pyngl.ucar.edu/Examples/gallery.shtml"&gt;PyNGL Graphical Gallery&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="8-ggplot"&gt;8. &lt;a href="https://ggplot2.tidyverse.org/reference/index.html"&gt;Ggplot&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/tidyverse/ggplot2.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://ggplot2.tidyverse.org/logo.png" style="max-height: 100px;" alt="ggplot logo"&gt;
A library for creating plots using the Grammar of Graphics, which provides a high-level interface for creating a wide range of statistical plots.&lt;/p&gt;
&lt;h3 id="9-pyqtgraph"&gt;9. &lt;a href="http://www.pyqtgraph.org/documentation/examples.html"&gt;PyQtGraph&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/pyqtgraph/pyqtgraph.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://pyqtgraph.readthedocs.io/en/latest/_static/peegee_02.svg" style="max-height: 100px;" alt="PyQtGraph logo"&gt;
&lt;strong&gt;PyQtGraph&lt;/strong&gt; is a pure-python graphics and GUI library built on &lt;a href="http://www.riverbankcomputing.co.uk/software/pyqt/intro"&gt;PyQt&lt;/a&gt; / &lt;a href="http://www.pyside.org/"&gt;PySide&lt;/a&gt; and &lt;a href="http://www.numpy.org/"&gt;numpy&lt;/a&gt;. It is intended for use in mathematics / scientific / engineering applications. Despite being written entirely in python, the library is very fast due to its heavy leverage of NumPy for number crunching and &lt;a href="https://doc.qt.io/qt-5/qgraphicsview.html"&gt;Qt's GraphicsView framework&lt;/a&gt; for fast display&lt;/p&gt;
&lt;h3 id="10-pyqwt"&gt;10. &lt;a href="https://pyqwt.sourceforge.net/"&gt;PyQwt&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A library for creating 2D and 3D plots and visualizations, which is built on top of PyQt and provides a wide range of plotting and visualization options.
PyQwt is a set of Python bindings for the &lt;a href="http://qwt.sourceforge.net/"&gt;Qwt&lt;/a&gt; C++ class library which extends the &lt;a href="http://www.trolltech.com/"&gt;Qt&lt;/a&gt; framework with widgets for scientific and engineering applications. It provides a widget to plot 2-dimensional data and various widgets to display and control bounded or unbounded floating point values.&lt;/p&gt;
&lt;p&gt;PyQwt addresses the problem of integrating &lt;a href="http://www.riverbankcomputing.co.uk/pyqt"&gt;PyQt&lt;/a&gt;, Qt, Qwt, &lt;a href="http://numpy.scipy.org/"&gt;NumPy&lt;/a&gt; and optionally &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt;. Look at the &lt;a href="https://pyqwt.sourceforge.net/cli-examples.html"&gt;Command Line Interface (CLI) examples&lt;/a&gt; and the &lt;a href="https://pyqwt.sourceforge.net/gui-examples.html"&gt;Graphical User Interface (GUI) examples&lt;/a&gt; to get an idea of what you can do with PyQwt.&lt;/p&gt;
&lt;h3 id="11-lets-plot"&gt;11. &lt;a href="https://lets-plot.org/"&gt;Lets-plot&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Lets-Plot  is a multiplatform plotting library based on the Grammar of Graphics. It's based on ggplot2-like grammar-of-graphics language. See examples here: &lt;a href="https://lets-plot.org/pages/charts.html"&gt;https://lets-plot.org/pages/charts.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="plotly-express"&gt;&lt;a href="https://plotly.com/python/plotly-express/"&gt;Plotly Express&lt;/a&gt; &lt;img alt="GitHub stars shield" src="https://img.shields.io/github/stars/plotly/plotly_express.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://images.plot.ly/logo/new-branding/plotly-logomark.png" style="max-height: 100px;" alt="Plotly Express logo"&gt;
It is not a new plotting library but a high-level interface to Plotly, which allows you to create interactive plots and visualizations with minimal code.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;X::[[Blog_post_ideas/plotting_histogram_line_scatter_plot_with_5_python_libs|plotting_histogram_line_scatter_plot_with_5_python_libs]]
X::[[Blog_post_ideas/best/plotting_histogram_line_scatter_plot_with_5_python_libs|plotting_histogram_line_scatter_plot_with_5_python_libs]]&lt;/p&gt;</content><category term="Howto"/><category term="holoviews"/><category term="bokeh"/><category term="matplotlib"/><category term="plotly"/><category term="bqplot"/><category term="data-visualization"/><category term="jupyter"/><category term="pandas"/><category term="ggplot2"/><category term="grammar-of-graphics"/><category term="plotnine"/><category term="bar-plots"/><category term="line-plots"/><category term="scatter-plots"/><category term="box-plots"/><category term="heat-maps"/><category term="geoplotlib"/><category term="geospatial-visualization"/><category term="choropleths"/><category term="pygal"/><category term="SVG"/><category term="charts"/><category term="graphs"/><category term="vega"/><category term="python-data-structures"/><category term="pandas-fu"/><category term="pynl"/><category term="scientific"/><category term="data"/><category term="multidimensional"/><category term="data"/><category term="ggplot2"/><category term="tidyverse"/><category term="python"/></entry><entry><title>Automated Signal Segmentation, Trend Detection, and Classification</title><link href="http://127.0.0.1:8000/automated-trend-detection-and-signal-segmentation/" rel="alternate"/><published>2022-09-12T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-09-12:/automated-trend-detection-and-signal-segmentation/</id><summary type="html">&lt;p&gt;This post presents the trend-classifier package that can be used for signal segmentation into parts where the trend is coherent.&lt;/p&gt;</summary><content type="html">&lt;h2 id="problem-statement"&gt;Problem statement&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Partition time series into segments, where the signal in the segment has a consistent trend (e.g. up-trend, down-trend)&lt;/li&gt;
&lt;li&gt;Characterise trend in the segment, e.g. provide coefficients of the equation describing the trend in the segment.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="windowing-approach"&gt;Windowing Approach&lt;/h2&gt;
&lt;p&gt;The discussed solution is based on the concept of analyzing signals using a sliding window with a fixed length. The consecutive windows can overlap.&lt;/p&gt;
&lt;p&gt;&lt;img alt="trend lines within the window" src="/images/trend_segmentation/trend_in_windows.jpg"&gt;
&lt;strong&gt;Figure 1. Windowing approach for trend detection - trend line fit to datapoints in each overlapping window.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For each window perform linear regression to find the line that best fits the signal. If the parameters of a linear regression between two windows do not differ too much, the signal covered by these two windows is considered as belonging to the same segment with a coherent trend.&lt;/p&gt;
&lt;h2 id="example"&gt;Example&lt;/h2&gt;
&lt;p&gt;Let's say, we wanted to do segmentation of the time series on segments with similar trends. For that task, you can use &lt;a href="https://pypi.org/project/trend-classifier/"&gt;trend-classifier&lt;/a&gt; Python library. It is pip installable (&lt;code&gt;pip3 install trend-classifier&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Here is an example that gets the time series data from YahooFinance and performs the analysis.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yfinance&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;yf&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;trend_classifier&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Segmenter&lt;/span&gt;

&lt;span class="c1"&gt;# download the data from yahoo finance&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AAPL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;2018-09-15&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;2022-09-05&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;progress&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x_in&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y_in&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Adj Close&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;seg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Segmenter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_in&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_in&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;calculate_segments&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, you can plot the time series with trend lines and segment boundaries with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_segments&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="segmentation example" src="/images/trend_segmentation/screenshoot_1.jpg"&gt;
&lt;strong&gt;Figure 2. Visualization of the signal segmentation based on the trend.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can inspect details about each segment (e.g. positive value for slope indicates an up-trend and a negative down-trend). To see info about the segment with index &lt;code&gt;3&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;devtools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;debug&lt;/span&gt;
&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can have information about all segments in tabular form using &lt;code&gt;Segmenter.segments.to_dataframe()&lt;/code&gt; method which produces Pandas DataFrame.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_dataframe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="controlling-generalization"&gt;Controlling generalization&lt;/h2&gt;
&lt;p&gt;There is a parameter that controls the "generalization" factor, i.e. you can try to fit a trend line to a smaller range of time series - you will end up with a large number of segments, or you can go for the segments spanning a bigger part of the time series (more general trend line) and end up with a time series divided into fewer segments. To control that behavior, when initializing &lt;code&gt;Segmenter()&lt;/code&gt; (e.g. &lt;code&gt;Segmenter(x_in, y_in, n=20)&lt;/code&gt; use various values for &lt;code&gt;n&lt;/code&gt; parameter. The larger &lt;code&gt;n&lt;/code&gt; the generalization is stronger (fewer segments).&lt;/p&gt;
&lt;p&gt;&lt;img alt="segmentation for n=20" src="../images/trend_segmentation/segments_n_20.jpg"&gt;
&lt;strong&gt;Figure 3. Signal segmentation with fine granularity (weak generalization), n=20.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="segmentation for n=80" src="/images/trend_segmentation/segments_n_80.jpg"&gt;
&lt;strong&gt;Figure 4. Signal segmentation with rough granularity (strong generalization), n=80.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="an-exemplary-application-of-trend-detection-and-segmentation"&gt;An exemplary application of trend detection and segmentation&lt;/h2&gt;
&lt;p&gt;This tool can be used to extract parts of the signal where the trend has given parameters. E.g. extract segments with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;up-trend&lt;/li&gt;
&lt;li&gt;down-trend&lt;/li&gt;
&lt;li&gt;the signal is in a horizontal channel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can write a very basic classifier based on the slope parameter. If the slope is positive, or higher than the threshold value, consider this segment with an up-trend. There are also separate rules for down-trend and horizontal-trend types.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;segment_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;segment&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;up&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;segment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;th&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;down&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;horiz&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we can use this function to get indices of trends in a given type, and plot them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;up_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segment_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;up&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;down_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segment_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;down&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;horiz_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segment_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;horiz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;up_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="up-trend" src="/images/trend_segmentation/uptrend.jpg"&gt;
&lt;strong&gt;Figure 5. Segments classified as "up-trend".&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;down_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="down-trend" src="/images/trend_segmentation/downtrend.jpg"&gt;
&lt;strong&gt;Figure 6. Segments classified as "down-trend".&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;horiz_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="horizontal-trend" src="/images/trend_segmentation/horiz_trend.jpg"&gt;
&lt;strong&gt;Figure 7. Segments classified as "horizontal-trend".&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The classification function used in the example was very simple and one can implement a more robust function e.g. one, that uses other than &lt;code&gt;slope&lt;/code&gt; data stored in the segment object.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;This article describes the &lt;a href="https://pypi.org/project/trend-classifier/"&gt;trend-classifier&lt;/a&gt; library that is using the calculation of linear regression within the overlapping windows for signal segmentation. If the parameters of the regression for the following windows are similar, then the windows are considered as belonging to the same trend and the segment is extended by the newly analyzed window and the operation is continued for the next windows. When using this tool you need to know the limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there is a trade-off between the efficiency (best fit) and the generalization (number of segments) - the smaller window size (parameter &lt;code&gt;n&lt;/code&gt; of the &lt;code&gt;Segmenter()&lt;/code&gt;) the better fit but also more resulting segments and less generalization.&lt;/li&gt;
&lt;li&gt;arbitrary (but configurable) threshold values are used to determine if the window is a continuation of the trend from the previous window or if it should be a new segment with a different trend)&lt;/li&gt;
&lt;li&gt;since the criterion for starting a new segment is a "significant" difference between two consecutive windows, one can imagine that if the trend is changing slowly, with small changes e.g. uptrend can change to downtrend, and no trend change will be detected.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="trend"/><category term="segmentation"/><category term="algotrading"/></entry><entry><title>The Best Self-Hosted, Open Source RSS Feed Readers in 2022</title><link href="http://127.0.0.1:8000/the-best-self-hosted-rss-feed-readers-in-2022/" rel="alternate"/><published>2022-08-31T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-08-31:/the-best-self-hosted-rss-feed-readers-in-2022/</id><summary type="html">&lt;p&gt;Best self-hosted RSS Feed Readers selected by popularity and project activity.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-rss"&gt;What is RSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#actively-developed-and-popular-self-hosted-feed-readers"&gt;Actively developed and popular self-hosted Feed readers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#newsblur"&gt;NewsBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tiny-tiny-rss"&gt;Tiny Tiny RSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#freshrss"&gt;FreshRSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#miniflux"&gt;miniflux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stringer"&gt;Stringer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#selfoss"&gt;selfoss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#commafeed"&gt;CommaFeed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#yarr"&gt;yarr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#runners-up"&gt;Runners-up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rssmonster"&gt;RSSMonster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sismics-reader"&gt;Sismics Reader&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;X::[[freshrss_deploy_on_azure]]
X::[[fresh_rss_deploy_on_GCP]]&lt;/p&gt;
&lt;p&gt;&lt;a id="what-is-rss"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-is-rss"&gt;What is RSS&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RSS&lt;/strong&gt; (&lt;strong&gt;Really Simple Syndication&lt;/strong&gt;) is a web feed that allows users and applications to access updates to websites in a standardized, computer-readable format. Subscribing to RSS feeds can allow a user to keep track of many different websites in a single news aggregator, which constantly monitors sites for new content, removing the need for the user to manually check them.&lt;/p&gt;
&lt;p&gt;Websites usually use RSS feeds to publish frequently updated information, such as blog entries, news headlines, episodes of audio and video series, or for distributing podcasts. An RSS document (called "feed", "web feed", or "channel") includes full or summarized text, and metadata, like publishing date and author's name. RSS formats are specified using a generic XML file.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(source: Wikipedia)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Did you know?&lt;/strong&gt;
In 2000, at the age of 14, &lt;a href="https://www.rollingstone.com/culture/culture-news/the-brilliant-life-and-tragic-death-of-aaron-swartz-177191/?sub_action=logged_in"&gt;Aaron Swartz&lt;/a&gt; co-authored RSS version 1.0, and shortly thereafter joined a working group at the World Wide Web Consortium to help develop common data formats used on the World Wide Web.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="actively-developed-and-popular-self-hosted-feed-readers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="actively-developed-and-popular-self-hosted-feed-readers"&gt;Actively developed and popular self-hosted Feed readers&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/awesome-selfhosted/awesome-selfhosted#feed-readers"&gt;Feed Readers&lt;/a&gt; section of &lt;a href="https://github.com/awesome-selfhosted/awesome-selfhosted#feed-readers"&gt;Awesome Self-Hosted&lt;/a&gt; mention 30 projects that provide functionality related to RSS. The search of the GitHub looking for &lt;code&gt;rss-reader&lt;/code&gt; tag This article aims to select those actively developed and gained popularity measured by the number of GitHub stars. The criteria for selecting the best readers were that the project is actively developed - &lt;strong&gt;the last commit within the last 6 months&lt;/strong&gt; and &lt;strong&gt;has more than 1k+ stars.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="newsblur"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="newsblur"&gt;&lt;a href="https://www.newsblur.com/"&gt;NewsBlur&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/samuelclay/NewsBlur.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;NewsBlur is a personal news reader bringing people together to talk about the world&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="NewsBlur screenshoot" src="https://www.newsblur.com/media/img/welcome/feature_1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="tiny-tiny-rss"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="tiny-tiny-rss"&gt;&lt;a href="https://git.tt-rss.org/"&gt;Tiny Tiny RSS&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Tiny Tiny RSS is a free and open source web-based news feed (RSS/Atom) reader and aggregator&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Tiny Tiny RSS screenshoot" src="https://tt-rss.org/images/ttrss/21.03/Screenshot%202021-03-10%20152046.webp"&gt;
NOTE: While Tiny Tiny RSS is popular and has many plugins, official build is only for the amd64 architecture.&lt;/p&gt;
&lt;p&gt;&lt;a id="freshrss"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="freshrss"&gt;&lt;a href="https://github.com/FreshRSS/FreshRSS"&gt;FreshRSS&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/FreshRSS/FreshRSS.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;FreshRSS is a self-hosted RSS feed aggregator.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you want to get look &amp;amp; feel of FreshRSS there is &lt;a href="https://demo.freshrss.org/i/?rid=657dd0c01d6eb"&gt;demo&lt;/a&gt; available.
&lt;img alt="screenshoot" src="https://github.com/FreshRSS/FreshRSS/raw/edge/docs/img/FreshRSS-screenshot.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="miniflux"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="miniflux"&gt;&lt;a href="https://miniflux.app/"&gt;miniflux&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/miniflux/v2.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Miniflux is a minimalist and opinionated feed reader, Written in Go, It's simple, fast, lightweight and super easy to install.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="miniflux screenshoot" src="https://miniflux.app/images/overview.png"&gt;&lt;/p&gt;
&lt;p&gt;Here is information on Miniflux Docker/docker-compose installation: &lt;a href="https://miniflux.app/docs/dacker.html"&gt;Miniflux Installation with Docker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="stringer"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="stringer"&gt;&lt;a href="https://github.com/stringer-rss/stringer"&gt;Stringer&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/stringer-rss/stringer.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A self-hosted, anti-social RSS reader. Stringer has no external dependencies, no social recommendations/sharing, and no fancy machine learning algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Stringer screenshoot" src="https://github.com/stringer-rss/stringer/raw/main/screenshots/stories.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="selfoss"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="selfoss"&gt;&lt;a href="https://selfoss.aditu.de/"&gt;selfoss&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/fossar/selfoss.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Multipurpose RSS reader and feed aggregation web application. It allows you to easily follow updates from different web sites, social networks and other platforms, all in single place.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="selfoss screenshoot" src="https://selfoss.aditu.de/images/screenshot-desktop.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="commafeed"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="commafeed"&gt;&lt;a href="https://github.com/Athou/commafeed"&gt;CommaFeed&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Athou/commafeed.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Google Reader inspired self-hosted RSS reader, based on Dropwizard and AngularJS. CommaFeed is now considered feature-complete and is in maintenance mode&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="CommaFeed screenshoot" src="https://user-images.githubusercontent.com/1256795/184886828-1973f148-58a9-4c6d-9587-ee5e5d3cc2cb.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="yarr"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="yarr"&gt;&lt;a href="https://github.com/nkanaev/yarr"&gt;yarr&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/nkanaev/yarr.svg?logo=github"&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;(yet another rss reader) is a web-based feed aggregator which can be used both as a desktop application and a personal self-hosted server.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="yarr screenshoot" src="https://github.com/nkanaev/yarr/raw/master/etc/promo.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="runners-up"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="runners-up"&gt;Runners-up&lt;/h2&gt;
&lt;p&gt;&lt;a id="rssmonster"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="rssmonster"&gt;&lt;a href="https://github.com/pietheinstrengholt/rssmonster"&gt;RSSMonster&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/pietheinstrengholt/rssmonster.svg?logo=github"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a id="sismics-reader"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="sismics-reader"&gt;&lt;a href="https://github.com/sismics/reader"&gt;Sismics Reader&lt;/a&gt; &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/sismics/reader.svg?logo=github"&gt;&lt;/h3&gt;
&lt;h2 id="hackable-rss-readers-written-in-python"&gt;Hackable RSS readers written in Python&lt;/h2&gt;
&lt;p&gt;If none of available feed readers meet you expectations you might want to customize existing one. There are few that are relatively simple and might be easier to build upon those. You can check the GitHub topic &lt;a href="https://github.com/topics/feed-reader?l=python"&gt;feed-reader&lt;/a&gt; to explore  popular but not necessarily lightweight options written in python. For small, hackable tools check the options below.&lt;/p&gt;
&lt;h3 id="microreader"&gt;&lt;a href="https://github.com/morganbengtsson/microreader"&gt;microreader&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Very simple web based RSS/Atom feed reader. Written in python with bottle.
There is one python module with app logic, one file with models and module for handling fav icons. At the moment of writing las commit was from 2015.&lt;/p&gt;
&lt;h3 id="spudooli-reader"&gt;&lt;a href="https://github.com/spudooli/spudooli-reader"&gt;spudooli-reader&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A reasonably capable RSS feed parser - It even handles the inventor of RSS, Dave Winer's feed, although doesn't handles dates or timezones well as I ran out of skills&lt;/li&gt;
&lt;li&gt;A simple, easy to use, and easy to understand interface&lt;/li&gt;
&lt;li&gt;Star a feed item for later appreciation&lt;/li&gt;
&lt;li&gt;An about page and a reading list page to show everyone the scope of your intersts&lt;/li&gt;
&lt;li&gt;A basic admin to add/remove feeds&lt;/li&gt;
&lt;li&gt;Single user log in to keep your feed safe&lt;/li&gt;
&lt;li&gt;Basic code that allows you to follow along, if Flask isn't your thing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="feedio"&gt;&lt;a href="https://github.com/seejay/feedIO"&gt;feedIO&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;"A News Aggregator that Knows What You Want to Read."&lt;/p&gt;
&lt;h3 id="habr-observer"&gt;&lt;a href="https://github.com/pltnk/habr-observer"&gt;habr-observer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;"An automatically updated feed with summaries of the best Habr.com articles generated by the YandexGPT neural network.""
Russian tool, to observe Russian tech news. Interesting thing is that is using Streamlit for UI and uses LLM for summarization.&lt;/p&gt;
&lt;h3 id="other-options-on-my-list-to-check"&gt;Other options on my list to check&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Nicksil/feedthing"&gt;GitHub - Nicksil/feedthing: ...because I just want to read my feeds.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/asphalt-framework/asphalt-feedreader"&gt;GitHub - asphalt-framework/asphalt-feedreader: Syndication feed reader for the Asphalt framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/alejobastidas/FeedRSS_reader_Python"&gt;GitHub - alejobastidas/FeedRSS_reader_Python: Python Feed Reader Python 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/0x1d107/feedor"&gt;GitHub - 0x1d107/feedor: asynchronous rss feed aggregator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/twm/yarrharr"&gt;GitHub - twm/yarrharr: Yarrharr Feed Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/alimasri/feedme"&gt;GitHub - alimasri/feedme: Automated email campaign using XML Feed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Tiendil/feeds.fun"&gt;GitHub - Tiendil/feeds.fun: News reader with tags&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dhvcc/rss-reader"&gt;GitHub - dhvcc/rss-reader: Command line RSS feed reader and json/html/pdf/epub converter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ibz/usocial"&gt;GitHub - ibz/usocial: Read. Listen. Pay back. The podcast client and feed reader for your personal server. With Lightning Network support.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="library-allowing-you-write-own-feed-reader"&gt;Library allowing you write own feed reader&lt;/h2&gt;
&lt;p&gt;When hacking you own feed reader you might take a look on &lt;a href="https://github.com/lemon24/reader"&gt;reader&lt;/a&gt; which is a Python feed reader library. It is designed to allow writing feed reader applications without any business code, and without depending on a particular framework.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="rss"/><category term="news"/><category term="feed-reader"/><category term="self-hosted"/><category term="atom"/><category term="news-aggregator"/></entry><entry><title>Discovering Hidden Gems - Popular and Lesser-Known Dataset Sharing Platforms</title><link href="http://127.0.0.1:8000/popular-and-lesser-known-dataset-sharing-platforms/" rel="alternate"/><published>2022-06-09T00:00:00+02:00</published><updated>2023-06-09T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-06-09:/popular-and-lesser-known-dataset-sharing-platforms/</id><summary type="html">&lt;p&gt;Looking for the key to unlocking valuable datasets? Dive into the world of Kaggle, UCI, and more as we unveil the best platforms for data enthusiasts.&lt;/p&gt;</summary><content type="html">&lt;p&gt;up::[[MOC_AI]]&lt;/p&gt;
&lt;p&gt;There are several popular dataset-sharing platforms available that researchers, data scientists, and machine learning practitioners can utilize to access and share datasets. Here are some of the best dataset-sharing platforms:&lt;/p&gt;
&lt;h2 id="kaggle"&gt;Kaggle&lt;/h2&gt;
&lt;p&gt;Kaggle is a well-known platform for data science competitions, but it also provides a &lt;a href="https://www.kaggle.com/datasets"&gt;dataset repository&lt;/a&gt; where users can discover and share datasets. It offers a wide range of datasets in various domains, along with tools for data exploration and collaboration.&lt;/p&gt;
&lt;h2 id="uci-machine-learning-repository"&gt;UCI Machine Learning Repository&lt;/h2&gt;
&lt;p&gt;The University of California, Irvine (UCI) hosts a &lt;a href="https://archive.ics.uci.edu/"&gt;repository of datasets&lt;/a&gt; specifically designed for machine learning research. It provides a diverse collection of datasets, including text, image, and time series data, covering a wide range of domains.&lt;/p&gt;
&lt;h2 id="google-dataset-search"&gt;Google Dataset Search&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://datasetsearch.research.google.com/"&gt;Google Dataset Search&lt;/a&gt; is a search engine that specifically focuses on indexing datasets. It aggregates datasets from various sources on the web, making it easier to find publicly available datasets. It provides information about the dataset, including its description, author, and availability.&lt;/p&gt;
&lt;h2 id="datagov"&gt;Data.gov&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://data.gov/"&gt;Data.gov&lt;/a&gt; is a U.S. government initiative that provides access to a wide range of datasets from different federal agencies. It offers datasets covering various domains such as health, climate, finance, transportation, and more. The platform aims to promote transparency and facilitate public access to government data.&lt;/p&gt;
&lt;h2 id="openml"&gt;OpenML&lt;/h2&gt;
&lt;p&gt;OpenML is an open-source platform that allows users to share, discover, and analyze &lt;a href="https://www.openml.org/search?type=data&amp;amp;sort=runs&amp;amp;status=active"&gt;datasets&lt;/a&gt; and machine learning experiments. It provides a collaborative environment for researchers and practitioners to collaborate and contribute to the development of machine learning algorithms.&lt;/p&gt;
&lt;h2 id="github"&gt;GitHub&lt;/h2&gt;
&lt;p&gt;Although GitHub is primarily a code hosting platform, it also serves as a repository for datasets. Many researchers and organizations share datasets on GitHub, making it a valuable resource for finding datasets across various domains. You can search for datasets using specific keywords or explore repositories dedicated to datasets.&lt;/p&gt;
&lt;h2 id="other-platforms"&gt;Other platforms&lt;/h2&gt;
&lt;p&gt;Here are 30 lesser-known dataset-sharing platforms that you can explore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DataHub: &lt;a href="https://datahub.io/"&gt;https://datahub.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Figshare: &lt;a href="https://figshare.com/"&gt;https://figshare.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quandl: &lt;a href="https://www.quandl.com/"&gt;https://www.quandl.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zillow Prize: &lt;a href="https://www.kaggle.com/c/zillow-prize-1"&gt;https://www.kaggle.com/c/zillow-prize-1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data.world: &lt;a href="https://data.world/"&gt;https://data.world/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenSNP: &lt;a href="https://opensnp.org/"&gt;https://opensnp.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dataverse: &lt;a href="https://dataverse.org/"&gt;https://dataverse.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Datacite: &lt;a href="https://www.datacite.org/"&gt;https://www.datacite.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Data Network: &lt;a href="https://www.opendatanetwork.com/"&gt;https://www.opendatanetwork.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HDX: &lt;a href="https://data.humdata.org/"&gt;https://data.humdata.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AWS Public Datasets: &lt;a href="https://registry.opendata.aws/"&gt;https://registry.opendata.aws/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Social Science Data Repository (SSDR): &lt;a href="https://data.nber.org/"&gt;https://data.nber.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Energy Data: &lt;a href="https://open-power-system-data.org/"&gt;https://open-power-system-data.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Neuro: &lt;a href="https://openneuro.org/"&gt;https://openneuro.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GeoNetwork: &lt;a href="https://geonetwork-opensource.org/"&gt;https://geonetwork-opensource.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zenodo: &lt;a href="https://zenodo.org/"&gt;https://zenodo.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Awesome Public Datasets: &lt;a href="https://github.com/awesomedata/awesome-public-datasets"&gt;https://github.com/awesomedata/awesome-public-datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Images: &lt;a href="https://storage.googleapis.com/openimages/web/index.html"&gt;https://storage.googleapis.com/openimages/web/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PubMed: &lt;a href="https://pubmed.ncbi.nlm.nih.gov/"&gt;https://pubmed.ncbi.nlm.nih.gov/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Earthdata: &lt;a href="https://earthdata.nasa.gov/"&gt;https://earthdata.nasa.gov/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Humanitarian Data Exchange (HDX): &lt;a href="https://data.humdata.org/"&gt;https://data.humdata.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Registry of Open Data on AWS: &lt;a href="https://registry.opendata.aws/"&gt;https://registry.opendata.aws/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;European Data Portal: &lt;a href="https://www.europeandataportal.eu/"&gt;https://www.europeandataportal.eu/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Global Database of Events, Language, and Tone (GDELT): &lt;a href="https://www.gdeltproject.org/"&gt;https://www.gdeltproject.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenMLCC: &lt;a href="https://openml.github.io/openmlcc/"&gt;https://openml.github.io/openmlcc/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data.gov.uk: &lt;a href="https://data.gov.uk/"&gt;https://data.gov.uk/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;National Centers for Environmental Information (NCEI): &lt;a href="https://www.ncei.noaa.gov/"&gt;https://www.ncei.noaa.gov/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DataONE: &lt;a href="https://www.dataone.org/"&gt;https://www.dataone.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;International Monetary Fund (IMF) Data: &lt;a href="https://www.imf.org/en/data"&gt;https://www.imf.org/en/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Data Soft: &lt;a href="https://www.opendatasoft.com/"&gt;https://www.opendatasoft.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="python"/></entry><entry><title>Lesser Known Backtesting Libraries</title><link href="http://127.0.0.1:8000/lesser-known-backtesting-libraries/" rel="alternate"/><published>2022-06-05T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-06-05:/lesser-known-backtesting-libraries/</id><summary type="html">&lt;p&gt;This article presents a set of lesser-known but interesting libraries that can be used for backtesting trading strategies and trading algorithms in general.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels='2,3' autolink=True autoanchor=True --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#bt"&gt;Bt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#autotrader"&gt;AutoTrader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#blueshift"&gt;Blueshift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#certis"&gt;Certis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#epymetheus"&gt;Epymetheus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deepcrypto"&gt;DeepCrypto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tradinggym"&gt;TradingGym&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vartests"&gt;vartests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#backtesting-for-cryptocurrency-trading"&gt;backtesting-for-cryptocurrency-trading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#trade-engine"&gt;Trade Engine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tradzqai"&gt;TradzQAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stock-analysis-engine"&gt;Stock Analysis Engine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#btgym"&gt;BTGym&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#baktst_org"&gt;BakTst_Org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stock-portfolio-backtester"&gt;Stock-Portfolio-Backtester&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#qstrader"&gt;QSTrader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gemini"&gt;gemini&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#closing-thoughts"&gt;Closing Thoughts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#credits"&gt;Credits&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;This article describes lesser-known python libraries/scripts that can be used for backtesting. &lt;a href="https://safjan.com/popular-backtesting-libraries/"&gt;Here&lt;/a&gt; is a list of the most popular backtesting libraries that are excluded from the scope of this article. Actively developed libraries are in the top of the list.&lt;/p&gt;
&lt;p&gt;&lt;a id="bt"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="bt"&gt;Bt&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/pmorissette/bt.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/pmorissette/bt"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pmorissette/bt"&gt;Bt&lt;/a&gt; framework allows you to easily create strategies that mix and match different &lt;a href="https://pmorissette.github.io/bt/bt.html#bt.core.Algo" title="bt.core.Algo"&gt;&lt;code&gt;Algos&lt;/code&gt;&lt;/a&gt;. It aims to foster the creation of easily testable, re-usable and flexible blocks of strategy logic to facilitate the rapid development of complex trading strategies.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goal: to save &lt;strong&gt;quants&lt;/strong&gt; from re-inventing the wheel and let them focus on the important part of the job - strategy development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To recognise potential of this tool look at the &lt;a href="https://pmorissette.github.io/bt/#a-quick-example"&gt;quick example&lt;/a&gt; provided by authors
The one of the interesting things that this library offers is &lt;a href="https://pmorissette.github.io/bt/tree.html"&gt;&lt;strong&gt;tree structure of strategies&lt;/strong&gt;&lt;/a&gt; - support for creative combining strategies.
The other reading focuses of composing optimal portfolio: &lt;a href="https://medium.com/@richardhwlin/flexible-backtesting-with-bt-7295c0dde5dd"&gt;Flexible Backtesting with BT. Introducing bt - the open-sourced… | by Richard L | Medium&lt;/a&gt;
&lt;strong&gt;NOTE&lt;/strong&gt;: at the time of writing, Bt is in alpha stage
&lt;a id="autotrader"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="autotrader"&gt;AutoTrader&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/kieran-mackle/AutoTrader.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/kieran-mackle/AutoTrader"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://kieran-mackle.github.io/AutoTrader/"&gt;AutoTrader&lt;/a&gt; - A Python-based development platform for automated trading systems - from backtesting to optimization to live-trading. AutoTrader is a Python-based platform intended to help in the development, optimization, and deployment of automated trading systems.&lt;/p&gt;
&lt;p&gt;&lt;a id="blueshift"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="blueshift"&gt;Blueshift&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://blueshift.quantinsti.com/docs/"&gt;Blueshift&lt;/a&gt; is to some extend free but not open source. You can research your ideas, backtest them, and take your strategies live with a broker of your choice on Blueshift. Blueshift helps you turn your ideas in to trading strategies.
Research and backtesting on the platform are free. Live strategy deployment is also free for a limited period.&lt;/p&gt;
&lt;p&gt;&lt;a id="certis"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="certis"&gt;Certis&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Yeachan-Heo/Certis.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/Yeachan-Heo/Certis"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Yeachan-Heo/Certis"&gt;Certis&lt;/a&gt; A Backtesting Engine. If you want to learn about the architecture of backtesting software you can definitely peek into &lt;a href="https://github.com/Yeachan-Heo/Certis"&gt;Certis repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="epymetheus"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="epymetheus"&gt;Epymetheus&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/epymetheus/epymetheus.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/epymetheus/epymetheus"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/epymetheus/epymetheus"&gt;Epymetheus&lt;/a&gt; is a multi-asset backtesting framework. It features an intuitive user API that lets analysts try out their trade strategies right away.&lt;/p&gt;
&lt;p&gt;&lt;a id="deepcrypto"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="deepcrypto"&gt;DeepCrypto&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Yeachan-Heo/DeepCrypto.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/Yeachan-Heo/DeepCrypto"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Yeachan-Heo/DeepCrypto"&gt;DeepCrypto&lt;/a&gt; Rapid Backtesting via Numba Simple &amp;amp; Vectorized trading strategy maker built-in live trading features.&lt;/p&gt;
&lt;p&gt;&lt;a id="tradinggym"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tradinggym"&gt;TradingGym&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Yvictor/TradingGym.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/Yvictor/TradingGym"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Yvictor/TradingGym"&gt;TradingGym&lt;/a&gt; is a toolkit for training and backtesting reinforcement learning algorithms. This was inspired by OpenAI Gym and imitated the framework form. Not only trading env but also has backtesting and in the future will implement real-time trading env with Interactive Broker API and so on.
This training environment originally designs for tick-data, but also supports for OHLC data format.&lt;/p&gt;
&lt;p&gt;&lt;a id="vartests"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="vartests"&gt;vartests&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/rafa-rod/vartests.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/rafa-rod/vartests"&gt;vartests&lt;/a&gt; is a Python library to perform some statistical tests to evaluate Value at Risk (VaR) Models.&lt;/p&gt;
&lt;p&gt;&lt;a id="backtesting-for-cryptocurrency-trading"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="backtesting-for-cryptocurrency-trading"&gt;backtesting-for-cryptocurrency-trading&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/CyberPunkMetalHead/backtesting-for-cryptocurrency-trading.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/CyberPunkMetalHead/backtesting-for-cryptocurrency-trading"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/CyberPunkMetalHead/backtesting-for-cryptocurrency-trading"&gt;backtesting-for-cryptocurrency-trading&lt;/a&gt; - you can use this simple crypto backtesting script to ensure your trading strategy is successful. Minimal setup required and works well with static TP and SL strategies. Trailing Stop Loss could improve profitability if added.&lt;/p&gt;
&lt;p&gt;You can use this to determine how profitable your Binance Volatility bot is.
&lt;a id="trade-engine"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="trade-engine"&gt;Trade Engine&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/xibalbas/trade-engine.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/xibalbas/trade-engine"&gt;Trade Engine&lt;/a&gt; - a library for demo trading or backtest and forward test simulation.&lt;/p&gt;
&lt;p&gt;&lt;a id="tradzqai"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tradzqai"&gt;TradzQAI&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/kkuette/TradzQAI.svg?logo=github"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/kkuette/TradzQAI"&gt;TradzQAI&lt;/a&gt; - Trading environment for RL agents, backtesting and training.&lt;/p&gt;
&lt;p&gt;&lt;a id="stock-analysis-engine"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="stock-analysis-engine"&gt;Stock Analysis Engine&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/AlgoTraders/stock-analysis-engine.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/AlgoTraders/stock-analysis-engine"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/AlgoTraders/stock-analysis-engine"&gt;Stock Analysis Engine&lt;/a&gt;. Build and tune investment algorithms for use with artificial intelligence (deep neural networks) with a distributed stack for running backtests using live pricing data on publicly traded companies with automated data feeds from: IEX Cloud, Tradier and FinViz (includes: pricing, options, news, dividends, daily, intraday, screeners, statistics, financials, earnings, and more).&lt;/p&gt;
&lt;p&gt;&lt;a id="btgym"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="btgym"&gt;BTGym&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Kismuz/btgym.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/Kismuz/btgym"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Kismuz/btgym"&gt;BTGym&lt;/a&gt;. Scalable event-driven RL-friendly backtesting library. Build on top of Backtrader with OpenAI Gym environment API.&lt;/p&gt;
&lt;p&gt;&lt;a id="baktst_org"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="baktst_org"&gt;BakTst_Org&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/xiaoyao153379/BakTst_Org.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/xiaoyao153379/BakTst_Org"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/xiaoyao153379/BakTst_Org"&gt;BakTst_Org&lt;/a&gt; is a prototype of the backtesting system used for BTC quantitative trading.&lt;/p&gt;
&lt;p&gt;&lt;a id="stock-portfolio-backtester"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="stock-portfolio-backtester"&gt;Stock-Portfolio-Backtester&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/faizancodes/Stock-Portfolio-Backtester.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/faizancodes/Stock-Portfolio-Backtester"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/faizancodes/Stock-Portfolio-Backtester"&gt;Stock-Portfolio-Backtester&lt;/a&gt;
Efficient way to backtest optimized portfolio allocations for proper hedging techniques.&lt;/p&gt;
&lt;p&gt;&lt;a id="qstrader"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="qstrader"&gt;QSTrader&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/mhallsmoore/qstrader.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/mhallsmoore/qstrader"&gt;
&lt;a href="https://github.com/mhallsmoore/qstrader"&gt;QSTrader&lt;/a&gt; is a free Python-based open-source modular schedule-driven backtesting framework for long-short equities and ETF-based systematic trading strategies.&lt;/p&gt;
&lt;p&gt;&lt;a id="gemini"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="gemini"&gt;gemini&lt;/h2&gt;
&lt;p&gt;&lt;img alt="github stars shield" src="https://img.shields.io/github/stars/anfederico/gemini.svg?logo=github"&gt; &lt;img alt="commit-activity" src="https://img.shields.io/github/commit-activity/y/mhallsmoore/qstrader"&gt;
&lt;a href="https://github.com/anfederico/gemini"&gt;gemini&lt;/a&gt; - Backtesting for sleepless cryptocurrency markets&lt;/p&gt;
&lt;p&gt;&lt;a id="closing-thoughts"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="closing-thoughts"&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;I would like to look closer at &lt;a href="https://github.com/epymetheus/epymetheus"&gt;Epymetheus&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="credits"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="credits"&gt;Credits&lt;/h2&gt;
&lt;p&gt;Heading photo from &lt;a href="https://unsplash.com/photos/InWI1lteYfU"&gt;unsplash&lt;/a&gt; authored by &lt;a href="https://unsplash.com/@moneyphotos"&gt;regularguy.eth&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Algorithmic Trading"/><category term="trading"/><category term="backtesting"/><category term="crypto"/><category term="algorithmic-trading"/><category term="algotrading"/><category term="quant"/></entry><entry><title>Top 10 Python Libraries for Document Classification</title><link href="http://127.0.0.1:8000/top-10-python-libraries-for-document-classification/" rel="alternate"/><published>2022-05-01T00:00:00+02:00</published><updated>2022-05-01T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-05-01:/top-10-python-libraries-for-document-classification/</id><summary type="html">&lt;p&gt;Unlock the power of document classification with these top Python libraries! Discover the best tools for effortless text analysis and more.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Document classification is the task of assigning a document to one or more predefined categories based on its content. This is a common task in many areas, including natural language processing, information retrieval, and machine learning. Python has a wide range of libraries that can be used for document classification, and in this blog post, we will explore the top 10 Python libraries for this task.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-scikit-learn"&gt;1. Scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-nltk"&gt;2. NLTK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-gensim"&gt;3. Gensim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-tensorflow"&gt;4. TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-pytorch"&gt;5. PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-keras"&gt;6. Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-pycaret"&gt;7. PyCaret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-fasttext"&gt;8. FastText&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-pytext"&gt;9. PyText&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#10-textblob"&gt;10. TextBlob&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-tools"&gt;Other tools&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="1-scikit-learn"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="1-scikit-learn"&gt;1. Scikit-learn&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Scikit-learn" src="https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/"&gt;Scikit-learn&lt;/a&gt; is a popular machine learning library for Python that provides a wide range of algorithms for document classification. It offers a simple and easy-to-use interface for training and testing machine learning models. Scikit-learn supports various feature extraction techniques, such as bag-of-words, TF-IDF, and word embeddings, which are essential for document classification tasks.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"&gt;Working With Text Data - scikit-learn 1.2.1 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"&gt;Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK. | by Javed Shaikh | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackabuse.com/text-classification-with-python-and-scikit-learn/"&gt;Text Classification with Python and Scikit-Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html"&gt;Classification of text documents using sparse features - scikit-learn 1.2.1 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dylancastillo.co/text-classification-using-python-and-scikit-learn/"&gt;Text Classification Using Python and Scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sanjayasubedi.com.np/machinelearning/nlp/text-classification-with-sklearn/"&gt;Text Classification with sklearn - Sanjaya’s Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="2-nltk"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="2-nltk"&gt;2. NLTK&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.nltk.org/"&gt;Natural Language Toolkit&lt;/a&gt; (NLTK) is a library that provides various tools and algorithms for natural language processing. NLTK offers a range of tools for document classification, including feature extraction, classification algorithms, and performance evaluation. NLTK also provides pre-trained models for sentiment analysis, text classification, and other NLP tasks.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/nltk/nltk"&gt;https://github.com/nltk/nltk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pythonprogramming.net/text-classification-nltk-tutorial/"&gt;Python Programming Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goodboychan.github.io/python/machine_learning/natural_language_processing/2020/10/23/01-Text-Classification-with-NLTK.html"&gt;Text Classification with NLTK | Chan`s Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nltk.org/book/ch06.html"&gt;6. Learning to Classify Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mylearningsinaiml.wordpress.com/nlp/text-classification-using-nltk/"&gt;Text Classification using NLTK | Foundations of AI &amp;amp; ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://manojkumar-bhattaram.medium.com/movie-reviews-text-classification-using-nltk-f6644cb9958d"&gt;Movie Reviews (Text) Classification Using NLTK | by Bhattaram Manojkumar | Medium&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="3-gensim"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="3-gensim"&gt;3. Gensim&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://radimrehurek.com/gensim/_static/images/gensim.png"  style="display:block;margin:0;padding:0;background-color:black;" alt="Gensim"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://radimrehurek.com/gensim/"&gt;Gensim&lt;/a&gt; is a Python library for topic modeling, text summarization, and document similarity analysis. It offers various algorithms for document classification, including Latent Dirichlet Allocation (LDA), Hierarchical Dirichlet Process (HDP), and Random Projections. Gensim is widely used for document classification in the field of information retrieval.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/RaRe-Technologies/gensim"&gt;https://github.com/RaRe-Technologies/gensim&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d"&gt;Implementing multi-class text classification with Doc2Vec | by Dipika Baad | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"&gt;Multi-Class Text Classification with Doc2Vec &amp;amp; Logistic Regression | by Susan Li | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://thinkingneuron.com/how-to-classify-text-using-word2vec/"&gt;How to classify text using Word2Vec - Thinking Neuron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;h2 id="word2vec-for-text-classification-how-to-in-python-cnn"&gt;&lt;a href="https://spotintelligence.com/2023/02/15/word2vec-for-text-classification/"&gt;Word2Vec For Text Classification [How To In Python &amp;amp; CNN]&lt;/a&gt;&lt;/h2&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="4-tensorflow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="4-tensorflow"&gt;4. TensorFlow&lt;/h2&gt;
&lt;p&gt;&lt;img alt="TensorFlow" src="https://www.gstatic.com/devrel-devsite/prod/ve53a52d1b45000b9df3bcdfe3060bb6d50689f5a6c2b8f03cdaa24fff649fba3/tensorflow/images/lockup.svg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; is a popular machine learning library that provides a wide range of tools and algorithms for document classification. TensorFlow offers various deep learning architectures, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which are widely used for document classification tasks. TensorFlow also supports transfer learning, which allows us to use pre-trained models for document classification.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/tensorflow/tensorflow"&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/keras/text_classification"&gt;Basic text classification  |  TensorFlow Core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/text/tutorials/text_classification_rnn"&gt;Text classification with an RNN  |  TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert"&gt;Classify text with BERT  |  Text  |  TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/hub/tutorials/tf2_text_classification"&gt;Text Classification with Movie Reviews  |  TensorFlow Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35"&gt;Multi Class Text Classification with LSTM using TensorFlow 2.0 | by Susan Li | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d"&gt;Multi-class Text Classification using BERT and TensorFlow | by Nicolo Cosimo Albanese | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="5-pytorch"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="5-pytorch"&gt;5. PyTorch&lt;/h2&gt;
&lt;p&gt;&lt;img alt="PyTorch" src="https://netico-group.com/wp-content/uploads/2019/08/pytorch-logo-600x400.2560360867c1eb4cba593aebe81840c961b271ce-300x200.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; is a machine learning library that provides a range of tools and algorithms for document classification. PyTorch offers various deep learning architectures, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which are widely used for document classification tasks. PyTorch also supports transfer learning and provides pre-trained models for document classification.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/pytorch/pytorch"&gt;https://github.com/pytorch/pytorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"&gt;Text classification with the torchtext library - PyTorch Tutorials 1.13.1+cu117 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6"&gt;Text Classification with LSTMs in PyTorch | by Fernando López | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"&gt;Site Unreachable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f"&gt;Text Classification with BERT in PyTorch | by Ruben Winastwan | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/prakashpandey9/Text-Classification-Pytorch"&gt;prakashpandey9/Text-Classification-Pytorch&lt;/a&gt; - Text classification using deep learning models in Pytorch&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df"&gt;Multiclass Text Classification using LSTM in Pytorch | by Aakanksha NS | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/RBeaudet/Text-Classification-Using-PyTorch"&gt;RBeaudet/Text-Classification-Using-PyTorch&lt;/a&gt; -  A didactic repository to understand Deep Learning models for text classification using PyTorch&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/swlh/text-classification-using-transformers-pytorch-implementation-5ff9f21bd106"&gt;Text Classification Using Transformers (Pytorch Implementation) | by Yassine Hamdaoui | The Startup | Medium&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="6-keras"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="6-keras"&gt;6. Keras&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Keras" src="https://keras.io/img/logo.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; is a high-level deep learning library that provides a simple and easy-to-use interface for building neural networks. Keras offers various deep learning architectures, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which are widely used for document classification tasks. Keras also provides pre-trained models for document classification.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/keras-team/keras"&gt;https://github.com/keras-team/keras&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://keras.io/examples/nlp/text_classification_from_scratch/"&gt;Text classification from scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://realpython.com/python-keras-text-classification/"&gt;Practical Text Classification With Python and Keras – Real Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://keras.io/examples/nlp/multi_label_classification/"&gt;Large-scale multi-label text classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://keras.io/examples/nlp/text_classification_with_transformer/"&gt;Text classification with Transformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="7-pycaret"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="7-pycaret"&gt;7. PyCaret&lt;/h2&gt;
&lt;p&gt;&lt;img alt="PyCaret" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fblog.jcharistech.com%2Fwp-content%2Fuploads%2F2020%2F07%2Fpycaret_logo-300x300.png&amp;amp;f=1&amp;amp;nofb=1&amp;amp;ipt=39725e063fa9b1d79db22a64f2eeefd78f4d9ba908960d11a5ee518a630ded4e&amp;amp;ipo=images"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pycaret.org/"&gt;PyCaret&lt;/a&gt; is a Python library for machine learning that provides various tools and algorithms for document classification. PyCaret offers a range of machine learning algorithms, such as logistic regression, support vector machines (SVMs), and decision trees, which are widely used for document classification tasks. PyCaret also provides automated machine learning (AutoML) capabilities, which can help us to quickly build and deploy machine learning models.&lt;/p&gt;
&lt;p&gt;GitHub:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pycaret.gitbook.io/docs/learn-pycaret/official-blog/nlp-text-classification-in-python-using-pycaret"&gt;NLP Text Classification in Python using PyCaret - PyCaret Official&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pycaret/pycaret/blob/master/resources/NLP%20Text-Classification%20in%20Python%20using%20PyCaret.md"&gt;pycaret/NLP Text-Classification in Python using PyCaret.md at master · pycaret/pycaret · GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.analyticsvidhya.com/blog/2021/08/beginners-guide-to-text-classification-using-pycaret/"&gt;Site Unreachable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/nlp-classification-in-python-pycaret-approach-vs-the-traditional-approach-602d38d29f06"&gt;NLP Text-Classification in Python: PyCaret Approach Vs The Traditional Approach | by Prateek Baghel | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datapeaker.com/en/big--data/Beginner's-Guide-to-Text-Classification-with-Pycaret/"&gt;Beginner's Guide to Classifying Text with PyCaret | Datapeaker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="8-fasttext"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="8-fasttext"&gt;8. FastText&lt;/h2&gt;
&lt;p&gt;&lt;img alt="FastText" src="https://fasttext.cc/img/fasttext-logo-color-web.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://fasttext.cc/"&gt;FastText&lt;/a&gt; is a library for text classification and word representation learning. It provides a simple and easy-to-use interface for building text classifiers. FastText is widely used for document classification tasks, especially for multilingual text classification. FastText also supports various feature extraction techniques, such as bag-of-words and n-gram features.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/facebookresearch/fastText/"&gt;https://github.com/facebookresearch/fastText/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://fasttext.cc/docs/en/supervised-tutorial.html"&gt;Text classification · fastText&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c"&gt;fastText for Text Classification. I explore a fastText classifier for… | by Shraddha Anala | Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ethen8181.github.io/machine-learning/deep_learning/multi_label/fasttext.html"&gt;fasttext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.geeksforgeeks.org/fasttext-working-and-implementation/"&gt;FastText Working and Implementation - GeeksforGeeks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@rukshanjayasekara/text-classification-with-fasttext-5cac26ce7bc6"&gt;Text Classification with FastText | by Rukshan Jayasekara | Medium&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="9-pytext"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="9-pytext"&gt;9. PyText&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytext.readthedocs.io/en/latest/"&gt;PyText&lt;/a&gt; is a deep learning library for natural language processing that provides various tools and algorithms for document classification. PyText offers a range of deep learning architectures, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which are widely used for document classification tasks. PyText also provides pre-trained models for document classification and supports transfer learning.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/facebookresearch/pytext"&gt;https://github.com/facebookresearch/pytext&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Project repository on GitHub is now in archive state.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/@jayrodge/text-document-classification-using-pytext-ca7e1c380d5f"&gt;Text/Document Classification using PyText | by Jay Rodge | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytext.readthedocs.io/en/master/train_your_first_model.html"&gt;Train your first model - PyText documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytext.readthedocs.io/en/master/"&gt;PyText Documentation - PyText documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="10-textblob"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="10-textblob"&gt;10. TextBlob&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt; is a library for processing textual data in Python. It provides various tools for natural language processing, including sentiment analysis, part-of-speech tagging, and text classification. TextBlob offers a simple and easy-to-use interface for building text classifiers. TextBlob is widely used for document classification tasks, especially for small datasets.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;Python offers a wide range of libraries for document classification. Each library has its own strengths and weaknesses, and the choice of library depends on the specific requirements of the task at hand. Scikit-learn, NLTK, Gensim, TensorFlow, PyTorch, Keras, PyCaret, FastText, PyText, and TextBlob are the top 10 Python libraries for document classification that we have explored in this blog post. By using these libraries, we can easily build and deploy machine learning models for document classification tasks.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href="https://github.com/sloria/TextBlob"&gt;https://github.com/sloria/TextBlob&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/classifiers.html"&gt;Tutorial: Building a Text Classification System - TextBlob 0.16.0 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/"&gt;Site Unreachable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stevenloria.com/simple-text-classification/"&gt;Tutorial: Simple Text Classification with Python and TextBlob | stevenloria.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.learnsteps.com/naive-bayesian-text-classifier-using-textblob-python/"&gt;Naive bayesian text classifier using textblob and python - Learn Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datapeaker.com/en/big--data/pnl-for-beginners-classifying-text-using-textblob/"&gt;NLP for beginners | Classifying text using TextBlob | Datapeaker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="other-tools"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="other-tools"&gt;Other tools&lt;/h2&gt;
&lt;p&gt;Here are 20 more Python libraries for document classification that were not discussed in this article:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Spacy&lt;/li&gt;
&lt;li&gt;Flair&lt;/li&gt;
&lt;li&gt;AllenNLP&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;li&gt;Vowpal Wabbit&lt;/li&gt;
&lt;li&gt;LightGBM&lt;/li&gt;
&lt;li&gt;XGBoost&lt;/li&gt;
&lt;li&gt;CatBoost&lt;/li&gt;
&lt;li&gt;HuggingFace&lt;/li&gt;
&lt;li&gt;MXNet&lt;/li&gt;
&lt;li&gt;Theano&lt;/li&gt;
&lt;li&gt;Caffe2&lt;/li&gt;
&lt;li&gt;TorchText&lt;/li&gt;
&lt;li&gt;Stanford CoreNLP&lt;/li&gt;
&lt;li&gt;Textacy&lt;/li&gt;
&lt;li&gt;Pattern&lt;/li&gt;
&lt;li&gt;Polyglot&lt;/li&gt;
&lt;li&gt;Apache Tika&lt;/li&gt;
&lt;li&gt;Apache Lucene&lt;/li&gt;
&lt;li&gt;PyTorch-NLP&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these libraries has its own unique features and benefits, and can be used for various document classification tasks. Some of them offer deep learning architectures, while others focus on traditional machine learning algorithms. By exploring these additional libraries, you can further expand your options for document classification in Python.&lt;/p&gt;</content><category term="Machine Learning"/><category term="document-intelligence"/><category term="AI"/><category term="python"/><category term="document-processing"/><category term="document-data-extraction"/><category term="machine-learning"/><category term="natural-language-processing"/><category term="NLP"/><category term="text-classification"/><category term="scikit-learn"/><category term="NLTK"/><category term="Gensim"/><category term="TensorFlow"/><category term="PyTorch"/><category term="Keras"/><category term="PyCaret"/><category term="FastText"/><category term="PyText"/><category term="TextBlob"/><category term="Spacy"/><category term="Flair"/><category term="AllenNLP"/><category term="Transformers"/><category term="Vowpal"/><category term="Wabbit"/><category term="LightGBM"/><category term="XGBoost"/><category term="CatBoost"/><category term="HuggingFace"/><category term="MXNet"/><category term="Theano"/><category term="Caffe2"/><category term="TorchText"/><category term="Stanford-CoreNLP"/><category term="Textacy"/><category term="Pattern"/><category term="Polyglot"/><category term="Apache-Tika"/><category term="Apache-Lucene"/><category term="PyTorch-NLP"/></entry><entry><title>Tutorial on How to Create New Theme for Pelican Static Site Generator</title><link href="http://127.0.0.1:8000/create-theme-for-pelican-blog-tutorial/" rel="alternate"/><published>2022-03-01T00:00:00+01:00</published><updated>2022-03-01T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-03-01:/create-theme-for-pelican-blog-tutorial/</id><summary type="html">&lt;p&gt;A great Pelican theme can make all the difference. Learn how to design your own with our comprehensive tutorial.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#1-choose-a-name-for-your-theme"&gt;1.  &lt;strong&gt;Choose a name for your theme&lt;/strong&gt;:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-create-a-new-directory-for-your-theme"&gt;2.  &lt;strong&gt;Create a new directory for your theme&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-create-a-templates-directory"&gt;3.  &lt;strong&gt;Create a &lt;code&gt;templates&lt;/code&gt; directory&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-create-a-base-template"&gt;4.  &lt;strong&gt;Create a base template&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-create-a-static-directory"&gt;5.  &lt;strong&gt;Create a &lt;code&gt;static&lt;/code&gt; directory&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-create-a-themeconf-file"&gt;6.  &lt;strong&gt;Create a &lt;code&gt;theme.conf&lt;/code&gt; file&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7-add-css-to-your-theme"&gt;7.  &lt;strong&gt;Add CSS to your theme&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#8-create-other-templates"&gt;8.  &lt;strong&gt;Create other templates&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#9-configure-your-pelican-project"&gt;9.  &lt;strong&gt;Configure your Pelican project&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#10-test-your-theme"&gt;10.  &lt;strong&gt;Test your theme&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="1-choose-a-name-for-your-theme"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="1-choose-a-name-for-your-theme"&gt;1.  Choose a name for your theme&lt;/h2&gt;
&lt;p&gt;Choose a name that reflects the design or purpose of your theme. Make sure it's not already taken by another Pelican theme.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-create-a-new-directory-for-your-theme"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="2-create-a-new-directory-for-your-theme"&gt;2.  Create a new directory for your theme&lt;/h2&gt;
&lt;p&gt;Create a new directory under the &lt;code&gt;themes&lt;/code&gt; directory in your Pelican project. Use the name you chose in step 1 as the directory name.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-create-a-templates-directory"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="3-create-a-templates-directory"&gt;3.  Create a &lt;code&gt;templates&lt;/code&gt; directory&lt;/h2&gt;
&lt;p&gt;Inside your theme directory, create a new directory called &lt;code&gt;templates&lt;/code&gt;. This is where you'll store all the HTML templates for your theme.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-create-a-base-template"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="4-create-a-base-template"&gt;4.  Create a base template&lt;/h2&gt;
&lt;p&gt;Create a base template for your theme. This template will define the basic structure of all your pages, and will be used as the starting point for all other templates. Here's an example of a simple base template:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cp"&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;html&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;head&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;title&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;{{ page.title }} | {{ site.title }}&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;title&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;head&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;header&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{ SITEURL }}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;{{ site.title }}&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;nav&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
                &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;ul&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
                    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{ SITEURL }}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Home&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
                    {% for category, articles in categories.items() %}
                        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{ SITEURL }}/category/{{ category|slug }}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;{{ category }}&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
                    {% endfor %}
                &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;ul&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;nav&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;header&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;main&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            {% block content %}{% endblock %}
        &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;main&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;footer&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="ni"&gt;&amp;amp;copy;&lt;/span&gt; {{ date.today().year }} {{ site.author }}
        &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;footer&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;html&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this template, &lt;code&gt;{{ page.title }}&lt;/code&gt; and &lt;code&gt;{{ site.title }}&lt;/code&gt; will be replaced with the title of the current page and the title of your site, respectively. The &lt;code&gt;header&lt;/code&gt; section includes a navigation menu that lists all the categories on your site. The &lt;code&gt;main&lt;/code&gt; section is where the content of each page will go. The &lt;code&gt;footer&lt;/code&gt; includes your copyright information.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-create-a-static-directory"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="5-create-a-static-directory"&gt;5.  Create a &lt;code&gt;static&lt;/code&gt; directory&lt;/h2&gt;
&lt;p&gt;Create a new directory called &lt;code&gt;static&lt;/code&gt; inside your theme directory. This is where you'll store all the static assets (like CSS, images, and JavaScript) for your theme.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-create-a-themeconf-file"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="6-create-a-themeconf-file"&gt;6.  Create a &lt;code&gt;theme.conf&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;Create a new file called &lt;code&gt;theme.conf&lt;/code&gt; in your theme directory. This file will contain metadata about your theme, like its name, author, and description. Here's an example &lt;code&gt;theme.conf&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[theme]&lt;/span&gt;
&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;My New Theme&lt;/span&gt;
&lt;span class="na"&gt;description&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;A simple, responsive theme for Pelican&lt;/span&gt;
&lt;span class="na"&gt;author&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;Jane Doe&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="7-add-css-to-your-theme"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="7-add-css-to-your-theme"&gt;7.  Add CSS to your theme&lt;/h2&gt;
&lt;p&gt;Create a new file called &lt;code&gt;style.css&lt;/code&gt; in your &lt;code&gt;static&lt;/code&gt; directory. This is where you'll define the styles for your theme. Here's an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c"&gt;/* Base styles */&lt;/span&gt;
&lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;font-family&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Arial&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;sans-serif&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#0066cc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;text-decoration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;none&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;hover&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;text-decoration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;underline&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c"&gt;/* Header styles */&lt;/span&gt;
&lt;span class="nt"&gt;header&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;background-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#f2f2f2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;border-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;solid&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#ddd&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;header&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;h1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;nav&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;ul&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;list-style&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;none&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;nav&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;display&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;inline-block&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-right&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c"&gt;/* Main styles */&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In addition to the base styles, you'll also want to add styles specific to your theme's layout and design. For example, if you're creating a blog theme, you might want to style the blog post titles, dates, and tags differently than the rest of the content. Here's an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c"&gt;/* Blog post styles */&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;post-title&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;font-size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;post-date&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;font-style&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;italic&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#666&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;post-tags&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;post-tags&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;display&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;inline-block&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;background-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#ddd&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-right&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#333&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;font-size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;text-transform&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;uppercase&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;border-radius&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;post-content&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;line-height&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c"&gt;/* Sidebar styles */&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;sidebar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;right&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="kt"&gt;%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;sidebar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;h2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;font-size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;sidebar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;ul&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;list-style&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;none&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;sidebar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="kt"&gt;px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;sidebar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mh"&gt;#0066cc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;text-decoration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;none&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;sidebar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;li&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;hover&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;text-decoration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;underline&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="8-create-other-templates"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="8-create-other-templates"&gt;8.  Create other templates&lt;/h2&gt;
&lt;p&gt;Now that you have a base template and some CSS, it's time to create the other templates for your theme. This will include templates for individual pages, blog posts, tags, categories, and any other pages your site might have. Here's an example of a blog post template:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;{% extends &amp;quot;base.html&amp;quot; %}

{% block content %}
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;article&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;h1&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;post-title&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;{{ page.title }}&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;h1&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;post-date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;{{ page.date.strftime(&amp;#39;%B %d, %Y&amp;#39;) }}&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;post-tags&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        {% for tag in page.tags %}
            &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="na"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{{ SITEURL }}/tag/{{ tag|slug }}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;{{ tag }}&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;a&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        {% endfor %}
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;post-content&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
        {{ page.content }}
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;article&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
{% endblock %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this template, the &lt;code&gt;extends&lt;/code&gt; tag tells Pelican to use the &lt;code&gt;base.html&lt;/code&gt; template as the starting point. The &lt;code&gt;block content&lt;/code&gt; tag defines where the content of the page should go. The &lt;code&gt;page.title&lt;/code&gt;, &lt;code&gt;page.date&lt;/code&gt;, and &lt;code&gt;page.tags&lt;/code&gt; variables are replaced with the title, date, and tags of the current blog post.&lt;/p&gt;
&lt;p&gt;&lt;a id="9-configure-your-pelican-project"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="9-configure-your-pelican-project"&gt;9.  Configure your Pelican project&lt;/h2&gt;
&lt;p&gt;Once you've created your new theme, you'll need to configure your Pelican project to use it. In your &lt;code&gt;pelicanconf.py&lt;/code&gt; file, set the &lt;code&gt;THEME&lt;/code&gt; variable to the name of your theme directory. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;THEME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;my-new-theme&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="10-test-your-theme"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="10-test-your-theme"&gt;10.  Test your theme&lt;/h2&gt;
&lt;p&gt;Finally, test your theme by running &lt;code&gt;pelican&lt;/code&gt; and generating your site. If everything is working correctly, your new theme should be applied to all the pages on your site.&lt;/p&gt;
&lt;p&gt;That's it! By following these steps, you should be able to create a new Pelican theme from scratch. Of course, this is just a basic example, and you can customize your theme further by adding more templates, styles, and features.&lt;/p&gt;</content><category term="Howto"/><category term="pelican"/><category term="static-site-generator"/><category term="theme-development"/><category term="web-development"/><category term="css"/><category term="html"/><category term="templates"/><category term="site-design"/><category term="web-design"/></entry><entry><title>Pro Tips for Diagnosing Regression Model Errors</title><link href="http://127.0.0.1:8000/regression-model-errors-plot/" rel="alternate"/><published>2022-02-22T00:00:00+01:00</published><updated>2023-02-22T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-02-22:/regression-model-errors-plot/</id><summary type="html">&lt;p&gt;Improve your regression model's accuracy and predictability by uncovering hidden errors with these essential plots.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tldr---list-of-the-plots-with-short-description"&gt;TLDR - list of the plots with short description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#plots"&gt;Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#residual-plot"&gt;Residual Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#histogram-of-residuals"&gt;Histogram of Residuals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#scale-location-plot"&gt;Scale-Location Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#q-q-plot"&gt;Q-Q Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#leverage-plot"&gt;Leverage Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cooks-distance-plot"&gt;Cook's Distance Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#actual-vs-predicted-plot"&gt;Actual vs. Predicted Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mean-absolute-error-plot"&gt;Mean Absolute Error Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This article is still a bit draft but it is published since might be an inspiration and starting point for crafting own visual analysis.
&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When building a regression model, it is essential to check how well the model performs. We use error metrics such as Mean Squared Error (MSE) or Mean Absolute Error (MAE) to measure the performance of our model. However, these error metrics do not always give us a complete picture of the model's performance. Therefore, it is essential to visualize the model's errors to gain a better understanding of how the model is performing.&lt;/p&gt;
&lt;p&gt;&lt;a id="tldr---list-of-the-plots-with-short-description"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tldr-list-of-the-plots-with-short-description"&gt;TLDR - list of the plots with short description&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Residual Plot&lt;/strong&gt;: A scatter plot of the residuals (the difference between the predicted values and the actual values) against the predicted values. This plot can show if there is a pattern in the residuals, indicating that the model is not capturing some important information in the data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q-Q Plot&lt;/strong&gt;: A plot that compares the distribution of the residuals to a normal distribution. If the residuals follow a normal distribution, they should fall along a straight line in the Q-Q plot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Histogram of Residuals&lt;/strong&gt;: A histogram of the residuals can show if the distribution is approximately normal or if there are outliers or skewness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scatter Plot&lt;/strong&gt;: A scatter plot of the predicted values against the actual values can show if the model is making systematic errors, such as under- or over-predicting values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Box Plot&lt;/strong&gt;: A box plot of the residuals can show if there are outliers or if the residuals are skewed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cook's Distance Plot&lt;/strong&gt;: A plot that shows the influence of each data point on the regression coefficients. Cook's distance is a measure of how much the regression coefficients change when a data point is removed from the dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leverage Plot&lt;/strong&gt;: A plot that shows how much each data point is affecting the regression line. A data point with high leverage has a large influence on the regression line.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale-Location Plot&lt;/strong&gt;: A plot that shows the square root of the standardized residuals against the predicted values. This plot can show if there is a non-linear relationship between the residuals and the predicted values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Residuals vs. Time Plot&lt;/strong&gt;: A plot that shows if the residuals are correlated with time. This can be important if the data is time-series data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Partial Regression Plot&lt;/strong&gt;: A plot that shows the relationship between the response variable and one predictor variable, while controlling for the effects of other predictor variables in the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this blog post, we will discuss some of the most insightful plots to visualize regression model prediction errors. We will  provide Python code snippets to generate each type of plot.&lt;/p&gt;
&lt;p&gt;&lt;a id="data-preparation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="data-preparation"&gt;Data Preparation&lt;/h2&gt;
&lt;p&gt;Before we start building our regression model, let's load the Boston Housing Dataset and split it into training and testing datasets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="n"&gt;boston&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_boston&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;boston_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;boston_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MEDV&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MEDV&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boston_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MEDV&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We will be using a simple linear regression model to predict the target variable (MEDV). Let's train our model and make predictions on the test dataset.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;

&lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now that we have made predictions using our regression model, let's move on to visualizing the model's errors.&lt;/p&gt;
&lt;p&gt;&lt;a id="plots"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="plots"&gt;Plots&lt;/h2&gt;
&lt;p&gt;&lt;a id="residual-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="residual-plot"&gt;Residual Plot&lt;/h3&gt;
&lt;p&gt;The residual plot is one of the most commonly used plots to visualize the regression model's errors. It shows the difference between the actual and predicted values (residuals) plotted against the predicted values.&lt;/p&gt;
&lt;p&gt;A good regression model will have residuals randomly scattered around the zero line. If the residuals have a pattern or are not randomly distributed, it indicates that the model is not performing well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;residuals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Predicted Values&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Residuals&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Residual Plot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="residual_plot" src="/images/model_error/fig_1.png"&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we can see that the residuals are randomly scattered around the zero line, which indicates that the model is performing well.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-can-we-learn-from-a-residual-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-can-we-learn-from-a-residual-plot"&gt;What can we learn from a residual plot?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;horizontal line&lt;/strong&gt; at 0 indicates that the model is unbiased, and the residuals are randomly distributed around 0.&lt;/li&gt;
&lt;li&gt;A curved or &lt;strong&gt;U-shaped&lt;/strong&gt; pattern suggests that the model is not capturing the non-linear relationship between the independent and dependent variables.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;funnel-shaped&lt;/strong&gt; pattern indicates that the variance of the residuals is not constant across the range of predicted values. This is known as heteroscedasticity and can be corrected by transforming the data or using a different model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="histogram-of-residuals"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="histogram-of-residuals"&gt;Histogram of Residuals&lt;/h3&gt;
&lt;p&gt;The histogram of residuals plot shows the distribution of the residuals. A good regression model will have residuals that follow a normal distribution. If the residuals have a skewed distribution, it indicates that the model is not performing well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Residuals&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Frequency&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Histogram of Residuals&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="residual_plot" src="/images/model_error/fig_2.png"&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we can see that the residuals follow a normal distribution, which indicates that the model is performing well.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-we-can-learn-from-residuals-histogram"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-we-can-learn-from-residuals-histogram"&gt;What we can learn from Residuals Histogram?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The residuals histogram can give us insights into the distribution of the errors in the model. If the histogram shows a roughly normal distribution, it indicates that the model is capturing the underlying pattern in the data, and the errors are distributed randomly around the mean. A normal distribution is ideal because it means that the model is making predictions that are equally likely to be too high or too low.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;skewed residuals histogram&lt;/strong&gt; indicates that the model is making systematic errors. If the histogram is &lt;strong&gt;skewed to the left&lt;/strong&gt;, it means that the model is over-predicting values, while if it is &lt;strong&gt;skewed to the right&lt;/strong&gt;, it means that the model is under-predicting values. Skewed distributions can also indicate the presence of outliers in the data.&lt;/li&gt;
&lt;li&gt;The residuals histogram can be used to identify &lt;strong&gt;outliers&lt;/strong&gt; in the data. Outliers are data points that fall far outside the expected range of values, and they can have a significant impact on the model's performance. Outliers can be seen as &lt;strong&gt;peaks or gaps in the histogram&lt;/strong&gt;, and they may need to be removed from the dataset to improve the model's accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="scale-location-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="scale-location-plot"&gt;Scale-Location Plot&lt;/h3&gt;
&lt;p&gt;A scale-location plot shows the relationship between the absolute square root of the residuals and the predicted values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# calculate the absolute square root of the residuals &lt;/span&gt;
&lt;span class="n"&gt;sqrt_abs_resid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 

&lt;span class="c1"&gt;# plot the square root of the absolute residuals against the predicted values &lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sqrt_abs_resid&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Scale-Location Plot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Predicted Values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Sqrt(|Residuals|)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="residual_plot" src="/images/model_error/fig_3.png"&gt;&lt;/p&gt;
&lt;p&gt;In this plot, the y-axis shows the square root of the absolute residuals, and the x-axis shows the predicted values. If the residuals are normally distributed, we expect to see a random scattering of points around the horizontal line.&lt;/p&gt;
&lt;p&gt;We can see from this plot that there is a slight curve in the line, which may indicate that the model is not capturing the full range of variation in the data.&lt;/p&gt;
&lt;h4 id="what-ca-we-learn-from-a-scale-location-plots"&gt;What ca we learn from a Scale-Location plots?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Scale location plots are used to check for &lt;strong&gt;heteroscedasticity&lt;/strong&gt;, which is the condition where the variance of the residuals changes as a function of the predicted values. In a scale location plot, the square root of the standardized residuals is plotted against the predicted values.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;horizontal line&lt;/strong&gt; in the scale location plot indicates that the residuals have constant variance across the range of the predicted values. This is a desirable condition for a regression model, and it suggests that the model is appropriately capturing the variability of the data.&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;curved or sloping line&lt;/strong&gt; in the scale location plot indicates that the variance of the residuals is not constant, and this is an indication of heteroscedasticity. This is a problem because it means that the model is not accounting for the changing variability of the data, which can lead to inaccurate predictions.&lt;/li&gt;
&lt;li&gt;A scale location plot can be used to identify potential outliers in the data. &lt;strong&gt;Points that are far away from the horizontal line&lt;/strong&gt; in the plot may be indicative of outliers that are contributing to the heteroscedasticity in the model.&lt;/li&gt;
&lt;li&gt;If a scale location plot shows &lt;strong&gt;heteroscedasticity&lt;/strong&gt;, it may be possible to improve the model by transforming the response variable or adding additional predictor variables to capture the underlying pattern of variability in the data.
&lt;a id="q-q-plot"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="q-q-plot"&gt;Q-Q Plot&lt;/h3&gt;
&lt;p&gt;The Q-Q (Quantile-Quantile) plot is a probability plot that shows the theoretical quantiles of the residuals against the actual quantiles of the residuals. A good regression model will have residuals that follow a straight line.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;

&lt;span class="c1"&gt;# create Q-Q plot with 45-degree line added to plot&lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qqplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;45&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="q-q" src="/images/model_error/fig_10.png"&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we can see that the residuals follow a straight line, which indicates that the model is performing well.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-can-we-learn-from-a-qq-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-can-we-learn-from-a-qq-plot"&gt;What can we learn from a QQ plot?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If the points on the plot fall on a &lt;strong&gt;straight line&lt;/strong&gt;, the residuals are normally distributed.&lt;/li&gt;
&lt;li&gt;If the points &lt;strong&gt;deviate from the straight line&lt;/strong&gt;, it suggests that the residuals are not normally distributed, and we may need to transform the data or use a different model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="leverage-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="leverage-plot"&gt;Leverage Plot&lt;/h3&gt;
&lt;p&gt;The leverage plot shows how much each data point is affecting the regression line. It plots the leverage score (i.e., the measure of how much a data point deviates from the mean) against the standardized residuals.&lt;/p&gt;
&lt;p&gt;A data point with high leverage has a large influence on the regression line. If a data point has high leverage and a large residual, it is called an influential point.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.graphics.regressionplots&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plot_leverage_resid2&lt;/span&gt;

&lt;span class="c1"&gt;# Load example data&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_rdataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;datasets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="c1"&gt;# Create X and y variables&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;speed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;dist&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Fit the linear regression model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Plot leverage plot&lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plot_leverage_resid2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="leverage plot" src="/images/model_error/fig_5.png"&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we can see that there are no influential points, which indicates that the model is performing well.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-we-can-learn-from-leverage-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-we-can-learn-from-leverage-plot"&gt;What we can learn from Leverage plot&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The leverage plot shows us how much each data point is affecting the regression line. A data point with high leverage has a large influence on the regression line.&lt;/li&gt;
&lt;li&gt;If a data point has &lt;strong&gt;high leverage and a large residual&lt;/strong&gt;, it is called an influential point. Influential points can have a significant impact on the regression line and the overall model performance.&lt;/li&gt;
&lt;li&gt;In general, we want to see a &lt;strong&gt;uniform distribution&lt;/strong&gt; of the leverage values, with most of the data points falling within the range of 0 to 1. If there are outliers or data points with high leverage values, it may indicate that the model is not capturing the full range of variability in the data.&lt;/li&gt;
&lt;li&gt;We can use the leverage plot to identify &lt;strong&gt;influential points&lt;/strong&gt; that may be affecting the model performance. If we remove these points from the dataset and retrain the model, we can see if the model performance improves.&lt;/li&gt;
&lt;li&gt;The leverage plot can also be used to diagnose &lt;strong&gt;multicollinearity&lt;/strong&gt;, which is a situation where two or more predictor variables in the model are highly correlated with each other. In this case, the leverage values may be high for multiple data points, indicating that they are highly influential in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="cooks-distance-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="cooks-distance-plot"&gt;Cook's Distance Plot&lt;/h3&gt;
&lt;p&gt;The Cook's distance plot shows the influence of each data point on the regression coefficients. The Cook's distance is a measure of how much the regression coefficients change when a data point is removed from the dataset.&lt;/p&gt;
&lt;p&gt;A data point with a high Cook's distance has a large influence on the regression coefficients. If a data point has a high Cook's distance and a large residual, it is an influential point.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.graphics.regressionplots&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;plot_regress_exog&lt;/span&gt;

&lt;span class="c1"&gt;# Load example data&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_rdataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;datasets&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="c1"&gt;# Create X and y variables&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;speed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;dist&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Fit the linear regression model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Create Cook&amp;#39;s distance plot&lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plot_regress_exog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;speed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;influence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_influence&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;cooks_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;influence&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cooks_distance&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cooks_distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Cook&amp;#39;s Distance Plot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;proper plot: TBD
&lt;img alt="residual_plot" src="/images/model_error/fig_6.png"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="residual_plot" src="/images/model_error/fig_7.png"&gt;
In the above plot, we can see that there are no influential points, which indicates that the model is performing well.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-can-we-learn-from-a-cooks-distance-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-can-we-learn-from-a-cooks-distance-plot"&gt;What can we learn from a Cook's distance plot?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Observations with high Cook's distance values are influential and may be driving the model's predictions. We may want to remove these observations and re-fit the model.&lt;/li&gt;
&lt;li&gt;Observations with &lt;strong&gt;high leverage values&lt;/strong&gt; (i.e., observations with extreme values of the independent variables) can also have high Cook's distance values. We may want to examine these observations more closely to determine if they are valid or outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="actual-vs-predicted-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="actual-vs-predicted-plot"&gt;Actual vs. Predicted Plot&lt;/h3&gt;
&lt;p&gt;An actual vs. predicted plot is a scatter plot that shows the actual values on the y-axis and the predicted values on the x-axis. It's a simple and intuitive way to evaluate the model's performance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_regression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;r2_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;

&lt;span class="c1"&gt;# Generate synthetic data&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_regression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Split the data into training and test sets&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Fit the linear regression model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Compute the predicted values for the test set&lt;/span&gt;
&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot the actual vs. predicted values&lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()],&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;k--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Actual&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Predicted&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Actual vs. Predicted&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="residual_plot" src="/images/model_error/fig_8.png"&gt;
&lt;a id="what-can-we-learn-from-an-actual-vs-predicted-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-can-we-learn-from-an-actual-vs-predicted-plot"&gt;What can we learn from an actual vs. predicted plot?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If the points on the plot fall on a &lt;strong&gt;straight line&lt;/strong&gt;, it indicates that the model is making accurate predictions.&lt;/li&gt;
&lt;li&gt;If the points &lt;strong&gt;deviate from the straight line&lt;/strong&gt;, it suggests that the model is not making accurate predictions and may need to be improved.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="mean-absolute-error-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="mean-absolute-error-plot-vs-parameter"&gt;Mean Absolute Error Plot vs. parameter&lt;/h3&gt;
&lt;p&gt;The mean absolute error (MAE) is a measure of the average absolute difference between the predicted values and the actual values. A mean absolute error plot shows the MAE for different values of the model's hyperparameters.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; MAE is one of the metrics that can be used evaluate hyperparameters setting. One can use RMSE, R2 as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Lasso&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="c1"&gt;# Define a list of alpha values to test&lt;/span&gt;
&lt;span class="n"&gt;alphas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate the MAE for each alpha value&lt;/span&gt;
&lt;span class="n"&gt;mae_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Lasso&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mae&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mae_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mae&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Plot the MAE values against the alpha values&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alphas&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mae_values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alpha&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Mean Absolute Error&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Mean Absolute Error Plot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="residual_plot" src="/images/model_error/fig_9.png"&gt;
In this example, &lt;code&gt;alphas&lt;/code&gt; is a list of different regularization parameter values to test for a Lasso regression model. The code fits a model for each value of &lt;code&gt;alpha&lt;/code&gt; and calculates the mean absolute error on the test set. The resulting MAE values are then plotted against the alpha values to show how the MAE changes with different regularization strengths.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-can-we-learn-from-a-mean-absolute-error-plot"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-can-we-learn-from-a-mean-absolute-error-plot"&gt;What can we learn from a mean absolute error plot?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The plot can help us &lt;strong&gt;choose the best value of the hyperparameter&lt;/strong&gt; by identifying the value that minimizes the MAE.&lt;/li&gt;
&lt;li&gt;If the &lt;strong&gt;plot is noisy&lt;/strong&gt; and &lt;strong&gt;does not have a clear minimum&lt;/strong&gt;, it suggests that the hyperparameter may not be important for the model's performance.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we discussed some of the most insightful plots to visualize regression model prediction errors. We used the Boston Housing Dataset and a linear regression model to illustrate each type of plot. We provided Python code snippets to generate each type of plot.&lt;/p&gt;
&lt;p&gt;We learned that a good regression model will have residuals that are randomly scattered around the zero line, follow a normal distribution, and follow a straight line on the Q-Q plot. We also learned that influential points can affect the regression line and the regression coefficients.&lt;/p&gt;
&lt;p&gt;By visualizing the model's errors, we can gain a better understanding of how the model is performing and identify areas for improvement. It is essential to use multiple types of plots to get a complete picture of the model's performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="python"/></entry><entry><title>15 Tools for Document Deskewing and Dewarping</title><link href="http://127.0.0.1:8000/tools-for-doc-deskewing-and-dewarping/" rel="alternate"/><published>2022-02-11T00:00:00+01:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-02-11:/tools-for-doc-deskewing-and-dewarping/</id><summary type="html">&lt;p&gt;Sometimes input for document processing tasks such as OCR, table detection or text segmentation can be scanned or photo taken from hand that do not have ideal perspective - is rotated or spatially distorted in some way (warped document). If you are looking for my recommendations go straight to the last section of this article "Summary and recommendations".&lt;/p&gt;</summary><content type="html">&lt;p&gt;Sometimes input for document processing tasks such as OCR, table detection, or text segmentation can be scan, or photo taken from hand that does not have an ideal perspective - is rotated or spatially distorted in some way (warped document).
If you are looking for my recommendations go straight to the last section of this article &lt;a href="#summary-and-recommendations"&gt;Summary and recommendations&lt;/a&gt;. This article was inspired by a list of OCR-related projects posted on the list &lt;a href="https://github.com/zacharywhitley/awesome-ocr"&gt;awesome-ocr&lt;/a&gt;. To give readers intuition about the popularity of the project - information about GitHub stars is added to each project (as of Feb 2022 - time of writing this article). To differentiate actively developed projects from ones that don't get commits anymore - the date of the last commit was added.&lt;/p&gt;
&lt;h2 id="the-typical-approach-for-deskewing"&gt;The typical approach for deskewing&lt;/h2&gt;
&lt;p&gt;The deskewing is typically realized by using Canny Edge Detection and Hough Transform to determine the angle of rotation (skew) and then applying rotation in opposite direction.&lt;/p&gt;
&lt;h2 id="the-typical-approach-for-dewarping"&gt;The typical approach for dewarping&lt;/h2&gt;
&lt;p&gt;Reconstruction of spatial (3D) structure of the document is typically done using the Deep Learning approach.&lt;/p&gt;
&lt;h2 id="list-of-approaches-presented-in-this-article"&gt;List of approaches presented in this article&lt;/h2&gt;
&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#dewarping"&gt;Dewarping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#page-dewarp-11k-stars"&gt;Page dewarp (1.1k stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#moran-579-stars"&gt;MORAN (579 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dewarpnet-291-stars"&gt;DewarpNet (291 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#document-image-dewarping---algorithm-241-stars"&gt;Document Image Dewarping  - algorithm (241 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unproject-text-104-stars"&gt;Unproject Text (104 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#docuwarp-stars-83"&gt;Docuwarp (stars 83)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#book-content-segmentation-and-dewarping-under-construction-11-stars"&gt;Book content segmentation and dewarping (under construction) (11 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deskewing"&gt;Deskewing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unpaper-770-stars"&gt;Unpaper (770 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#alyn-222-stars"&gt;Alyn (222 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deskew-211-stars"&gt;Deskew (211 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#galfardeskew-102-stars"&gt;galfar/deskew (102 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#skew-correction-12-stars"&gt;Skew correction (12 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deskewing-stars-8"&gt;Deskewing (stars 8)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#text-deskewing-5-stars"&gt;Text deskewing (5 stars)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary-and-recommendations"&gt;Summary and recommendations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-to-use-for-deskewing"&gt;What to use for Deskewing?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-to-use-for-unwarping-and-deskewing"&gt;What to use for Unwarping and Deskewing?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-to-use-for-document-segmentation"&gt;What to use for Document Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="dewarping"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="dewarping"&gt;Dewarping&lt;/h1&gt;
&lt;p&gt;&lt;a id="page-dewarp-11k-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="page-dewarp"&gt;Page dewarp &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/mzucker/page_dewarp.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: Oct 2016, but the reworked version is actively developed (last commit: 24 Jan 2022)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/mzucker/page_dewarp"&gt;page_dewarp&lt;/a&gt; - Page dewarping and thresholding using a "cubic sheet" model&lt;/p&gt;
&lt;p&gt;&lt;img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_before_after.png" alt="before and after dewarp" style="zoom:50%;" /&gt;&lt;/p&gt;
&lt;p&gt;Read more here: &lt;a href="https://mzucker.github.io/2016/08/15/page-dewarping.html"&gt;Page dewarping&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: It is written in Python but using Python 2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since the original work of mzucker was written in Python 2 and not developed further there was the initiative to renovate the original scripts and there is &lt;a href="https://github.com/lmmx/page-dewarp"&gt;page-dewarp&lt;/a&gt; which is also available on Pypi, and it is pip installable (&lt;code&gt;pip install page-dewarp&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;a id="moran-579-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="moran"&gt;MORAN &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/Canjie-Luo/MORAN_v2.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 30 Jul 2019&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/Canjie-Luo/MORAN_v2"&gt;MORAN_v2&lt;/a&gt;  - A Multi-Object Rectified Attention Network for Scene Text Recognition&lt;/p&gt;
&lt;p&gt;&lt;img alt="img" src="https://github.com/Canjie-Luo/MORAN_v2/raw/master/demo/MORAN_v2.gif"&gt;&lt;/p&gt;
&lt;p&gt;Written in Python, using PyTorch&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: The project is only free for academic research purposes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="dewarpnet-291-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="dewarpnet"&gt;DewarpNet &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/cvlab-stonybrook/DewarpNet.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 6 Sep 2021&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/cvlab-stonybrook/DewarpNet"&gt;DewarpNet&lt;/a&gt; - This repository contains the codes for &lt;a href="https://www3.cs.stonybrook.edu/~cvl/projects/dewarpnet/storage/paper.pdf"&gt;&lt;strong&gt;DewarpNet&lt;/strong&gt;&lt;/a&gt; training.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/cvlab-stonybrook/DewarpNet/blob/master/dwnet.png"&gt;&lt;img src="https://github.com/cvlab-stonybrook/DewarpNet/raw/master/dwnet.png" alt="img" style="zoom:50%;" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://sagniklp.github.io/dewarpnet-webpage/"&gt;DewarpNet project web page&lt;/a&gt;. Here is how the authors characterize their solution in the abstract of the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DewarpNet, a deep learning approach for document image unwarping from a single image. Our insight is that the 3D geometry of the document not only determines the warping of its texture but also causes the illumination effects. Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1hJKCb4eF1AJih_dhZOJSF5VR-ZtRNaap/view"&gt;DewarpNet pre-trained models&lt;/a&gt; are available for download from Google Drive.&lt;/p&gt;
&lt;p&gt;&lt;a id="document-image-dewarping---algorithm-241-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="document-image-dewarping-algorithm"&gt;Document Image Dewarping  - algorithm &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/xellows1305/Document-Image-Dewarping.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 30 Sep 2019&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/xellows1305/Document-Image-Dewarping"&gt;Document-Image-Dewarping&lt;/a&gt;- Document image dewarping is approached by using text lines and line segments.&lt;/p&gt;
&lt;p&gt;In this repository, there is &lt;strong&gt;no public code&lt;/strong&gt; to use but just algorithm description and executable available for &lt;a href="http://ispl.synology.me:8480/sharing/uA2DTRA8U"&gt;download&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="unproject-text-104-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="unproject-text"&gt;Unproject Text &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/mzucker/unproject_text.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 13 Oct 2016&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/mzucker/unproject_text"&gt;unproject_text&lt;/a&gt; - &lt;strong&gt;Perspective recovery of text using transformed ellipses&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It is not exactly dewarping but perspective correction which is why it was placed in the dewarping section.&lt;/p&gt;
&lt;p&gt;Written in Python, it is pretty lightweight: using numpy, scipy, cv2,...&lt;/p&gt;
&lt;p&gt;In a nutshell, letters are replaced with ellipses and the axes of ellipses are used to determine what affine transformation is needed to correct perspective:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://mzucker.github.io/images/unproject_text/example0_ellipses.png" alt="contours with areas" style="zoom:50%;" /&gt;&lt;/p&gt;
&lt;p&gt;Image source: repository owner's &lt;a href="https://mzucker.github.io/2016/10/11/unprojecting-text-with-ellipses.html"&gt;writeup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More information about the method can be found in the &lt;a href="https://mzucker.github.io/2016/10/11/unprojecting-text-with-ellipses.html"&gt;article&lt;/a&gt; and &lt;a href="http://www.sciencedirect.com/science/article/pii/S0262885613001066"&gt;paper by Carlos Merino-Gracia et al.&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;a id="docuwarp-stars-83"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="docuwarp"&gt;Docuwarp &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/thomasjhuang/deep-learning-for-document-dewarping.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 18 Oct 2021&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/thomasjhuang/deep-learning-for-document-dewarping"&gt;Docuwarp&lt;/a&gt; - An application of high-resolution &lt;strong&gt;GANs to dewarp images&lt;/strong&gt; of perturbed documents.
This project is focused on dewarping document images through the usage of pix2pixHD, a GAN that is useful for the general image to image translation. The objective is to take images of documents that are warped, folded, crumpled, etc., and convert the image to a "dewarped" state by using pix2pixHD to train and perform inference.&lt;/p&gt;
&lt;p&gt;Written in Python.&lt;/p&gt;
&lt;p&gt;&lt;a id="book-content-segmentation-and-dewarping-under-construction-11-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="book-content-segmentation-and-dewarping-under-construction"&gt;Book content segmentation and dewarping (under construction)  &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/RaymondMcGuire/BOOK-CONTENT-SEGMENTATION-AND-DEWARPING.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last update of code: 2018&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/RaymondMcGuire/BOOK-CONTENT-SEGMENTATION-AND-DEWARPING"&gt;Book Content Segmentation and Dewarping&lt;/a&gt; - Using &lt;strong&gt;FCN (fully convolution network) to segment the image&lt;/strong&gt; into 3 parts (left page, right page, and background).&lt;/p&gt;
&lt;p&gt;The segmentation demo is available here: &lt;a href="https://raymondmgwx.github.io/?e=Project_BookContent&amp;amp;&amp;amp;theme=Image-Process-Content"&gt;https://raymondmgwx.github.io/?e=Project_BookContent&amp;amp;&amp;amp;theme=Image-Process-Content&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: that Data Augment and Dewarp Algorithm are in TODO of this project.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="deskewing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="deskewing"&gt;Deskewing&lt;/h1&gt;
&lt;p&gt;&lt;a id="unpaper-770-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="unpaper"&gt;Unpaper  &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/unpaper/unpaper.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 21 Jan 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/unpaper/unpaper"&gt;unpaper&lt;/a&gt; is a &lt;strong&gt;post-processing tool for scanned sheets of paper&lt;/strong&gt;, especially for book pages that have been scanned from previously created photocopies.&lt;/p&gt;
&lt;p&gt;The main purpose is to make scanned book pages better readable on screen after conversion to PDF. Additionally, unpaper might be useful to enhance the quality of scanned pages before performing optical character recognition (OCR).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;unpaper tries to clean scanned images by removing dark edges that appeared through scanning or copying on areas outside the actual page content&lt;/strong&gt; (e.g. dark areas between the left-hand side and the right-hand side of a double-sided book-page scan).&lt;/p&gt;
&lt;p&gt;The program also tries to detect misaligned centering and rotation of pages and will &lt;strong&gt;automatically straighten each page&lt;/strong&gt; by rotating it to the correct angle. This process is called "deskewing".&lt;/p&gt;
&lt;p&gt;Written mostly in C.&lt;/p&gt;
&lt;p&gt;&lt;a id="alyn-222-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="alyn"&gt;Alyn  &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/kakul/Alyn.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 14 Jun 2017&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/kakul/Alyn"&gt;Alyn&lt;/a&gt; - &lt;strong&gt;Skew detection and correction&lt;/strong&gt; in images containing text. It uses Canny Edge Detection and  Hough Transform to determine skew.&lt;/p&gt;
&lt;p&gt;How the Alyns' skew detection works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Converts the image to greyscale&lt;/li&gt;
&lt;li&gt;Performs Canny Edge Detection on the Image&lt;/li&gt;
&lt;li&gt;Calculates the Hough Transform values&lt;/li&gt;
&lt;li&gt;Determines the peaks&lt;/li&gt;
&lt;li&gt;Determines the deviation of each peaks from 45-degree angle&lt;/li&gt;
&lt;li&gt;Segregates the detected peaks into bins&lt;/li&gt;
&lt;li&gt;Chooses the probable skew angle using the value in the bins&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alyn is written in Python can be installed with pip (&lt;code&gt;pip install allyn&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a id="deskew-211-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="deskew"&gt;Deskew &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/sbrunner/deskew.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit : 10 Feb 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/sbrunner/deskew"&gt;deskew&lt;/a&gt; - Library used to &lt;strong&gt;deskew a scanned document&lt;/strong&gt;. Skew detection and correction in images containing text
Written in Python, lightweight. Inspired by &lt;a href="#alyn-222-stars"&gt;Alyn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="galfardeskew-102-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="galfardeskew"&gt;galfar/deskew &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/galfar/deskew.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 6 Jan 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/galfar/deskew"&gt;galfar/deskew&lt;/a&gt; - Deskew is a &lt;strong&gt;command-line tool&lt;/strong&gt; for deskewing scanned text documents. It uses Hough transform to detect "text lines" in the image. As an output, you get an image rotated so that the lines are horizontal.&lt;/p&gt;
&lt;p&gt;There are binaries built for these platforms: Win64, Win32, Linux 64bit, macOS, and Linux ARMv7. GUI frontend for this CLI tool is available as well (Windows, Linux, and macOS),&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: It is written in Pascal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="skew-correction-12-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="skew-correction"&gt;Skew correction &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/prajwalmylar/skew_correction.svg?logo=github"&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/prajwalmylar/skew_correction"&gt;skew_correction&lt;/a&gt; - &lt;strong&gt;Deskewing images&lt;/strong&gt; with slanted content by finding the deviation using Canny Edge Detection.&lt;/p&gt;
&lt;p&gt;&lt;a id="deskewing-stars-8"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="deskewing_1"&gt;Deskewing &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/sauravbiswasiupr/deskewing.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 12 Jan 2014&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/sauravbiswasiupr/deskewing"&gt;deskewing&lt;/a&gt; - Contains code to &lt;strong&gt;deskew images using MLPs, LSTMs and LLS transformations.&lt;/strong&gt;
Written in Python.&lt;/p&gt;
&lt;p&gt;&lt;a id="text-deskewing-5-stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="text-deskewing"&gt;Text deskewing &lt;img alt="github stars shield" src="https://img.shields.io/github/stars/dehaisea/text_deskewing.svg?logo=github"&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Last commit: 9 Mar 2018&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/dehaisea/text_deskewing"&gt;text_deskewing&lt;/a&gt; - &lt;strong&gt;Rotate text images if they are not straight&lt;/strong&gt; for better text detection and recognition. Uses Canny Edge Detection and probabilistic Hough Transform.&lt;/p&gt;
&lt;p&gt;It is written in Python and the repository does not contain a lot of code - it is easy to follow and learn how those simple techniques can be used to desk the text.&lt;/p&gt;
&lt;p&gt;&lt;a id="summary-and-recommendations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="summary-and-recommendations"&gt;Summary and recommendations&lt;/h1&gt;
&lt;p&gt;&lt;a id="what-to-use-for-deskewing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-to-use-for-deskewing"&gt;What to use for Deskewing?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you need to &lt;strong&gt;deskew&lt;/strong&gt; and additionally &lt;strong&gt;clean-up&lt;/strong&gt; document from scanning artifacts use: &lt;a href="https://github.com/unpaper/unpaper"&gt;unpaper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you just need to &lt;strong&gt;correct the rotation&lt;/strong&gt; of the document use: &lt;a href="https://github.com/kakul/Alyn"&gt;Alyn&lt;/a&gt; or &lt;a href="https://github.com/sbrunner/deskew"&gt;deskew&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you want to &lt;strong&gt;learn about using Edge Detection and Hough Transform&lt;/strong&gt; for document deskewing you might want to have look at: &lt;a href="https://github.com/dehaisea/text_deskewing"&gt;text_deskewing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="what-to-use-for-unwarping-and-deskewing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="what-to-use-for-unwarping-and-deskewing"&gt;What to use for Unwarping and Deskewing?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For dewarping book pages that have &lt;strong&gt;smooth bendings&lt;/strong&gt; consider using &lt;a href="https://github.com/lmmx/page-dewarp"&gt;page-dewarp&lt;/a&gt; (renovated version of popular &lt;a href="https://github.com/mzucker/page_dewarp"&gt;page_dewarp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For more complex dewarping including e.g. &lt;strong&gt;folded pages&lt;/strong&gt; use Deep Learning-based solutions such as &lt;a href="https://github.com/cvlab-stonybrook/DewarpNet"&gt;DewarpNet&lt;/a&gt; or &lt;a href="https://github.com/thomasjhuang/deep-learning-for-document-dewarping"&gt;Docuwarp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If you are working with flat pages and you just need to &lt;strong&gt;correct perspective&lt;/strong&gt; &lt;a href="https://github.com/mzucker/unproject_text"&gt;unproject_text&lt;/a&gt; might be the right tool for you.
&lt;a id="what-to-use-for-document-segmentation"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-to-use-for-document-segmentation"&gt;What to use for Document Segmentation&lt;/h2&gt;
&lt;p&gt;Document segmentation was not in the scope of this article. You can check &lt;a href="https://github.com/zacharywhitley/awesome-ocr"&gt;awesome-ocr&lt;/a&gt; section on Document Segmentation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zacharywhitley/awesome-ocr"&gt;awesome-ocr&lt;/a&gt; - a rich collection of OCR-related projects and tools&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The image used in the header of this article comes from the paper &lt;a href="https://www3.cs.stonybrook.edu/~cvl/projects/dewarpnet/storage/paper.pdf"&gt;DewarpNet: Single-Image Document Unwarping With Stacked 3D and 2D Regression Networks&lt;/a&gt;. &lt;em&gt;Sagnik Das, Ke Ma, Zhixin Shu, Dimitris Samaras, Roy Shilkrot&lt;/em&gt;; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 131-140&lt;/li&gt;
&lt;li&gt;All other images in the article come from the project owners or related sources (papers, articles)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="document-intelligence"/><category term="document-processing"/><category term="computer-vision"/><category term="digital-image-processing"/><category term="canny-edge-detector"/><category term="hough-transform"/><category term="deep-learning"/></entry><entry><title>How to Get Most of GitHub Copilot</title><link href="http://127.0.0.1:8000/how-to-get-most-of-github-copilot/" rel="alternate"/><published>2022-01-25T00:00:00+01:00</published><updated>2023-08-24T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2022-01-25:/how-to-get-most-of-github-copilot/</id><summary type="html">&lt;p&gt;This post describes techniques that help to get most accurate suggestions from the GitHub Copilot "Your AI pair programmer". For those who never heard of Copilot there is short introduction, if you already know Copilot - you can jump directly to section 4 - "How to get most of GitHub Copilot".&lt;/p&gt;</summary><content type="html">&lt;h2 id="tldr"&gt;TLDR&lt;/h2&gt;
&lt;p&gt;To get accurate autosuggestion from Copilot you can:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Modify signature&lt;/strong&gt; of received suggestion to one that capture all relevant inputs and wait for new suggestion.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write pseudocode&lt;/strong&gt;: describe steps for the function with complex business logic.&lt;/li&gt;
&lt;li&gt;For functions with complex business logic &lt;strong&gt;divide them into multiple smaller functions&lt;/strong&gt;. Purpose of smaller function is easier to describe with just function name or the comment/docstring.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="outline"&gt;Outline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;What is GitHub Copilot?&lt;/li&gt;
&lt;li&gt;How to get it?&lt;/li&gt;
&lt;li&gt;How to use it?&lt;/li&gt;
&lt;li&gt;How to get most of GitHub Copilot?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="1-what-is-github-copilot"&gt;1. What is GitHub Copilot?&lt;/h2&gt;
&lt;p&gt;In brief: &lt;a href="https://copilot.github.com/"&gt;GitHub Copilot&lt;/a&gt;, gives suggestions for whole lines or entire functions right inside your editor as you type. Currently, there is support for Visual Studio Code, Neovim, and JetBrains IDEs like PyCharm and IntelliJ IDEA.
GitHub Copilot is powered by the OpenAI Codex AI system, and, as authors claimed, trained on public internet text and billions of lines of code.&lt;/p&gt;
&lt;p&gt;The way how it is used is that human programmer provides comment with intention what code he/she would like to get, and as autocomplete suggestion gets whole functions of classes. GitHub Copilot uses not only exact comment test but also context of the code that is expected to be auto-generated.&lt;/p&gt;
&lt;p&gt;&lt;img alt="diagram GitHub Copilot" src="/images/copilot/diagram-github-copilot-739x355-3516979431.png"&gt;
Figure 1. Data exchange between GitHub Copilot service, OpenAI Codex Model and Private Code and IDE. Image from: copilot.github.com&lt;/p&gt;
&lt;h2 id="2-how-to-get-it"&gt;2. How to get it?&lt;/h2&gt;
&lt;p&gt;Access to GitHub Copilot is limited to a small group of testers during the technical preview. You can sign up and join the wait-list to try it out &lt;a href="https://github.com/features/copilot/signup"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="3-how-to-use-it"&gt;3. How to use it?&lt;/h2&gt;
&lt;p&gt;Provide a docstring, comment, function name, or the piece of code itself, GitHub Copilot will use the context you’ve provided and generate code as suggestion.&lt;/p&gt;
&lt;h2 id="4-how-to-get-most-of-github-copilot"&gt;4. How to get most of GitHub Copilot?&lt;/h2&gt;
&lt;p&gt;After using the GitHub Copilot service for almost three months, I was amazed how useful suggestions from Copilot can be. Of course, it is not ideal - suggestions are not always meeting my expectations. Sometimes it took few attempts to formulate my expectations to get the code that was looking good to me.
What I learned by the time of writing this post, are the few simple rules that can increase accuracy of the suggestions. I will outline them below. I'm using three main strategies to communicate my expectations with the GitHub Copilot service:&lt;/p&gt;
&lt;h3 id="simple-case-write-short-comment"&gt;Simple case - write short comment&lt;/h3&gt;
&lt;p&gt;For the simple cases: just write  comment that describe behavior for the code that you want to get as suggestion.
As example - we want to get a function that will return paths to all images in given directory and sub-directories. We are interested only in images of given type (specified file extensions).&lt;/p&gt;
&lt;p&gt;Here is first attempt to get code auto completion from GitHub copilot.&lt;/p&gt;
&lt;p&gt;&lt;img alt="img" src="/images/copilot/copilot_1.gif"&gt;&lt;/p&gt;
&lt;p&gt;Figure 2. Autosuggestion obtained as auto-completion to comment describing behavior of the function.&lt;/p&gt;
&lt;p&gt;The example above illustrate how we started with initial suggestion for the comment and how the suggested  comment was refined as suggestions were provided by Copilot. The first suggestion for the code was very near the expectation - one thing that is missing is the support for selecting file extensions of interest.&lt;/p&gt;
&lt;p&gt;Let's try to modify function signature and add list of extensions (and type hints):
old signature was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_images_in_dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;new signature:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_images_in_dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;extensions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this time resulting code that meet requirements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_images_in_dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;extensions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Get list of images in a directory, recursively. Include extensions:&lt;/span&gt;
&lt;span class="sd"&gt;    .jpg, .png, cr2, etc.&lt;/span&gt;
&lt;span class="sd"&gt;    :param directory: str&lt;/span&gt;
&lt;span class="sd"&gt;    :param extensions: list&lt;/span&gt;
&lt;span class="sd"&gt;    :return: list&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dirs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;walk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;extensions&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
                &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Other examples of functions that short comment can sufficiently describe responsibility:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# get image of the day from bing (or Wikipedia/NASA/...)

# convert a list of dictionaries to a list of lists

# get information using public API from stocktwits
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="simple-case-write-function-name"&gt;Simple case - write function name&lt;/h2&gt;
&lt;p&gt;Starting with descriptive function name can work as well:
 &lt;img alt="img" src="/images/copilot/copilot_2.gif"&gt;&lt;/p&gt;
&lt;h3 id="more-complex-cases-write-pseudocode"&gt;More complex cases - write pseudocode&lt;/h3&gt;
&lt;p&gt;For the complex cases: write line of comment that describe behavior for the code that you want to get as suggestion.
Example:&lt;/p&gt;
&lt;p&gt;Comment with purpose of the function and pseudocode describing elements of solution&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Check if two nested lists have the same dictionaries&lt;/span&gt;
&lt;span class="c1"&gt;# pseudocode:&lt;/span&gt;
&lt;span class="c1"&gt;# 1. Check if two top-level lists have the same length&lt;/span&gt;
&lt;span class="c1"&gt;# 2. Check if each pair of lower-level lists have the same length&lt;/span&gt;
&lt;span class="c1"&gt;# 3. Check if each pair of lower-level lists have the same keys and values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Resulting code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;check_lists_equal_nested&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;list_2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Check if two nested lists have the same dictionaries&lt;/span&gt;
&lt;span class="sd"&gt;    :param list_1: list&lt;/span&gt;
&lt;span class="sd"&gt;    :param list_2: list&lt;/span&gt;
&lt;span class="sd"&gt;    :return: bool&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# 1. Check if two top-level lists have the same length&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

    &lt;span class="c1"&gt;# 2. Check if each pair of lower-level lists have the same length&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

    &lt;span class="c1"&gt;# 3. Check if each pair of lower-level lists have the same keys and values&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;list_1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;list_2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="img" src="/images/copilot/copilot_1.gif"&gt;&lt;/p&gt;
&lt;h2 id="more-complex-cases-write-pseudocode_1"&gt;More complex cases - write pseudocode&lt;/h2&gt;
&lt;p&gt;For functions with complex business logic &lt;strong&gt;divide them into multiple smaller functions&lt;/strong&gt;. Purpose of smaller function is easier to describe with just function name or the comment/docstring. This is well known rule (at least from ancient Greeks) allowing to break big problem into smaller that are much easier to address.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Divide et Impera!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Divide and conquer!&lt;/em&gt; : in politics and sociology, it is &lt;em&gt;a strategy of gaining and maintaining power by breaking up larger concentrations of power into pieces that individually have less power than the one implementing the strategy.&lt;/em&gt; &lt;a href="https://en.wikipedia.org/wiki/Divide_and_rule"&gt;wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This rule also works when applied to using GitHub Copilot, I will not go into details since the method is pretty obvious and if not one can easily find multiple texts on applying "Divide and conquer" applications to software engineering.&lt;/p&gt;
&lt;h3 id="my-impressions-after-using-copilot"&gt;My impressions after using Copilot&lt;/h3&gt;
&lt;p&gt;I have started using Copilot in November 2021. When I had copilot plugin enabled I put more attention to use meaningful function names expecting the reward in form of auto-suggested code that will fit my needs.&lt;/p&gt;
&lt;p&gt;I found Copilot great for using python for automation tasks - many simple functions that somebody else for sure has written someday, but it is too expensive to search e.g. GitHub snippet or code. Copilot comes in handy as replacement of gists of basic functions for manipulating text, files, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;warp speed image comes from &lt;a href="https://videohive.net/item/warp-speed-effect-v10/47100"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="github"/><category term="coding"/><category term="ai"/></entry><entry><title>Understanding Micro and Macro Averages in Multiclass Multilabel Problems</title><link href="http://127.0.0.1:8000/micro-and-macro-averages-in-multiclass-multilabel-problems/" rel="alternate"/><published>2021-12-22T00:00:00+01:00</published><updated>2023-01-17T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-12-22:/micro-and-macro-averages-in-multiclass-multilabel-problems/</id><summary type="html">&lt;p&gt;Learn about micro and macro averages in multiclass multilabel problems, the difference between multiclass and multilabel problems and when to use micro and macro averages.&lt;/p&gt;</summary><content type="html">&lt;p&gt;When working with multiclass multilabel problems, it's important to understand how to calculate micro and macro averages in order to evaluate the performance of a model. Micro and macro averages provide different perspectives on the performance of a model and are useful in different situations. In this article, we will explore the concepts of micro and macro averages and how to calculate them.&lt;/p&gt;
&lt;h2 id="multiclass-vs-multilabel"&gt;Multiclass vs Multilabel&lt;/h2&gt;
&lt;p&gt;Before diving into micro and macro averages, let's first understand the difference between multiclass and multilabel problems.&lt;/p&gt;
&lt;p&gt;In a &lt;strong&gt;multiclass&lt;/strong&gt; problem, there is only &lt;strong&gt;one correct label&lt;/strong&gt; per example. For example, in a problem of classifying animals into different categories (e.g. mammals, birds, reptiles, etc.) each example can only have one correct label.&lt;/p&gt;
&lt;p&gt;On the other hand, in a &lt;strong&gt;multilabel&lt;/strong&gt; problem, there can be &lt;strong&gt;multiple correct labels&lt;/strong&gt; per example. For example, in a problem of categorizing music into different genres (e.g. rock, pop, hip hop, etc.) an example can belong to multiple genres.&lt;/p&gt;
&lt;h2 id="micro-and-macro-averages"&gt;Micro and Macro Averages&lt;/h2&gt;
&lt;p&gt;In multiclass multilabel problems, there are two commonly used measures of performance: micro and macro averages.&lt;/p&gt;
&lt;h3 id="micro-average"&gt;Micro Average&lt;/h3&gt;
&lt;p&gt;Micro average is calculated by taking the total number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) and then using these counts to calculate the precision, recall, and F1-score. Micro average gives more weight to the majority class and is particularly &lt;strong&gt;useful when the classes are imbalanced&lt;/strong&gt;. The formula for calculating micro average is as follows:&lt;/p&gt;
&lt;p&gt;$$Precision_{micro} = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} (TP_i + FP_i)}$$&lt;/p&gt;
&lt;p&gt;$$Recall_{micro} = \frac{\sum_{i=1}^{n} TP_i}{\sum_{i=1}^{n} (TP_i + FN_i)}$$&lt;/p&gt;
&lt;p&gt;$$F1-score_{micro} = 2 * \frac{Precision_{micro} * Recall_{micro}}{Precision_{micro} + Recall_{micro}}$$&lt;/p&gt;
&lt;h3 id="macro-average"&gt;Macro Average&lt;/h3&gt;
&lt;p&gt;Macro average, on the other hand, calculates the performance of each class individually and then takes the unweighted mean of the class-wise performance. Macro average gives equal weight to each class and is &lt;strong&gt;useful when all classes are of equal importance&lt;/strong&gt;. The formula for calculating macro average is as follows:&lt;/p&gt;
&lt;p&gt;$$Precision_{macro} = \frac{1}{n} \sum_{i=1}^{n} \frac{TP_i}{TP_i + FP_i}$$&lt;/p&gt;
&lt;p&gt;$$Recall_{macro} = \frac{1}{n} \sum_{i=1}^{n} \frac{TP_i}{TP_i + FN_i}$$&lt;/p&gt;
&lt;p&gt;$$F1-score_{macro} = 2 * \frac{Precision_{macro} * Recall_{macro}}{Precision_{macro} + Recall_{macro}}$$&lt;/p&gt;
&lt;p&gt;where n is the number of classes.&lt;/p&gt;
&lt;p&gt;It's important to note that in multilabel problems, these metrics are calculated for each label independently and then combined by taking the mean, sum or some other combination.&lt;/p&gt;
&lt;h2 id="choosing-between-micro-and-macro-averages"&gt;Choosing between Micro and Macro averages&lt;/h2&gt;
&lt;p&gt;The choice between micro and macro averages depends on the specific problem and the desired perspective on the model's performance.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Micro averages&lt;/strong&gt; are useful when the &lt;strong&gt;classes are imbalanced&lt;/strong&gt; and it is important to have a better understanding of the model's &lt;strong&gt;performance on the majority class&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Macro averages,&lt;/strong&gt; on the other hand, are useful when &lt;strong&gt;all classes are of equal importance&lt;/strong&gt; and you want to have a better understanding of the model's &lt;strong&gt;performance on each class individually&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In some cases, it may be useful to report both micro and macro averages to get a more comprehensive understanding of the model's performance.&lt;/p&gt;
&lt;p&gt;It's also important to note that micro and macro averages are not the only metrics to evaluate the performance of a model in multiclass multilabel problems. Other metrics such as ROC-AUC, &lt;a href="https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics"&gt;Hamming loss&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Jaccard_index"&gt;Jaccard similarity&lt;/a&gt; can also be used.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Micro and macro averages are important measures of performance in multiclass multilabel problems. They provide different perspectives on the model's performance and should be used depending on the specific problem and desired perspective. It's important to consider the class imbalance and the relative importance of each class when choosing between micro and macro averages.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading image from &lt;a href="https://unsplash.com/photos/pv5SUbgRRIU"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@vackground"&gt;vackground.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="classification"/><category term="metrics"/><category term="precision"/><category term="recal"/><category term="f1"/><category term="micro-average"/><category term="macro-average"/><category term="multi-class"/><category term="multi-label"/></entry><entry><title>Setup VS Code as NIM IDE</title><link href="http://127.0.0.1:8000/vscode-as-nim-ide/" rel="alternate"/><published>2021-12-02T00:00:00+01:00</published><updated>2023-01-13T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-12-02:/vscode-as-nim-ide/</id><summary type="html">&lt;p&gt;Learn how to set up VSCode as a Nim IDE from scratch. Includes instructions for downloading VSCode, installing the Nim extension, configuring settings, and debugging Nim code.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://nim-lang.org/"&gt;Nim&lt;/a&gt; is a statically-typed, imperative programming language that is designed to be efficient, expressive, and easy to learn. It is often compared to other programming languages like Python, C, and Go. One of the great things about Nim is that it can be used to create efficient command-line tools, web servers, and desktop applications.&lt;/p&gt;
&lt;p&gt;In this tutorial, we will go over how to set up &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; (VS Code) as a Nim IDE (Integrated Development Environment) from scratch. This includes downloading VS Code, installing the Nim extension, and configuring the necessary settings for debugging and code completion.&lt;/p&gt;
&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation-steps"&gt;Installation steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-download-and-install-vscode"&gt;1. Download and Install VSCode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-install-the-nim-extension"&gt;2 Install the Nim Extension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-configure-the-settings"&gt;3. Configure the Settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-download-nim"&gt;4. Download Nim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-create-a-new-nim-file"&gt;5. Create a new Nim file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-debugging"&gt;6. Debugging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="installation-steps"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="installation-steps"&gt;Installation steps&lt;/h2&gt;
&lt;p&gt;&lt;a id="1-download-and-install-vscode"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-download-and-install-vscode"&gt;1. Download and Install VSCode&lt;/h3&gt;
&lt;p&gt;The first step is to download and install VS Code. You can do this by visiting the VS Code website (&lt;a href="https://code.visualstudio.com/"&gt;https://code.visualstudio.com/&lt;/a&gt;) and selecting the appropriate download for your operating system. Once the download is complete, simply follow the instructions to install VSCode on your computer.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-install-the-nim-extension"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-install-the-nim-extension"&gt;2 Install the Nim Extension&lt;/h3&gt;
&lt;p&gt;Once VS Code is installed, open it and press &lt;kbd&gt;Ctrl+Shift+X&lt;/kbd&gt; (&lt;kbd&gt;Cmd+Shift+X&lt;/kbd&gt; on macOS) to open the Extensions pane. Search for &lt;code&gt;"Nim"&lt;/code&gt; and select the "Nim" extension by "Konstantin Zaitsev" and click on the install button. This extension provides syntax highlighting, code completion, and other features for Nim development.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-configure-the-settings"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-configure-the-settings"&gt;3. Configure the Settings&lt;/h3&gt;
&lt;p&gt;Now that the Nim extension is installed, you can configure the settings to make your Nim development experience even better. In VSCode, go to &lt;code&gt;File &amp;gt; Preferences &amp;gt; Settings&lt;/code&gt; (&lt;code&gt;Code &amp;gt; Settings &amp;gt; Settings&lt;/code&gt; on macOS) and search for "Nim". From here, you can configure settings such as the path to the Nim compiler, the formatting of the code, and the behavior of the code completion.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-download-nim"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-download-nim"&gt;4. Download Nim&lt;/h3&gt;
&lt;p&gt;You will also have to download the Nim compiler, which you can do from the official website (&lt;a href="https://nim-lang.org/install.html"&gt;https://nim-lang.org/install.html&lt;/a&gt;). Once downloaded, you can add the bin path of the Nim compiler to your environment variable. On macOS, you can &lt;code&gt;brew install nim&lt;/code&gt;.
Check if Nim is installed with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;nim&lt;span class="w"&gt; &lt;/span&gt;--version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;you should see something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Nim Compiler Version 1.6.10 [MacOSX: amd64]
Compiled at 2022-11-21
Copyright (c) 2006-2021 by Andreas Rumpf

active boot switches: -d:release -d:nimUseLinenoise
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now write and compile first program. Create text file &lt;code&gt;hello.nim&lt;/code&gt; with content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;echo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Hello world!&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then compile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;output&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nl"&gt;Hint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;used&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;file&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/usr/local/Cellar/nim/1.6.10/nim/config/nim.cfg&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nl"&gt;Hint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;used&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;file&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/usr/local/Cellar/nim/1.6.10/nim/config/config.nims&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;.........................................................&lt;/span&gt;
&lt;span class="nl"&gt;CC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Cellar&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;1.6.10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;private&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;digitsutils&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;
&lt;span class="nl"&gt;CC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Cellar&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;1.6.10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;system&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dollars&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;
&lt;span class="nl"&gt;CC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Cellar&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;1.6.10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;system&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;
&lt;span class="nl"&gt;CC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;..&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Cellar&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;1.6.10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;system&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;
&lt;span class="nl"&gt;CC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;
&lt;span class="nl"&gt;Hint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Link&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nl"&gt;Hint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;gc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;refc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;opt&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;none&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;BUILD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nl"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;release&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;generates&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;faster&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;26644&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.776&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;31.555&lt;/span&gt;&lt;span class="n"&gt;MiB&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;peakmem&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;proj&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Users&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;krystian&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;safjan&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;out&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Users&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;krystian&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;safjan&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;hello&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;SuccessX&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Time to run it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./hello
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Hello world!
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="5-create-a-new-nim-file"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-create-a-new-nim-file"&gt;5. Create a new Nim file&lt;/h3&gt;
&lt;p&gt;Now that everything is set up in the system, you can create a new Nim file in VS Code by going to &lt;code&gt;File &amp;gt; New File&lt;/code&gt; and then saving the file with a &lt;code&gt;.nim&lt;/code&gt; file extension. You can now start writing Nim code, and use the features provided by the extension such as syntax highlighting, code completion, and debugging.&lt;/p&gt;
&lt;p&gt;For example, create file &lt;code&gt;hello2.nim&lt;/code&gt; with content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;echo &amp;quot;Hello world!!!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and hit the Run button (triangle), and in the output window you should see:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;nim&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nl"&gt;verbosity&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="nl"&gt;hints&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;off&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;/Users/krystian.safjan/hello2.nim&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;Hello&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;world&lt;/span&gt;&lt;span class="err"&gt;!!!&lt;/span&gt;



&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Done&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exited&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.959&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="6-debugging"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-debugging"&gt;6. Debugging&lt;/h3&gt;
&lt;p&gt;For the information on debugging in VS Code, you can refer to the official documentation &lt;a href="https://code.visualstudio.com/docs/editor/debugging"&gt;Debugging in Visual Studio Code&lt;/a&gt; and guide specific for the Nim language: &lt;a href="https://github.com/jasonprogrammer/nim-debug-example"&gt;A walkthrough for setting up debugging of Nim code in VSCode&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Setting up VSCode as a Nim IDE is relatively easy, and provides a great development experience. The Nim extension provides a wide range of features that make Nim development much more efficient and enjoyable.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nim-lang/Nim/wiki/Editor-Support"&gt;Editor Support · nim-lang/Nim Wiki · GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jasonprogrammer/nim-debug-example"&gt;A walkthrough for setting up debugging of Nim code in VSCode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading photo from &lt;a href="https://unsplash.com/photos/w7ZyuGYNpRQ"&gt;Unsplash&lt;/a&gt; by &lt;a href="https://unsplash.com/@ikukevk"&gt;Kevin Ku&lt;/a&gt;&lt;/p&gt;</content><category term="Howto"/><category term="nim"/><category term="ide"/><category term="programming"/><category term="vscode"/><category term="debugging"/><category term="nimrod"/></entry><entry><title>Top Popular ZSH Plugins on GitHub (2021)</title><link href="http://127.0.0.1:8000/top-popular-zsh-plugins-on-github-2021/" rel="alternate"/><published>2021-11-29T00:00:00+01:00</published><updated>2021-11-29T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-11-29:/top-popular-zsh-plugins-on-github-2021/</id><summary type="html">&lt;p&gt;Explore the most popular Zsh plugins from the 1800+ options on the Awesome Zsh plugins GitHub project. See which ones have the highest number of stars from the Zsh community.&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This article was written in 2021, for a more recent version of the ranking see: &lt;a href="../top-popular-zsh-plugins-on-github-2023/"&gt;2023&lt;/a&gt;.
The older articles have description of the selected, interesting tools that are not in this article - so you might also to visit the older editions of the survey.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The collection &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt; of projects that can be useful for Zsh users grew substantially from the first, 2018 release of my article on Top-popular Zsh plugins - from 800+ to 1800+.  In this article, I'm listing top-popular tools that might be interesting for Zsh users or in most cases console users. I have divided them into four categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt; - general tools that are popular among console lovers, in most cases not limited to Zsh&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frameworks - tools for managing Zsh configuration and plugins&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompts&lt;/strong&gt; - projects that help to configure shell prompts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python tools&lt;/strong&gt; - tools that ease work with Python virtual environments&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="github-stars-as-an-indicator-of-project-popularity"&gt;GitHub Stars as an indicator of project popularity&lt;/h1&gt;
&lt;p&gt;One of the ways to estimate project popularity is to look at the number of stars. GitHub users often use stars as a kind of bookmark to mark interesting projects to come back to later. People that marked a project with a star are called "Star gazers". For sake of this article, to extract the "most popular" plugin projects I have used stars.&lt;/p&gt;
&lt;h1 id="top-popular-plugins-as-of-november-2021"&gt;Top popular plugins as of November 2021&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description of the Github project repo&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/nvbn/thefuck"&gt;thefuck&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Magnificent app which corrects your previous console command.&lt;/td&gt;
&lt;td&gt;64.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/junegunn/fzf"&gt;fzf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A command-line fuzzy finder&lt;/td&gt;
&lt;td&gt;40.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sharkdp/bat"&gt;bat&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cat(1) clone with wings.&lt;/td&gt;
&lt;td&gt;30.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/BurntSushi/ripgrep"&gt;ripgrep&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ripgrep recursively searches directories for a regex pattern while respecting your gitignore&lt;/td&gt;
&lt;td&gt;28.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/romkatv/powerlevel10k"&gt;powerlevel10k&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A Zsh theme&lt;/td&gt;
&lt;td&gt;22.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-autosuggestions"&gt;zsh-autosuggestions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish-like autosuggestions for zsh&lt;/td&gt;
&lt;td&gt;19.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/altercation/solarized"&gt;solarized&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;precision color scheme for multiple applications (terminal, vim, etc.) with both dark/light modes&lt;/td&gt;
&lt;td&gt;15.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/so-fancy/diff-so-fancy"&gt;diff-so-fancy&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Good-lookin' diffs. Actually… nah… The best-lookin' diffs.&lt;/td&gt;
&lt;td&gt;14.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/rupa/z"&gt;z&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;z - jump around&lt;/td&gt;
&lt;td&gt;13.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/bhilburn/powerlevel9k"&gt;powerlevel9k&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Powerlevel9k was a tool for building a beautiful and highly functional CLI, customized for you. P9k had a substantial impact on CLI UX, and its legacy is now continued by P10k.&lt;/td&gt;
&lt;td&gt;13.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/wting/autojump"&gt;autojump&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cd command that learns - easily navigate directories from the command line&lt;/td&gt;
&lt;td&gt;13.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-syntax-highlighting"&gt;zsh-syntax-highlighting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish shell-like syntax highlighting for Zsh.&lt;/td&gt;
&lt;td&gt;13.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/asdf-vm/asdf"&gt;asdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Extendable version manager with support for Ruby, Node.js, Elixir, Erlang &amp;amp; more&lt;/td&gt;
&lt;td&gt;12.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/bcicen/ctop"&gt;ctop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Top-like interface for container metrics&lt;/td&gt;
&lt;td&gt;12.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ahmetb/kubectx"&gt;kubectx&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Faster way to switch between clusters and namespaces in kubectl&lt;/td&gt;
&lt;td&gt;11.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/tmuxinator/tmuxinator"&gt;tmuxinator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Manage complex tmux sessions easily&lt;/td&gt;
&lt;td&gt;11.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/jonas/tig"&gt;tig&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Text-mode interface for git&lt;/td&gt;
&lt;td&gt;10.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/denisidoro/navi"&gt;navi&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An interactive cheatsheet tool for the command-line&lt;/td&gt;
&lt;td&gt;10.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/peco/peco"&gt;peco&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Simplistic interactive filtering tool&lt;/td&gt;
&lt;td&gt;6.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I have selected two projects that I haven't described in previous years but I'm a happy user of both.&lt;/p&gt;
&lt;h1 id="1-diff-so-fancy"&gt;1. &lt;a href="https://github.com/so-fancy/diff-so-fancy"&gt;diff-so-fancy&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As authors call it &lt;code&gt;diff-so-fancy&lt;/code&gt; strives to make your diffs human-readable instead of machine-readable. This helps improve code quality and helps you spot defects faster and it does what it promises. See the screenshot that compares vanilla &lt;code&gt;git diff&lt;/code&gt; vs. &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;diff-so-fancy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://user-images.githubusercontent.com/3429760/32387617-44c873da-c082-11e7-829c-6160b853adcb.png"&gt;&lt;img alt="diff-highlight vs diff-so-fancy" src="https://user-images.githubusercontent.com/3429760/32387617-44c873da-c082-11e7-829c-6160b853adcb.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You might have noticed that differences are spotted not only on the line level but also on the character level.&lt;/p&gt;
&lt;h1 id="2-tig"&gt;2. &lt;a href="https://github.com/jonas/tig"&gt;tig&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Tig is a ncurses-based text-mode interface for git. It functions mainly as a Git repository browser, but can also assist in staging changes for commit at the chunk level. Tig allows for convenient staging files in an interactive mode:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/zsh/tig_status.jpg" alt="tig status view" style="zoom:50%;" /&gt;&lt;/p&gt;
&lt;p&gt;In the main view, you can interactively browse commit history and jump into inspecting selected commit.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/zsh/tig_main.jpg" alt="tig status view" style="zoom:50%;" /&gt;&lt;/p&gt;
&lt;p&gt;Next to git-gui it is my favorite tool for staging git changes.&lt;/p&gt;
&lt;h1 id="frameworks"&gt;Frameworks&lt;/h1&gt;
&lt;p&gt;Here is a quick glance at the most popular frameworks that manage configuration and plugins for Zsh. &lt;a href="(https://github.com/ohmyzsh/ohmyzsh)"&gt;Oh-my-zsh&lt;/a&gt; regins here having ~10x stars compared to the closest contender &lt;a href="https://github.com/sorin-ionescu/prezto"&gt;prezto&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ohmyzsh/ohmyzsh"&gt;ohmyzsh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A delightful community-driven (with 1900+ contributors) framework for managing your Zsh configuration. Includes 300+ optional plugins (rails, git, macOS, hub, docker, homebrew, node, php, python, etc), 140+ themes to spice up your morning, and an auto-update tool that makes it easy to keep up with the latest updates from the community.&lt;/td&gt;
&lt;td&gt;136.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sorin-ionescu/prezto"&gt;prezto&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The configuration framework for Zsh&lt;/td&gt;
&lt;td&gt;12.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/antigen"&gt;antigen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The plugin manager for Zsh.&lt;/td&gt;
&lt;td&gt;6.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zimfw/zimfw"&gt;zimfw&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zim: Modular, customizable, and blazing fast Zsh framework&lt;/td&gt;
&lt;td&gt;2.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/dotphiles/dotzsh"&gt;dotzsh&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A community-driven framework for Zsh&lt;/td&gt;
&lt;td&gt;193&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/psyrendust/alf"&gt;alf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Alf is an out-of-this-world super fast and configurable framework for Zsh; it's modeled after Prezto and Antigen while utilizing Oh My Zsh under the covers; and offers standard defaults, aliases, functions, auto-completion, automated updates, and installable prompt themes and plugins.&lt;/td&gt;
&lt;td&gt;88&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="prompts"&gt;Prompts&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Popular theme managers such as powerlevel9k and powerlevel10k also provides highly configurable prompts but are not listed here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/starship/starship"&gt;starship&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The minimal, blazing-fast, and infinitely customizable prompt for any shell!&lt;/td&gt;
&lt;td&gt;20.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/denysdovhan/spaceship-prompt"&gt;spaceship-prompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A Zsh prompt for Astronauts&lt;/td&gt;
&lt;td&gt;15.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sindresorhus/pure"&gt;pure&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pretty, minimal, and fast ZSH prompt&lt;/td&gt;
&lt;td&gt;10.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/magicmonty/bash-git-prompt"&gt;bash-git-prompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An informative and fancy bash prompt for Git users&lt;/td&gt;
&lt;td&gt;6.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/b-ryan/powerline-shell"&gt;powerline-shell&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A beautiful and useful prompt for your shell&lt;/td&gt;
&lt;td&gt;5.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/nojhan/liquidprompt"&gt;liquidprompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A full-featured &amp;amp; carefully designed adaptive prompt for Bash &amp;amp; Zsh&lt;/td&gt;
&lt;td&gt;4.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/arialdomartini/oh-my-git"&gt;oh-my-git&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An opinionated git prompt for bash and zsh&lt;/td&gt;
&lt;td&gt;3.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/jonmosco/kube-ps1"&gt;kube-ps1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes prompt info for bash and zsh&lt;/td&gt;
&lt;td&gt;2.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/justjanne/powerline-go"&gt;powerline-go&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A beautiful and useful low-latency prompt for your shell, written in go&lt;/td&gt;
&lt;td&gt;2.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/olivierverdier/zsh-git-prompt"&gt;zsh-git-prompt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Informative git prompt for Zsh&lt;/td&gt;
&lt;td&gt;1.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/romkatv/gitstatus"&gt;gitstatus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Git status for Bash and Zsh prompt&lt;/td&gt;
&lt;td&gt;1.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/geometry-zsh/geometry"&gt;geometry&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;geometry is a minimal, fully customizable, and composable zsh prompt theme&lt;/td&gt;
&lt;td&gt;0.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/reobin/typewritten"&gt;typewritten&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A minimal, lightweight, informative zsh prompt theme&lt;/td&gt;
&lt;td&gt;0.6k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="python-tools"&gt;Python tools&lt;/h1&gt;
&lt;p&gt;Since Python dominates in the code I write, I have added a section for two projects that helps to manage virtual environments.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/berdario/pew"&gt;pew&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A tool to manage multiple virtual environments written in pure python&lt;/td&gt;
&lt;td&gt;1.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/MichaelAquilina/zsh-autoswitch-virtualenv"&gt;zsh-autoswitch-virtualenv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ZSH plugin to automatically switch python virtualenvs (including pipenv and poetry) as you move between directories&lt;/td&gt;
&lt;td&gt;0.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id="final-words"&gt;Final words&lt;/h1&gt;
&lt;p&gt;It is interesting how the number of starts growing - the IT world includes more and more people every day. Lots of these people wish to optimize their work environment. Those who work often in console takes look at the helper tools listed on pages as &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt; or &lt;a href="https://github.com/k4m4/terminals-are-sexy"&gt;Terminals Are Sexy&lt;/a&gt;. This article aimed to highlight projects gaining a lot of attention and brings added value to the awesome lists that are ordered alphabetically without information on the number of stars.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Linux"/><category term="zsh"/><category term="scrapping"/><category term="python"/><category term="Linux"/></entry><entry><title>Unleashing the Power of T-Sne for Dimensionality Reduction in Python</title><link href="http://127.0.0.1:8000/tsne-tutorial/" rel="alternate"/><published>2021-03-15T00:00:00+01:00</published><updated>2021-03-15T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-03-15:/tsne-tutorial/</id><summary type="html">&lt;p&gt;Want to create beautiful visualizations from complex data? Discover the power of T-SNE for dimensionality reduction in Python.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dataset"&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visualizing-the-data"&gt;Visualizing the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#applying-t-sne"&gt;Applying T-SNE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visualizing-the-t-sne-results"&gt;Visualizing the T-SNE results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interactive-visualizations"&gt;Interactive visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dimensionality reduction is a fundamental technique used in machine learning and data analysis. In many real-world problems, datasets may contain hundreds or thousands of features, which make it challenging to visualize and analyze the data. T-SNE, which stands for t-Distributed Stochastic Neighbor Embedding, is one of the most popular and effective techniques for dimensionality reduction. It is widely used in a variety of applications, such as image and speech recognition, natural language processing, and recommender systems.&lt;/p&gt;
&lt;p&gt;In this tutorial, we will explore T-SNE and its application to a dataset with 10 concrete, named, correlated features. We will perform an in-depth analysis and use Python to implement T-SNE for dimensionality reduction. We will also visualize the results using interactive visualizations.&lt;/p&gt;
&lt;p&gt;&lt;a id="dataset"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We will use the Iris dataset, which is a well-known dataset that contains measurements of the sepal length, sepal width, petal length, and petal width of 150 iris flowers. Each flower is labeled as one of three species: Setosa, Versicolor, or Virginica. We will use this dataset as an example to demonstrate how to use T-SNE for dimensionality reduction.&lt;/p&gt;
&lt;p&gt;&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;Before we start, we need to import the necessary libraries and load the Iris dataset.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sns&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;

&lt;span class="c1"&gt;# Load the Iris dataset&lt;/span&gt;
&lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="n"&gt;feature_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;
&lt;span class="n"&gt;target_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now inspect the dataset and get a better understanding of the features and labels.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Print the feature names and the first 5 rows of the dataset&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Feature names: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;# Print the target names and the first 5 labels&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Target names: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Feature names:  [&amp;#39;sepal length (cm)&amp;#39;, &amp;#39;sepal width (cm)&amp;#39;, &amp;#39;petal length (cm)&amp;#39;, &amp;#39;petal width (cm)&amp;#39;]
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
0                5.1               3.5                1.4               0.2
1                4.9               3.0                1.4               0.2
2                4.7               3.2                1.3               0.2
3                4.6               3.1                1.5               0.2
4                5.0               3.6                1.4               0.2
Target names:  [&amp;#39;setosa&amp;#39; &amp;#39;versicolor&amp;#39; &amp;#39;virginica&amp;#39;]
[0 0 0 0 0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can see that the dataset has four features, and the labels are represented by integers from 0 to 2, corresponding to the three species.&lt;/p&gt;
&lt;p&gt;&lt;a id="visualizing-the-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="visualizing-the-data"&gt;Visualizing the data&lt;/h2&gt;
&lt;p&gt;Before we apply T-SNE for dimensionality reduction, we can visualize the data to get a better understanding of the relationships between the features and the labels. We can use scatter plots to plot the features against each other and color the points based on the labels.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Create a scatter plot matrix&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pairplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="pairplot" src="/images/tsne_tutorial/pair_plot_pre_tsne.png"&gt;&lt;/p&gt;
&lt;p&gt;We can see that some of the features are highly correlated, such as the petal length and petal width, while others, such as the sepal length and sepal width, show less correlation. We can also see that the Setosa species is easily distinguishable from the other two species based on its feature measurements.&lt;/p&gt;
&lt;p&gt;&lt;a id="applying-t-sne"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="applying-t-sne"&gt;Applying T-SNE&lt;/h2&gt;
&lt;p&gt;Now that we have visualized the data, we can apply T-SNE to reduce the dimensionality of the dataset. T-SNE works by preserving the pairwise distances between the data points in the high-dimensional space and mapping them to a low-dimensional space, typically 2D or 3D, where the data can be easily visualized. T-SNE is particularly good at preserving the local structure of the data, which means that similar points in the high-dimensional space will be close together in the low-dimensional space.&lt;/p&gt;
&lt;p&gt;To apply T-SNE, we can use the &lt;code&gt;TSNE&lt;/code&gt; class from the &lt;code&gt;sklearn.manifold&lt;/code&gt; module in scikit-learn.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;

&lt;span class="c1"&gt;# Apply T-SNE with 2 components to reduce the dimensionality of the dataset&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, we are reducing the dimensionality of the dataset to 2 components, which will allow us to visualize the data in a 2D space. We also set the &lt;code&gt;random_state&lt;/code&gt; parameter to ensure reproducibility of the results.&lt;/p&gt;
&lt;p&gt;&lt;a id="visualizing-the-t-sne-results"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="visualizing-the-t-sne-results"&gt;Visualizing the T-SNE results&lt;/h2&gt;
&lt;p&gt;We can now visualize the T-SNE results using a scatter plot, where the points are colored based on their species labels. We can use the &lt;code&gt;plt.scatter&lt;/code&gt; function to create the scatter plot and the &lt;code&gt;plt.colorbar&lt;/code&gt; function to add a colorbar to show the mapping between colors and species labels.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Create a scatter plot of the T-SNE results&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_tsne&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X_tsne&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;viridis&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Species&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="scatter plot post T-SNE" src="/images/tsne_tutorial/scatter_plot_post_tsne.png"&gt;&lt;/p&gt;
&lt;p&gt;We can see that the T-SNE results separate the three species quite well, with minimal overlap between the points. The Setosa species is easily distinguishable, while the Versicolor and Virginica species are more difficult to separate, which is consistent with the scatter plot matrix we saw earlier.&lt;/p&gt;
&lt;p&gt;&lt;a id="interactive-visualizations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="interactive-visualizations"&gt;Interactive visualizations&lt;/h2&gt;
&lt;p&gt;To create interactive visualizations of the T-SNE results, we can use the &lt;code&gt;bokeh&lt;/code&gt; library in Python. &lt;code&gt;bokeh&lt;/code&gt; is a powerful library for creating interactive visualizations, and it provides a range of tools and features for customizing and manipulating plots.&lt;/p&gt;
&lt;p&gt;First, we need to install &lt;code&gt;bokeh&lt;/code&gt; and import the necessary modules.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;!&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;bokeh&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.io&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;output_notebook&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;show&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.plotting&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ColumnDataSource&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HoverTool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CategoricalColorMapper&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.palettes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Category10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we can create a scatter plot using &lt;code&gt;bokeh&lt;/code&gt;. We start by creating a &lt;code&gt;ColumnDataSource&lt;/code&gt; object, which contains the data we want to plot, and the mappings between the data and the plot elements. We can use the &lt;code&gt;HoverTool&lt;/code&gt; tool to display additional information about the points when the mouse cursor is over them. We can also use the &lt;code&gt;CategoricalColorMapper&lt;/code&gt; mapper to map the species labels to colors.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Create a scatter plot of the T-SNE results using bokeh&lt;/span&gt;
&lt;span class="n"&gt;output_notebook&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ColumnDataSource&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_tsne&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X_tsne&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;color&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="c1"&gt;# Create a color mapper to map species labels to colors&lt;/span&gt;

&lt;span class="n"&gt;color_mapper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CategoricalColorMapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;factors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;palette&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Category10&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Create the scatter plot&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Iris dataset - T-SNE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;HoverTool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tooltips&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Species&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;@label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)])],&lt;/span&gt; &lt;span class="n"&gt;x_axis_label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;T-SNE component 1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_axis_label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;T-SNE component 2&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;field&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;color&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;transform&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;color_mapper&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;legend_field&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;label&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Output:
[T-SNE scatter plot with Bokeh]&lt;/p&gt;
&lt;p&gt;We can see that the Bokeh scatter plot provides several additional features that were not available in the static scatter plot. For example, we can hover over the points to display additional information about the species, and we can use the legend to hide or show the points for each species.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this tutorial, we have learned how to use T-SNE for dimensionality reduction on a dataset with 10 concrete, named, and correlated features. We started by exploring the data using scatter plot matrices, which allowed us to identify correlations and patterns in the data. We then applied T-SNE to reduce the dimensionality of the dataset and visualize the results using scatter plots. Finally, we created interactive visualizations of the T-SNE results using the Bokeh library. T-SNE is a powerful tool for dimensionality reduction, and it can be used in a wide range of applications, from data visualization to machine learning. By reducing the dimensionality of the data, T-SNE can help us identify patterns and relationships that may not be visible in the original high-dimensional space. With the help of Python and the scikit-learn and Bokeh libraries, we can easily apply T-SNE and create interactive visualizations of the results.&lt;/p&gt;
&lt;p&gt;See also:
&lt;a href="https://distill.pub/2016/misread-tsne/"&gt;How to Use t-SNE Effectively&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="data-visualization"/><category term="machine-learning"/><category term="python"/><category term="t-sne"/><category term="dimensionality-reduction"/><category term="scatter-plot"/><category term="bokeh"/><category term="seaborn"/><category term="numpy"/><category term="pandas"/></entry><entry><title>Kurtosis in Simple Terms, Interpretation and Gotchas</title><link href="http://127.0.0.1:8000/kurtosis-in-simple-terms/" rel="alternate"/><published>2021-02-18T00:00:00+01:00</published><updated>2021-02-18T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-02-18:/kurtosis-in-simple-terms/</id><summary type="html">&lt;p&gt;Statistics can be tricky, but understanding kurtosis is a must for anyone who wants to avoid making common mistakes in statistical analyses. Learn how to interpret it in this comprehensive guide.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interpreting-kurtosis"&gt;Interpreting Kurtosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-normal-distribution"&gt;1.  Normal distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-flat-distributions"&gt;2.  Flat distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-peaked-distributions"&gt;3.  Peaked distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-interpretation-in-context"&gt;4.  Interpretation in context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5-use-caution-in-small-samples"&gt;5.  Use caution in small samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6-compare-with-skewness"&gt;6.  Compare with skewness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#when-kurtosis-can-be-misleading"&gt;When kurtosis can be misleading?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-outliers"&gt;1.  Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-sample-size"&gt;2.  Sample size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-non-normal-distributions"&gt;3. Non-normal distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-different-distributions-can-have-similar-kurtosis"&gt;4.  Different distributions can have similar kurtosis:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#further-reading"&gt;Further reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Kurtosis is a statistical term that measures the shape of a distribution, specifically the degree of peakness or flatness in the tails of a distribution. A distribution with high kurtosis has a sharp peak and heavy tails, while a distribution with low kurtosis has a flatter peak and lighter tails.&lt;/p&gt;
&lt;p&gt;The kurtosis of a distribution is defined as the fourth standardized moment, which can be expressed mathematically as:
$$
\begin{equation} \text{kurtosis} = \frac{\mu_4}{\sigma^4} - 3 \end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\mu_4$ is the fourth central moment (a measure of the spread of the data), and $\sigma$ is the standard deviation of the data. The term "standardized" means that we divide the moment by the appropriate power of the standard deviation to make the kurtosis a dimensionless quantity. The subtraction of 3 is a convention that makes the kurtosis of the normal distribution equal to 0.&lt;/p&gt;
&lt;p&gt;Kurtosis can be positive or negative, depending on the shape of the distribution. A positive kurtosis means that the tails of the distribution are heavier than the tails of the normal distribution, while a negative kurtosis means that the tails are lighter than the normal distribution. A kurtosis of 0 means that the distribution is perfectly normal.&lt;/p&gt;
&lt;p&gt;Kurtosis is important in statistics because it can affect the interpretation of statistical analyses. For example, the presence of outliers or extreme values can cause a distribution to have high kurtosis, which can affect the results of some statistical tests. In addition, some statistical models assume that the distribution of the data is normal, so it is important to check the kurtosis of the data to see if this assumption is valid.&lt;/p&gt;
&lt;p&gt;To better understand kurtosis, let's take a look at some examples. We'll start by generating some data from different distributions and calculating their kurtosis.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;kurtosis&lt;/span&gt;

&lt;span class="c1"&gt;# Generate data from a normal distribution&lt;/span&gt;
&lt;span class="n"&gt;normal_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;normal_kurtosis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kurtosis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normal_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate data from a uniform distribution&lt;/span&gt;
&lt;span class="n"&gt;uniform_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;low&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;uniform_kurtosis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kurtosis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;uniform_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Generate data from a Laplace distribution&lt;/span&gt;
&lt;span class="n"&gt;laplace_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;laplace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;laplace_kurtosis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kurtosis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;laplace_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We generated data from three different distributions: a normal distribution, a uniform distribution, and a Laplace distribution. We then calculated the kurtosis of each distribution using the &lt;code&gt;kurtosis&lt;/code&gt; function from the &lt;code&gt;scipy.stats&lt;/code&gt; module.&lt;/p&gt;
&lt;p&gt;Let's plot the histograms of the three distributions, along with their kurtosis values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bmh&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;axs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normal_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Normal kurtosis = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;normal_kurtosis&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;uniform_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Uniform kurtosis = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;uniform_kurtosis&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;laplace_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Laplace kurtosis = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;laplace_kurtosis&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will generate a plot with three histograms, one for each of the three distributions, along with their kurtosis values.&lt;/p&gt;
&lt;p&gt;&lt;img alt="kurtosis for normal uniform and laplace distributions" src="/images/kurtosis/kurtosis_normal_uniform_laplace.jpg"&gt;
&lt;em&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Kurtosis for: normal uniform and laplace distributions&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the normal distribution has a kurtosis close to 0, indicating that it is roughly normal. The uniform distribution has a negative kurtosis, indicating that it is flatter than the normal distribution. The Laplace distribution has a positive kurtosis, indicating that it has heavier tails than the normal distribution.&lt;/p&gt;
&lt;p&gt;It's also worth noting that kurtosis is not the same as skewness, which measures the degree of asymmetry in a distribution. A distribution can have high kurtosis and be symmetric, or have low kurtosis and be skewed.&lt;/p&gt;
&lt;p&gt;In summary:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kurtosis&lt;/strong&gt; is a statistical term that measures the shape of a distribution, specifically the degree of peakness or flatness in the tails of a distribution.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A distribution with high kurtosis has a sharp peak and heavy tails, while a distribution with low kurtosis has a flatter peak and lighter tails. Kurtosis can be positive or negative, depending on the shape of the distribution, and it is important in statistics because it can affect the interpretation of statistical analyses. It's also important to note that kurtosis is not the same as skewness, which measures the degree of asymmetry in a distribution.&lt;/p&gt;
&lt;p&gt;&lt;a id="interpreting-kurtosis"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="interpreting-kurtosis"&gt;Interpreting Kurtosis&lt;/h2&gt;
&lt;p&gt;Interpreting kurtosis can be somewhat subjective, as the appropriate level of kurtosis can vary depending on the context and the goals of the analysis. However, there are some general guidelines that can be used to help interpret kurtosis in practical terms.&lt;/p&gt;
&lt;p&gt;&lt;a id="1-normal-distribution"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-normal-distribution"&gt;1.  Normal distribution&lt;/h3&gt;
&lt;p&gt;A normal distribution &lt;strong&gt;has a kurtosis of 0&lt;/strong&gt;. A distribution that deviates significantly from this value may be considered non-normal, although the degree of deviation that is considered significant will depend on the context.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-flat-distributions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-flat-distributions"&gt;2.  Flat distributions&lt;/h3&gt;
&lt;p&gt;Distributions with &lt;strong&gt;low kurtosis values (less than 0) are relatively flat&lt;/strong&gt;, meaning they have fewer extreme values than a normal distribution. These distributions are sometimes called "platykurtic". Examples of such distributions include the uniform distribution and the exponential distribution. In some cases, a platykurtic distribution may be desirable, as it can reduce the impact of outliers on statistical analyses.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-peaked-distributions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-peaked-distributions"&gt;3.  Peaked distributions&lt;/h3&gt;
&lt;p&gt;Distributions with high kurtosis values (greater than 0) are relatively peaked, meaning they have more extreme values than a normal distribution. These distributions are sometimes called "leptokurtic". Examples of such distributions include the Laplace distribution and the Student's t-distribution. A leptokurtic distribution may be desirable in certain contexts, as it can increase the sensitivity of statistical analyses.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-interpretation-in-context"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-interpretation-in-context"&gt;4.  Interpretation in context&lt;/h3&gt;
&lt;p&gt;The interpretation of kurtosis should always be considered in the context of the specific analysis being performed. For example, in finance, a leptokurtic distribution may be desirable for modeling stock returns, as it can capture the possibility of extreme returns. On the other hand, in clinical trials, a platykurtic distribution may be desirable for modeling adverse events, as it can reduce the impact of rare and extreme events.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-use-caution-in-small-samples"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-use-caution-in-small-samples"&gt;5.  Use caution in small samples&lt;/h3&gt;
&lt;p&gt;Interpretation of kurtosis can be more difficult in small samples, as the kurtosis value may be influenced by a few extreme values. In such cases, it may be useful to examine the distribution visually or to perform a sensitivity analysis to assess the impact of extreme values on the results.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-compare-with-skewness"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-compare-with-skewness"&gt;6.  Compare with skewness&lt;/h3&gt;
&lt;p&gt;As mentioned earlier, kurtosis is not the same as skewness, which measures the degree of asymmetry in a distribution. In some cases, it may be useful to examine both kurtosis and skewness together to fully understand the shape of the distribution.&lt;/p&gt;
&lt;p&gt;&lt;a id="when-kurtosis-can-be-misleading"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="when-kurtosis-can-be-misleading"&gt;When kurtosis can be misleading?&lt;/h2&gt;
&lt;p&gt;Kurtosis is a statistical measure that describes the shape of a distribution. It provides information about the heaviness of the tails of the distribution and the presence of outliers. However, there are certain situations in which kurtosis can be misleading, and it is important to be aware of these.&lt;/p&gt;
&lt;p&gt;&lt;a id="1-outliers"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-outliers"&gt;1.  Outliers&lt;/h3&gt;
&lt;p&gt;Kurtosis is sensitive to outliers. Outliers can distort the kurtosis value, making it difficult to interpret the shape of the distribution accurately. Therefore, it is important to inspect the data for outliers before calculating kurtosis.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-sample-size"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-sample-size"&gt;2.  Sample size&lt;/h3&gt;
&lt;p&gt;The sample size can also affect the kurtosis value. With a small sample size, kurtosis may not accurately reflect the shape of the population distribution.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-non-normal-distributions"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-non-normal-distributions"&gt;3. Non-normal distributions&lt;/h3&gt;
&lt;p&gt;Kurtosis is only a useful measure for symmetric distributions. In the case of asymmetric distributions, kurtosis may not provide any useful information. For example, a distribution with a heavy tail on one side may have a kurtosis value that suggests a peaked distribution, which is misleading.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-different-distributions-can-have-similar-kurtosis"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-different-distributions-can-have-similar-kurtosis"&gt;4.  Different distributions can have similar kurtosis&lt;/h3&gt;
&lt;p&gt;Two different distributions can have the same kurtosis value. Therefore, kurtosis alone may not provide enough information to compare the shape of different distributions.&lt;/p&gt;
&lt;p&gt;&lt;a id="further-reading"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="further-reading"&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Kurtosis"&gt;Kurtosis on Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.jstor.org/stable/23349427"&gt;Kurtosis: A Critical Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51"&gt;Understanding Boxplots&lt;/a&gt; (includes a section on kurtosis)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/visualizing-the-central-limit-theorem-e30a69bde634"&gt;Visualizing the Central Limit Theorem&lt;/a&gt; (includes a section on kurtosis)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aakinshin.net/posts/misleading-kurtosis/"&gt;Misleading kurtosis | Andrey Akinshin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aakinshin.net/posts/lowland-multimodality-detection/"&gt;Lowland multimodality detection | Andrey Akinshin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Header image by Joxemai4 - Own work, CC BY-SA 3.0 &lt;a href="https://commons.wikimedia.org/w/index.php?curid=9523977"&gt;wikimedia&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="statistics"/><category term="python"/></entry><entry><title>Finding Errors in Data - Data Validation</title><link href="http://127.0.0.1:8000/finding-errors-in-data/" rel="alternate"/><published>2021-01-31T00:00:00+01:00</published><updated>2023-01-31T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-01-31:/finding-errors-in-data/</id><summary type="html">&lt;p&gt;Explore methods to detect &amp;amp; fix errors in data, including validation, visualizations, statistical tests, cleaning techniques, machine learning &amp;amp; data quality tools. Get concise, easy to understand information with examples &amp;amp; links to external resources.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-validation-a-key-method-for-finding-errors-in-data"&gt;Data Validation: A Key Method for Finding Errors in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-is-data-validation"&gt;What is Data Validation?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#why-is-data-validation-important"&gt;Why is Data Validation Important?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#types-of-data-validation"&gt;Types of Data Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tools-for-data-validation"&gt;Tools for Data Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-visualizations-to-identify-outliers-and-anomalies-in-data"&gt;Using Visualizations to Identify Outliers and Anomalies in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-are-outliers-and-anomalies-in-data"&gt;What are Outliers and Anomalies in Data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visualizing-outliers-and-anomalies-in-data"&gt;Visualizing Outliers and Anomalies in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-visualizations-to-identify-errors-in-data"&gt;Using Visualizations to Identify Errors in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tools-for-data-visualization"&gt;Tools for Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion-1"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-statistical-methods-to-find-errors-in-data"&gt;Using Statistical Methods to Find Errors in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mean-and-standard-deviation"&gt;Mean and Standard Deviation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#z-scores"&gt;Z-scores&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hypothesis-testing"&gt;Hypothesis Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-data-cleaning-techniques-to-remove-or-correct-errors"&gt;Using Data Cleaning Techniques to Remove or Correct Errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#imputation"&gt;Imputation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#normalization"&gt;Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#encoding"&gt;Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-machine-learning-algorithms-to-identify-errors-in-data"&gt;Using Machine Learning Algorithms to Identify Errors in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#clustering"&gt;Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#anomaly-detection"&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data plays a critical role in the field of data science, and the accuracy and quality of data directly impacts the success of data-driven projects. Therefore, it's essential to have methods in place to identify and address errors in data. In this article, we will discuss various methods and techniques used in the data science community to find errors in data, including data validation, visualization, statistical methods, data cleaning, machine learning, and data quality tools. We will also provide examples and references to external resources for further study. Whether you are a seasoned data scientist or a beginner, this article will provide valuable insights into the methods for finding errors in data and help you ensure the quality of your data.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Validation:&lt;/strong&gt; Using data validation rules to check for errors such as incorrect data types, range violations, or missing values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: Using visualizations such as histograms, scatter plots, and box plots to identify outliers and anomalies in the data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistical Methods:&lt;/strong&gt; Using statistical methods such as mean, standard deviation, Z-scores, and hypothesis testing to identify errors in data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Cleaning:&lt;/strong&gt; Using data cleaning techniques such as imputation, normalization, and encoding to remove or correct errors in the data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning:&lt;/strong&gt; Using machine learning algorithms such as clustering, anomaly detection, and classification to automatically identify errors in the data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Quality Tools:&lt;/strong&gt; Using data quality tools such as Talend, Trifacta, and Informatica to automate the process of finding and fixing errors in data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: no single method is enough to find all errors in data, and a combination of these methods should be used to ensure the highest quality data.&lt;/p&gt;
&lt;p&gt;&lt;a id="data-validation-a-key-method-for-finding-errors-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="data-validation-a-key-method-for-finding-errors-in-data"&gt;Data Validation: A Key Method for Finding Errors in Data&lt;/h2&gt;
&lt;p&gt;Data validation is a crucial step in the process of data analysis and is used to ensure that the data is accurate, consistent, and meets the specified requirements. In this blog post, we'll discuss the importance of data validation and how it can be used to find errors in data.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-is-data-validation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="what-is-data-validation"&gt;What is Data Validation?&lt;/h3&gt;
&lt;p&gt;Data validation refers to the process of checking data for accuracy and completeness, and ensuring that it meets specific requirements and constraints. The goal of data validation is to identify errors and inconsistencies in the data, such as incorrect data types, range violations, and missing values, and to ensure that the data meets the specifications required for further analysis and decision making.&lt;/p&gt;
&lt;p&gt;&lt;a id="why-is-data-validation-important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="why-is-data-validation-important"&gt;Why is Data Validation Important?&lt;/h3&gt;
&lt;p&gt;Data validation is an important step in the data analysis process because it helps to ensure the quality of the data. Errors and inconsistencies in data can have significant impacts on the accuracy and reliability of data-driven decisions and can lead to incorrect conclusions and ineffective solutions. In many cases, errors in data may not be immediately apparent, and without proper validation, they can go unnoticed and lead to significant problems later on.&lt;/p&gt;
&lt;p&gt;&lt;a id="types-of-data-validation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="types-of-data-validation"&gt;Types of Data Validation&lt;/h3&gt;
&lt;p&gt;There are several types of data validation techniques that can be used to find errors in data, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data type validation:&lt;/strong&gt; Ensuring that data is stored in the correct format, such as text, numeric, or date.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Range validation:&lt;/strong&gt; Checking data values to ensure they fall within specified ranges or limits.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency validation:&lt;/strong&gt; Verifying that data is consistent across different sources or systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Required field validation:&lt;/strong&gt; Checking that required fields are present and not empty.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Format validation:&lt;/strong&gt; Checking that data meets specified formats, such as email addresses or phone numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="tools-for-data-validation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="tools-for-data-validation"&gt;Tools for Data Validation&lt;/h3&gt;
&lt;p&gt;There are several tools and techniques available for data validation, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spreadsheet software&lt;/strong&gt;: Many spreadsheet programs, such as Microsoft Excel and Google Sheets, include data validation features that can be used to validate data and ensure that it meets specific requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Database management systems&lt;/strong&gt;: Many database management systems, such as Microsoft Access and Oracle, include data validation capabilities that can be used to validate data as it is entered into the database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Programming languages&lt;/strong&gt;: Many programming languages, such as Python and R, include data validation libraries and functions that can be used to validate data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data quality tools&lt;/strong&gt;: There are several data quality tools available, such as Talend, Trifacta, and Informatica, that can be used to automate the data validation process and ensure that data meets specific requirements.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Data validation is a critical step in the data analysis process, and it is essential to ensure the quality and accuracy of data. By using data validation techniques, you can identify and address errors and inconsistencies in the data, and ensure that the data meets the specific requirements and constraints required for further analysis and decision making. Whether you are using spreadsheet software, database management systems, programming languages, or data quality tools, data validation is an important tool for finding errors in data.&lt;/p&gt;
&lt;p&gt;&lt;a id="using-visualizations-to-identify-outliers-and-anomalies-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="using-visualizations-to-identify-outliers-and-anomalies-in-data"&gt;Using Visualizations to Identify Outliers and Anomalies in Data&lt;/h2&gt;
&lt;p&gt;Data visualization is an important tool for data analysis, and it can be used to identify outliers and anomalies in the data. This blog post will focus on the use of visualizations, such as histograms, scatter plots, and box plots, to identify errors in data.&lt;/p&gt;
&lt;p&gt;&lt;a id="what-are-outliers-and-anomalies-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="what-are-outliers-and-anomalies-in-data"&gt;What are Outliers and Anomalies in Data?&lt;/h3&gt;
&lt;p&gt;Outliers and anomalies in data are values that are significantly different from other values in the data set. Outliers are values that fall outside of the range of the majority of data points, while anomalies are values that are not only different from the majority of data points, but also deviate from the expected pattern of the data. Outliers and anomalies can arise due to measurement errors, errors in data entry, or other factors.&lt;/p&gt;
&lt;p&gt;&lt;a id="visualizing-outliers-and-anomalies-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="visualizing-outliers-and-anomalies-in-data"&gt;Visualizing Outliers and Anomalies in Data&lt;/h3&gt;
&lt;p&gt;There are several visualizations that can be used to identify outliers and anomalies in data, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Histograms: Histograms display the distribution of data by showing the frequency of data values in different intervals. Outliers can be identified by observing the data values that fall outside of the range of the majority of data points.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scatter Plots: Scatter plots display the relationship between two variables by plotting the values of each variable on a two-dimensional graph. Outliers can be identified by observing the data points that fall outside of the general pattern of the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Box Plots: Box plots display the distribution of data by showing the median, quartiles, and range of the data. Outliers can be identified by observing the data points that fall outside of the range represented by the box plot.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="using-visualizations-to-identify-errors-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="using-visualizations-to-identify-errors-in-data"&gt;Using Visualizations to Identify Errors in Data&lt;/h3&gt;
&lt;p&gt;Visualizations can be a powerful tool for identifying errors in data. By visualizing the data, you can quickly identify outliers and anomalies that might otherwise be missed in a tabular or numerical representation of the data. This can help you to identify data errors such as incorrect data types, range violations, or missing values, and to make informed decisions about how to address these errors.&lt;/p&gt;
&lt;p&gt;&lt;a id="tools-for-data-visualization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="tools-for-data-visualization"&gt;Tools for Data Visualization&lt;/h3&gt;
&lt;p&gt;There are several tools available for data visualization, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spreadsheet software&lt;/strong&gt;: Many spreadsheet programs, such as Microsoft Excel and Google Sheets, include data visualization features, such as histograms, scatter plots, and box plots, that can be used to visualize data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data visualization software&lt;/strong&gt;: There are several data visualization software programs available, such as &lt;a href="https://www.tableau.com/"&gt;Tableau&lt;/a&gt;, &lt;a href="https://www.qlik.com/"&gt;QlikView&lt;/a&gt;, and &lt;a href="https://d3js.org/"&gt;D3.js&lt;/a&gt;, that can be used to create advanced data visualizations and identify outliers and anomalies in the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Programming languages&lt;/strong&gt;: Many programming languages, such as Python and R, include data visualization libraries, such as &lt;a href="https://matplotlib.org/"&gt;Matplotlib&lt;/a&gt; and &lt;a href="https://ggplot2.tidyverse.org/"&gt;ggplot2&lt;/a&gt;, that can be used to create custom data visualizations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="conclusion-1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="conclusion_1"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Data visualization is a valuable tool for finding errors in data, and it can be used to identify outliers and anomalies in the data. Whether you are using spreadsheet software, data visualization software, or programming languages, visualizations can help you to quickly identify errors in the data and make informed decisions about how to address these errors. By using visualizations, such as histograms, scatter plots, and box plots, you can ensure the quality and accuracy of your data, and make better data-driven decisions.&lt;/p&gt;
&lt;p&gt;Visualizing data is an effective way to identify errors in data, and it is a valuable tool for data analysis and data quality assurance. With the right tools and techniques, you can create visualizations that reveal outliers and anomalies in the data, helping you to make better data-driven decisions and ensure the accuracy of your data.&lt;/p&gt;
&lt;p&gt;&lt;a id="using-statistical-methods-to-find-errors-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="using-statistical-methods-to-find-errors-in-data"&gt;Using Statistical Methods to Find Errors in Data&lt;/h2&gt;
&lt;p&gt;Data errors can have serious consequences for decision-making, so it's crucial to detect them as soon as possible. One of the ways to identify errors in data is by using statistical methods. In this article, we will discuss the following statistical methods for finding errors in data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean and Standard Deviation&lt;/li&gt;
&lt;li&gt;Z-scores&lt;/li&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="mean-and-standard-deviation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="mean-and-standard-deviation"&gt;Mean and Standard Deviation&lt;/h3&gt;
&lt;p&gt;The mean and standard deviation are commonly used statistical measures that provide information about the central tendency and spread of a set of data. The mean is the average of all the data points, and the standard deviation is a measure of how much the data points in a set deviate from the mean.&lt;/p&gt;
&lt;p&gt;When working with a large dataset, it's often helpful to calculate the mean and standard deviation of your data. This can give you a quick idea of how your data is distributed, and whether there are any outliers that might be affecting the overall pattern of your data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;
If you have a dataset of 100 customer satisfaction ratings, and the mean is 8.5 and the standard deviation is 2.0, this tells you that the majority of your customers are happy with your service, but that there are some outliers (customers who gave a low rating) that may require further investigation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To calculate the mean and standard deviation in Python, you can use the &lt;code&gt;numpy&lt;/code&gt; library:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# Example data&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate mean&lt;/span&gt;
&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate standard deviation&lt;/span&gt;
&lt;span class="n"&gt;std_dev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="z-scores"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="z-scores"&gt;Z-scores&lt;/h3&gt;
&lt;p&gt;Z-scores, also known as standard scores, provide information about how far a data point is from the mean in terms of standard deviations. This can help you identify outliers in your data that are far from the mean.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;
If a customer's satisfaction rating is 4 and the mean is 8.5 with a standard deviation of 2.0, the Z-score for this data point is &lt;code&gt;-2.25&lt;/code&gt; (i.e., the data point is 2.25 standard deviations away from the mean). This tells you that this particular customer is not satisfied with your service and requires further investigation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To calculate Z-scores in Python, you can use the following formula:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;z_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_point&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;std_dev&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="hypothesis-testing"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="hypothesis-testing"&gt;Hypothesis Testing&lt;/h3&gt;
&lt;p&gt;Hypothesis testing is a statistical method used to make inferences about a population based on a sample of data. It involves formulating a null hypothesis and an alternative hypothesis, and using statistical techniques to determine the probability of obtaining the observed results if the null hypothesis were true. If this probability is low (below a specified significance level), the null hypothesis is rejected and the alternative hypothesis is accepted.&lt;/p&gt;
&lt;p&gt;In the context of finding errors in data, hypothesis testing can be used to identify whether a sample mean is significantly different from a specified value, or whether two samples are significantly different from each other.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;
For example, consider a sample of data with a mean of 8.5 and a standard deviation of 1.2. We can use a one-sample t-test to determine whether the sample mean is significantly different from a specified value (in this case, 8.5). The null hypothesis would be that the sample mean is equal to 8.5, and the alternative hypothesis would be that the sample mean is not equal to 8.5.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here's an example of how to perform a one-sample t-test in Python using the &lt;code&gt;scipy&lt;/code&gt; library:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ttest_1samp&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;7.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;t_statistic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ttest_1samp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the &lt;code&gt;ttest_1samp&lt;/code&gt; function is used to perform a one-sample t-test, which tests whether the sample mean is significantly different from a specified value (in this case, 8.5). The &lt;code&gt;t_statistic&lt;/code&gt; and &lt;code&gt;p_value&lt;/code&gt; returned by the function can be used to determine whether the hypothesis is true or false. If the &lt;code&gt;p_value&lt;/code&gt; is less than a specified significance level (such as 0.05), this suggests that the hypothesis is false and that the sample mean is significantly different from 8.5.&lt;/p&gt;
&lt;p&gt;Hypothesis testing is a useful method for identifying errors in data, allowing you to make informed decisions based on accurate information. By formulating and testing hypotheses, you can determine whether a sample mean is significantly different from a specified value or whether two samples are significantly different from each other, helping you to quickly identify outliers and anomalies in your data.&lt;/p&gt;
&lt;p&gt;&lt;a id="using-data-cleaning-techniques-to-remove-or-correct-errors"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="using-data-cleaning-techniques-to-remove-or-correct-errors"&gt;Using Data Cleaning Techniques to Remove or Correct Errors&lt;/h2&gt;
&lt;p&gt;Data cleaning, also known as data cleansing or data scrubbing, is the process of detecting and correcting errors and inconsistencies in data. Data cleaning is an essential step in the data pre-processing stage and is critical to the accuracy and reliability of any analysis performed on the data.&lt;/p&gt;
&lt;p&gt;There are several data cleaning techniques that can be used to remove or correct errors in the data, including imputation, normalization, and encoding.&lt;/p&gt;
&lt;p&gt;&lt;a id="imputation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="imputation"&gt;Imputation&lt;/h3&gt;
&lt;p&gt;Imputation is the process of filling in missing values in a dataset. This is important because missing values can affect the results of any analysis performed on the data. There are several imputation techniques that can be used, including mean imputation, median imputation, and mode imputation.&lt;/p&gt;
&lt;p&gt;In mean imputation, missing values are replaced with the mean of the remaining values in the same column. In median imputation, missing values are replaced with the median of the remaining values in the same column. And in mode imputation, missing values are replaced with the mode of the remaining values in the same column.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="c1"&gt;# Load the data into a pandas DataFrame&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Impute missing values with the mean&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the &lt;code&gt;fillna&lt;/code&gt; method is used to fill missing values in the &lt;code&gt;data&lt;/code&gt; DataFrame with the mean of the values in the same column.&lt;/p&gt;
&lt;p&gt;&lt;a id="normalization"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="normalization"&gt;Normalization&lt;/h3&gt;
&lt;p&gt;Normalization is the process of transforming data into a standard scale, such as transforming data from the range [0, 100] to the range [-1, 1]. This is important because normalizing the data makes it easier to compare values across different features, and can also improve the performance of some machine learning algorithms.&lt;/p&gt;
&lt;p&gt;There are several normalization techniques that can be used, including min-max normalization, z-score normalization, and log normalization.&lt;/p&gt;
&lt;h4 id="min-max-normalization"&gt;Min-Max Normalization&lt;/h4&gt;
&lt;p&gt;Min-Max normalization is a technique that scales data to a specific range, usually between 0 and 1. This is achieved by subtracting the minimum value of the data from each data point and then dividing by the range of the data (the difference between the maximum and minimum values). The resulting values are then scaled to the desired range.&lt;/p&gt;
&lt;p&gt;Here is an example in Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;min_max_normalization&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_val&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_val&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;min_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;max_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;scaled_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;min_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_data&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;min_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;normalized_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaled_data&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_val&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;min_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;min_val&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;normalized_data&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;normalized_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;min_max_normalization&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;normalized_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, the minimum value of the data is 1, the maximum value is 5, and the range is 4. The data is then scaled to the desired range of 0 to 1 by subtracting the minimum value from each data point and dividing by the range.&lt;/p&gt;
&lt;p&gt;Min-Max normalization is a useful technique for scaling data to a specific range, especially when the data is not normally distributed or when the data has extreme values that would otherwise dominate the scale of the data.&lt;/p&gt;
&lt;h4 id="z-score-normalization"&gt;Z-Score Normalization&lt;/h4&gt;
&lt;p&gt;Z-Score normalization, also known as standard score normalization, is a technique that transforms data by scaling it to have a mean of zero and a standard deviation of one. This is achieved by subtracting the mean of the data from each data point and then dividing by the standard deviation of the data. The resulting values are called Z-scores, and they provide a way to compare data points based on their standard deviations from the mean.&lt;/p&gt;
&lt;p&gt;Here is an example in Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;z_score_normalization&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;z_scores&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;z_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z_score_normalization&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z_scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.41421356&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.70710678&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;        &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.70710678&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.41421356&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, the mean of the data is 3, the standard deviation is 1.58113883, and the Z-scores are calculated by subtracting the mean from each data point and dividing by the standard deviation.&lt;/p&gt;
&lt;h4 id="log-normalization"&gt;Log Normalization&lt;/h4&gt;
&lt;p&gt;Log normalization is a technique that transforms data by taking the natural logarithm of each data point. This can be useful when the data is highly skewed or when there are extreme values that would otherwise dominate the scale of the data. By taking the logarithm, the data is compressed and the impact of extreme values is reduced.&lt;/p&gt;
&lt;p&gt;Here is an example in Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;log_normalization&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;log_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;log_data&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;log_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;log_normalization&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;        &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.69314718&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.09861229&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.38629436&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.60943791&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example, the logarithm of each data point is taken, which compresses the data and reduces the impact of extreme values.&lt;/p&gt;
&lt;p&gt;&lt;a id="encoding"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="encoding"&gt;Encoding&lt;/h3&gt;
&lt;p&gt;Encoding is the process of converting categorical data, such as text data, into numerical data that can be processed by machine learning algorithms. There are several encoding techniques that can be used, including one-hot encoding, label encoding, and binary encoding.&lt;/p&gt;
&lt;p&gt;One-hot encoding creates a binary variable for each unique value in the categorical data, with a value of 1 indicating the presence of the value and a value of 0 indicating the absence of the value.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# One-hot encode the categorical data&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Category&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example, the &lt;code&gt;get_dummies&lt;/code&gt; method is used to one-hot encode the categorical data in the &lt;code&gt;data&lt;/code&gt; DataFrame, creating a binary variable for each unique value in the &lt;code&gt;Category&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;Using data cleaning techniques such as imputation, normalization, and encoding is an effective way to remove or correct errors in the data. By using these techniques, you can improve the accuracy and reliability of any analysis performed on the data, making it easier to make informed decisions based on the data.&lt;/p&gt;
&lt;p&gt;&lt;a id="using-machine-learning-algorithms-to-identify-errors-in-data"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="using-machine-learning-algorithms-to-identify-errors-in-data"&gt;Using Machine Learning Algorithms to Identify Errors in Data&lt;/h2&gt;
&lt;p&gt;Machine learning algorithms provide a powerful toolset for identifying errors in data. There are several types of machine learning algorithms that can be used for this purpose, including clustering, anomaly detection, and classification. These algorithms are capable of automatically analyzing large and complex datasets, and can identify patterns and anomalies in the data that may not be easily detected through manual inspection.&lt;/p&gt;
&lt;p&gt;&lt;a id="clustering"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="clustering"&gt;Clustering&lt;/h3&gt;
&lt;p&gt;Clustering is a type of unsupervised machine learning algorithm that groups similar data points together. The idea is to divide the data into clusters based on their similarity, so that data points within a cluster are more similar to each other than to data points in other clusters. Clustering can be used to identify errors in the data by grouping together data points that are not similar to other data points in the same cluster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;
A clustering algorithm could be used to identify customers who have made multiple purchases from a retail store. If a customer has made purchases in several different categories, it is unlikely that their purchases would belong to the same cluster. This may indicate that the customer data is incorrect, and that the purchases have been made by several different customers with the same name.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="anomaly-detection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="anomaly-detection"&gt;Anomaly Detection&lt;/h3&gt;
&lt;p&gt;Anomaly detection is another type of unsupervised machine learning algorithm that is used to identify data points that are significantly different from other data points in the same dataset. Anomaly detection algorithms can be used to identify errors in the data by flagging data points that do not conform to the expected patterns and distributions in the data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;
An anomaly detection algorithm could be used to identify fraud in a financial dataset. The algorithm would analyze the data to identify transactions that are significantly different from other transactions, and flag these transactions as potentially fraudulent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a id="classification"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="classification"&gt;Classification&lt;/h3&gt;
&lt;p&gt;Classification is a type of supervised machine learning algorithm that is used to predict a categorical variable based on one or more predictor variables. Classification algorithms can be used to identify errors in the data by flagging data points that do not belong to the expected category.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;
A classification algorithm could be used to identify incorrect data in a dataset of job applicants. The algorithm could be trained to predict the type of job that each applicant is applying for based on their qualifications and experience. If an applicant's data is incorrect, the algorithm may flag this data as belonging to an unexpected category.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Machine learning algorithms provide a powerful toolset for identifying errors in data. Clustering, anomaly detection, and classification algorithms can be used to automatically analyze large and complex datasets, and can identify patterns and anomalies in the data that may not be easily detected through manual inspection. By using machine learning algorithms to identify errors in the data, organizations can improve the accuracy and quality of their data, and make more informed decisions based on the data.&lt;/p&gt;
&lt;h2 id="using-data-quality-tools-to-find-and-fix-errors-in-data"&gt;Using Data Quality Tools to Find and Fix Errors in Data&lt;/h2&gt;
&lt;p&gt;Data quality is an important aspect of data science that can greatly affect the accuracy and usefulness of results obtained from data analysis. To address this, several data quality tools have been developed that help automate the process of finding and fixing errors in data. This blog post will discuss the use of data quality tools such as Talend, Trifacta, and Informatica to identify and correct errors in data.&lt;/p&gt;
&lt;h3 id="talend"&gt;Talend&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.talend.com/products/data-integration/"&gt;Talend&lt;/a&gt; is a widely used open-source data integration tool that provides a range of data quality features. It allows you to profile your data, which helps you understand the structure and content of your data, identify errors and inconsistencies, and determine the best way to clean and transform your data. Talend also provides a range of data cleaning and transformation functions, such as data type conversion, string manipulation, and data normalization, which you can use to correct errors in your data. Additionally, Talend allows you to define and enforce data quality rules, which help you ensure that your data meets certain quality standards.&lt;/p&gt;
&lt;h3 id="trifacta"&gt;Trifacta&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.trifacta.com/platform/data-wrangling/"&gt;Trifacta&lt;/a&gt; is a data wrangling tool that provides a user-friendly interface for data cleaning and transformation. It allows you to easily identify and correct errors in your data, such as missing values, incorrect data types, and outliers, by using a combination of visualizations, machine learning algorithms, and data wrangling operations. Trifacta also provides a range of data quality functions, such as data type conversion, string manipulation, and data normalization, which you can use to correct errors in your data. Additionally, Trifacta allows you to define and enforce data quality rules, which help you ensure that your data meets certain quality standards.&lt;/p&gt;
&lt;h3 id="informatica"&gt;Informatica&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.informatica.com/products/data-quality.html"&gt;Informatica&lt;/a&gt; is a powerful data quality tool that provides a range of features for data profiling, data cleaning, and data transformation. It allows you to profile your data, which helps you understand the structure and content of your data, identify errors and inconsistencies, and determine the best way to clean and transform your data. Informatica also provides a range of data cleaning and transformation functions, such as data type conversion, string manipulation, and data normalization, which you can use to correct errors in your data. Additionally, Informatica allows you to define and enforce data quality rules, which help you ensure that your data meets certain quality standards.&lt;/p&gt;
&lt;p&gt;Data quality tools such as Talend, Trifacta, and Informatica provide a powerful and automated way to find and fix errors in data. By using these tools, you can ensure that your data is accurate, consistent, and of high quality, which will greatly improve the accuracy and usefulness of results obtained from data analysis.&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="data-engineering"/><category term="dataset"/><category term="data-visualization"/><category term="data-cleaning"/></entry><entry><title>Pandas Schema Validation</title><link href="http://127.0.0.1:8000/pandas-schema-validation/" rel="alternate"/><published>2021-01-16T00:00:00+01:00</published><updated>2023-01-16T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2021-01-16:/pandas-schema-validation/</id><summary type="html">&lt;p&gt;Overview of the available tools and methods for schema validation in pandas, examplary code snippets and recommendation for when to use given tool.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#overview-of-available-tools-and-methods"&gt;Overview of Available Tools and Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#built-in-attributes"&gt;Built-in Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pandas-schema"&gt;Pandas Schema&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#great-expectations"&gt;Great Expectations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pandera"&gt;Pandera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-enforce"&gt;Data-enforce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#comparison-and-discussion"&gt;Comparison and Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;Pandas is a widely used library for data manipulation and analysis in Python. To ensure the data is in the correct format and conforms to certain constraints, schema validation is crucial. This process can be useful in various situations such as when importing data from external sources or before performing further analysis or machine learning tasks.&lt;/p&gt;
&lt;p&gt;There are several tools and methods available for schema validation in pandas such as &lt;code&gt;pandas_schema&lt;/code&gt;, &lt;code&gt;great_expectations&lt;/code&gt;, &lt;code&gt;pandera&lt;/code&gt; and &lt;code&gt;data-enforce&lt;/code&gt;. &lt;a href="https://github.com/multimeric/PandasSchema"&gt;pandas_schema&lt;/a&gt; and &lt;a href="https://docs.greatexpectations.io/en/latest/index.html"&gt;great_expectations&lt;/a&gt;  are widely used libraries for pandas data validation. &lt;a href="https://github.com/unionai-oss/pandera"&gt;pandera&lt;/a&gt;  and &lt;a href="https://github.com/CedricFR/dataenforce"&gt;data-enforce&lt;/a&gt;  are also popular libraries for pandas data validation.&lt;/p&gt;
&lt;p&gt;In this article, we will overview the available tools and methods for schema validation in pandas and provide example code snippets and links to further resources. We will also discuss the advantages and disadvantages of each tool and provide recommendations for when to use them.&lt;/p&gt;
&lt;p&gt;&lt;a id="overview-of-available-tools-and-methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="overview-of-available-tools-and-methods"&gt;Overview of Available Tools and Methods&lt;/h2&gt;
&lt;p&gt;The tools and methods discussed below accompanying exemplary code snippets. You can use the following contents of a &lt;code&gt;data.csv&lt;/code&gt; that comply with the schema used in this article:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;name,age,gender,col1,col2,col3,col4,col5
Alice,25,female,1,2.5,text1,True,2022-01-01
Bob,30,male,2,3.5,text2,False,2022-02-01
Charlie,35,male,3,4.5,text3,True,2022-03-01
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This file contains a dataframe with 8 columns: name, age, gender, col1, col2, col3, col4 and col5.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; and &lt;code&gt;gender&lt;/code&gt; are of type object&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt; is of type int&lt;/li&gt;
&lt;li&gt;&lt;code&gt;col1&lt;/code&gt;,&lt;code&gt;col2&lt;/code&gt; are of type int and float respectively&lt;/li&gt;
&lt;li&gt;&lt;code&gt;col3&lt;/code&gt; is of type object&lt;/li&gt;
&lt;li&gt;&lt;code&gt;col4&lt;/code&gt; is of type boolean&lt;/li&gt;
&lt;li&gt;&lt;code&gt;col5&lt;/code&gt; is of type datetime&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This file can be used in the examples above to perform data validation using different libraries and methods.&lt;/p&gt;
&lt;p&gt;Here is an example of the contents of a &lt;code&gt;data2.csv&lt;/code&gt; file that does not comply with the schema used in the article:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;name,age,gender,col1,col2,col3,col4,col5
Alice,25,female,1,2.5,text1,True,2022-01-01
Bob,30,male,2,3.5,text2,False,2022-02-01
Charlie,35,male,3,4.5,text3,True,2022-03-01
David,170,male,4,5.5,text4,True,2022-04-01
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This file contains a dataframe with 8 columns: name, age, gender, col1, col2, col3, col4 and col5.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The age of David is 170 which is out of range of the schema defined in the article which is range(0, 150)&lt;/li&gt;
&lt;li&gt;This file will not comply with the schema defined in the article and will raise an error when trying to validate it using the code provided in the article&lt;/li&gt;
&lt;li&gt;This file can be used to demonstrate the validation process and how it will raise errors for invalid data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="built-in-attributes"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="built-in-attributes"&gt;Built-in Attributes&lt;/h3&gt;
&lt;p&gt;Pandas provides built-in attributes such as &lt;code&gt;.dtypes&lt;/code&gt; and &lt;code&gt;.shape&lt;/code&gt; that can be used to check the data types and dimensions of a DataFrame. Here's an example of using these attributes to check that a DataFrame has the expected number of rows and columns, and that the columns have the expected data types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Check that the DataFrame has the expected number of rows and columns&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Check that the columns have the expected data types&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;col1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;col2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;col3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;col4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;col5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="pandas-schema"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="pandas-schema"&gt;Pandas Schema&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pandas_schema&lt;/code&gt; is a library that allows you to specify constraints on a DataFrame and then validate that the DataFrame conforms to those constraints. Here's an example of using the &lt;code&gt;pandas_schema&lt;/code&gt; library to define a schema for a DataFrame and then validate that the DataFrame conforms to the schema:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas_schema&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Schema&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pandas_schema.validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LeadingWhitespaceValidation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TrailingWhitespaceValidation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IsDtypeValidation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;InListValidation&lt;/span&gt;

&lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Schema&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LeadingWhitespaceValidation&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;TrailingWhitespaceValidation&lt;/span&gt;&lt;span class="p"&gt;()]),&lt;/span&gt;
    &lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;IsDtypeValidation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;InListValidation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;))]),&lt;/span&gt;
    &lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;InListValidation&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])])&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;errors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="great-expectations"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="great-expectations"&gt;Great Expectations&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://docs.greatexpectations.io/docs/"&gt;Great Expectations&lt;/a&gt; is a library that allows you to define and validate schemas using a more human-readable syntax. Here's an example of using the &lt;code&gt;great_expectations&lt;/code&gt; library to define a schema for a DataFrame and then validate that the DataFrame conforms to the schema:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;great_expectations&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ge&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ge&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define the schema&lt;/span&gt;
&lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;columns&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;col1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;expect_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;int&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;col2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;expect_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;float&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;col3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;expect_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;string&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;col4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;expect_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;bool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;col5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;expect_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;datetime&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# Validate the DataFrame against the schema&lt;/span&gt;
&lt;span class="n"&gt;validation_result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Check for any validation errors&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Data validation successful&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Validation errors:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="pandera"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="pandera"&gt;Pandera&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pandera&lt;/code&gt; is a library that allows you to define and validate schemas using a more human-readable syntax and more functionality. Here's an example of using the &lt;code&gt;pandera&lt;/code&gt; library to define a schema for a DataFrame and then validate that the DataFrame conforms to the schema:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandera&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pa&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define the schema&lt;/span&gt;
&lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrameSchema&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Float&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Boolean&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Datetime&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="c1"&gt;# Validate the DataFrame against the schema&lt;/span&gt;
&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="data-enforce"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="data-enforce"&gt;Data-enforce&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;data-enforce&lt;/code&gt; is a library that allows you to define and validate schemas using a more human-readable syntax and more functionality. Here's an example of using the &lt;code&gt;data-enforce&lt;/code&gt; library to define a schema for a DataFrame and then validate that the DataFrame conforms to the schema:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;data_enforce&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;de&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define the schema&lt;/span&gt;
&lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;de&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;de&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Float&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;de&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;de&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Boolean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;col5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;de&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Datetime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# Validate the DataFrame against the schema&lt;/span&gt;
&lt;span class="n"&gt;de&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;enforce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="comparison-and-discussion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="comparison-and-discussion"&gt;Comparison and Discussion&lt;/h2&gt;
&lt;p&gt;Each of these tools has its own advantages and disadvantages depending on the specific use case.&lt;/p&gt;
&lt;p&gt;The built-in attributes such as &lt;code&gt;.dtypes&lt;/code&gt; and &lt;code&gt;.shape&lt;/code&gt; may be sufficient for simple validation tasks, but they lack advanced functionality such as custom validation logic and integration with other data pipeline tools.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pandas_schema&lt;/code&gt; and &lt;code&gt;great_expectations&lt;/code&gt; libraries offer more advanced functionality such as custom validation logic and integration with other data pipeline tools, and they have a more human-readable syntax. &lt;code&gt;pandera&lt;/code&gt; and &lt;code&gt;data-enforce&lt;/code&gt; libraries also offer more advanced functionality than the built-in attributes and they have a more human-readable syntax.&lt;/p&gt;
&lt;p&gt;The choice of tool will depend on the complexity of the schema and the specific requirements of the project. &lt;strong&gt;If the schema is simple&lt;/strong&gt; and you only need to check data types and dimensions, the &lt;strong&gt;built-in attributes&lt;/strong&gt; may be sufficient. However, if you need &lt;strong&gt;more advanced functionality&lt;/strong&gt; such as custom validation logic or integration with other data pipeline tools, &lt;code&gt;pandas_schema&lt;/code&gt;, &lt;code&gt;great_expectations&lt;/code&gt;, &lt;code&gt;pandera&lt;/code&gt; or &lt;code&gt;data-enforce&lt;/code&gt; libraries are better choices.&lt;/p&gt;
&lt;p&gt;Overall, it is recommended to use &lt;code&gt;great_expectations&lt;/code&gt; for more complex projects, as it has more functionality and a more human-readable syntax. However, if you're looking for a more lightweight solution &lt;code&gt;pandas_schema&lt;/code&gt;, &lt;code&gt;pandera&lt;/code&gt; and &lt;code&gt;data-enforce&lt;/code&gt; are also good options.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="pandas"/><category term="python"/><category term="schema-validation"/><category term="pandas-schema"/><category term="great_expectations"/><category term="pandera"/><category term="data-enforce"/><category term="data-manipulation"/><category term="data-analysis"/><category term="tools-for-schema-validation"/><category term="methods-for-schema-validation"/><category term="dataframe-validation"/><category term="dtype-validation"/><category term="leading-whitespace-validation"/><category term="trailing-whitespace-validation"/><category term="in-list-validation"/></entry><entry><title>Evaluation of Interpretability for Explainable AI</title><link href="http://127.0.0.1:8000/evaluation-of-interpretability-for-explainable-ai/" rel="alternate"/><published>2020-11-05T00:00:00+01:00</published><updated>2023-01-19T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2020-11-05:/evaluation-of-interpretability-for-explainable-ai/</id><summary type="html">&lt;p&gt;Learn about the evaluation of interpretability in machine learning with this guide. Discover different levels and methods for assessing the explainability of models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Interpretability in machine learning is a complex and multifaceted topic. It refers to the ability of a model to explain its decisions and predictions to humans in a way that is understandable and meaningful. However, there is currently no agreed-upon definition or method for measuring interpretability.&lt;/p&gt;
&lt;!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#three-levels-of-evaluating-interpretability"&gt;Three levels of evaluating interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#application-level"&gt;Application level&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#human-level"&gt;Human level&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#function-level"&gt;Function level&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="three-levels-of-evaluating-interpretability"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="three-levels-of-evaluating-interpretability"&gt;Three levels of evaluating interpretability&lt;/h2&gt;
&lt;p&gt;Doshi-Velez and Kim (2017) propose a framework for evaluating interpretability at three levels: application level, human level, and function level.&lt;/p&gt;
&lt;p&gt;&lt;img alt="taxonomy of evaluation interpretability - three levels" src="/images/evaluation_of_interpretability/taxonomy_of_evaluation_interpretability.png"&gt;
&lt;strong&gt;Figure 1. Taxonomy of evaluation approaches for interpretability, source: Doshi-Velez and Kim (2017)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="application-level"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="application-level"&gt;Application level&lt;/h3&gt;
&lt;p&gt;At the application level, interpretability is evaluated by &lt;strong&gt;testing the explanation in the context of the real-world task for which the model was developed&lt;/strong&gt;. For example, in the case of a fracture detection software that uses machine learning to locate and mark fractures in X-rays, radiologists would test the software directly to evaluate the model. This approach requires a well-designed experimental setup and an understanding of how to assess the quality of the explanations. A good baseline for this evaluation is always how well a human would perform at explaining the same decision.&lt;/p&gt;
&lt;p&gt;&lt;a id="human-level"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="human-level"&gt;Human level&lt;/h3&gt;
&lt;p&gt;The human level evaluation is a simplified version of the application level evaluation. &lt;strong&gt;Instead of testing with domain experts, experiments are conducted with laypersons&lt;/strong&gt;. This approach is more cost-effective and allows for more testers to participate. For example, a user might be shown different explanations and asked to choose the best one.&lt;/p&gt;
&lt;p&gt;&lt;a id="function-level"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="function-level"&gt;Function level&lt;/h3&gt;
&lt;p&gt;The function level evaluation &lt;strong&gt;does not require human participation&lt;/strong&gt;. This approach is best suited for models that have already been evaluated at the human level by others. For example, it may be known that decision trees are well understood by end-users. In this case, a proxy for explanation quality might be the depth of the tree. Shorter trees would receive a higher explainability score. However, it is important to ensure that the predictive performance of the tree remains good and does not decrease too much compared to a larger tree.&lt;/p&gt;
&lt;h2 id="tools-and-methods-for-interpretability"&gt;Tools and methods for Interpretability&lt;/h2&gt;
&lt;p&gt;It's also worth noting that there are other approaches for evaluating interpretability, like using intrinsic and extrinsic evaluation metrics, as well as using techniques like saliency maps, LIME, SHAP, etc. These methods allow us to understand the feature importance and decision path of a model and evaluate the interpretability of a model.&lt;/p&gt;
&lt;p&gt;Overall, the evaluation of interpretability is an active area of research, and there is no one-size-fits-all approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,” no. Ml: 1–13. &lt;a href="http://arxiv.org/abs/1702.08608"&gt;http://arxiv.org/abs/1702.08608&lt;/a&gt;( 2017).&lt;/li&gt;
&lt;li&gt;Christoph Molnar, Interpretable Machine Learning. A Guide for Making Black Box Models Explainable (2019). Available &lt;a href="http://leanpub.com/interpretable-machine-learning"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Responsible AI"/><category term="rai"/><category term="interpretability"/><category term="xai"/><category term="explainability"/><category term="ai"/><category term="machine-learning"/><category term="model"/><category term="responsible-ai"/></entry><entry><title>Metrics Used to Compare Histograms</title><link href="http://127.0.0.1:8000/metrics-to-compare-histograms/" rel="alternate"/><published>2020-01-19T00:00:00+01:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2020-01-19:/metrics-to-compare-histograms/</id><summary type="html">&lt;p&gt;Learn about metrics used to compare histograms with examples of how to calculate them in python. From Chi-Squared distance to Kullback-Leibler divergence and Earth Mover's distance. A comprehensive guide.&lt;/p&gt;</summary><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Comparing histograms is a crucial step in data analysis, as it allows us to gain insights into the underlying distributions of our data. There are several metrics that can be used to compare histograms, each with its own strengths and weaknesses. In this article, we will discuss some of the most commonly used metrics for comparing histograms and provide examples of how to calculate them in Python.&lt;/p&gt;
&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#most-common-methods"&gt;Most common methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1-chi-squared-distance"&gt;1. Chi-Squared Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-earth-movers-distance"&gt;2. Earth Mover's Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-kullback-leibler-divergence"&gt;3. Kullback-Leibler Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-methods-for-histogram-comparison"&gt;Other methods for histogram comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="most-common-methods"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="most-common-methods"&gt;Most common methods&lt;/h2&gt;
&lt;p&gt;&lt;a id="1-chi-squared-distance"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-chi-squared-distance"&gt;1. Chi-Squared Distance&lt;/h3&gt;
&lt;p&gt;The Chi-Squared distance, also known as the Chi-Squared test, measures the difference between two histograms by comparing the observed frequencies to the expected frequencies. The Chi-Squared distance is defined as:&lt;/p&gt;
&lt;p&gt;$$ \chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i} $$&lt;/p&gt;
&lt;p&gt;Where $O_i$ is the observed frequency in bin $i$, $E_i$ is the expected frequency in bin $i$, and $n$ is the number of bins. The Chi-Squared distance is sensitive to large differences between the observed and expected frequencies, and is commonly used in hypothesis testing to determine if two histograms come from the same distribution.&lt;/p&gt;
&lt;p&gt;To calculate the Chi-Squared distance in Python, we can use the &lt;code&gt;scipy.stats.chisquare&lt;/code&gt; function from the SciPy library. Here is an example of how to use this function to calculate the Chi-Squared distance between two histograms:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;chisquare&lt;/span&gt;

&lt;span class="c1"&gt;# observed frequencies&lt;/span&gt;
&lt;span class="n"&gt;obs1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;obs2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# calculate the Chi-Squared distance&lt;/span&gt;
&lt;span class="n"&gt;chi2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chisquare&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obs2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chi2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="2-earth-movers-distance"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-earth-movers-distance"&gt;2. Earth Mover's Distance&lt;/h3&gt;
&lt;p&gt;The Earth Mover's distance (EMD) is a more sophisticated metric that takes into account the shape of the histograms as well as the differences in frequency. The EMD is defined as the minimum amount of "work" required to transform one histogram into the other, where "work" is defined as the product of the difference in frequency and the distance between the bins. The EMD is commonly used in image processing and computer vision, but can also be used to compare histograms.&lt;/p&gt;
&lt;p&gt;To calculate the EMD in Python, we can use the &lt;code&gt;emd&lt;/code&gt; function from the &lt;code&gt;pyemd&lt;/code&gt; library. Here is an example of how to use this function to calculate the EMD between two histograms:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyemd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;emd&lt;/span&gt;

&lt;span class="c1"&gt;# histogram bin centers&lt;/span&gt;
&lt;span class="n"&gt;bins1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;bins2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# histogram frequencies&lt;/span&gt;
&lt;span class="n"&gt;freq1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;freq2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# calculate the EMD&lt;/span&gt;
&lt;span class="n"&gt;emd_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;freq1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;freq2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emd_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="3-kullback-leibler-divergence"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-kullback-leibler-divergence"&gt;3. Kullback-Leibler Divergence&lt;/h3&gt;
&lt;p&gt;The Kullback-Leibler divergence (KLD), also known as the relative entropy, measures the difference between two probability distributions. The KLD is defined as:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_{i=1}^{n} P(i) \log\frac{P(i)}{Q(i)} $$&lt;/p&gt;
&lt;p&gt;Where $P$ is the probability distribution of the first histogram, $Q$ is the probability distribution of the second histogram, and $n$ is the number of bins. The KLD is a measure of the information lost when approximating one histogram with the other. It is commonly used in information theory and machine learning.&lt;/p&gt;
&lt;p&gt;To calculate the KLD in Python, we can use the &lt;code&gt;scipy.stats.entropy&lt;/code&gt; function from the SciPy library. Here is an example of how to use this function to calculate the KLD between two histograms:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;

&lt;span class="c1"&gt;# histogram frequencies&lt;/span&gt;
&lt;span class="n"&gt;freq1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;freq2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# normalize the frequencies to get probability distributions&lt;/span&gt;
&lt;span class="n"&gt;prob1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;freq1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;freq1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;prob2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;freq2&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;freq2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# calculate the KLD&lt;/span&gt;
&lt;span class="n"&gt;kld&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;entropy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kld&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="other-methods-for-histogram-comparison"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="other-methods-for-histogram-comparison"&gt;Other methods for histogram comparison&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Intersection&lt;/strong&gt;: it is a simple but widely used measure, which counts the number of bins where the histograms overlap. This metric gives a value between 0 and the minimum number of samples in the two histograms, with 0 indicating no overlap and the maximum value indicating perfect overlap.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bhattacharyya Distance&lt;/strong&gt;: The Bhattacharyya distance is a measure of similarity between two histograms. It is based on the Bhattacharyya coefficient, which is a measure of the similarity of two probability distributions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wasserstein Distance&lt;/strong&gt;: Also known as the "Earth Mover's Distance" (EMD), it is a distance measure between probability distributions. It is widely used in image processing and computer vision, and has been applied to the comparison of histograms.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are other metrics such as &lt;a href="https://en.wikipedia.org/wiki/Hellinger_distance"&gt;Hellinger distance&lt;/a&gt;, &lt;a href="https://encyclopediaofmath.org/wiki/Jeffreys_distance"&gt;Jeffreys divergence&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence"&gt;Jensen-Shannon divergence&lt;/a&gt;, etc. that can be used to compare histograms as well.&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There are several metrics that can be used to compare histograms, each with its own strengths and weaknesses.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Chi-Squared&lt;/strong&gt; distance is &lt;strong&gt;sensitive to large differences in frequency&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;Earth Mover's Distance&lt;/strong&gt; takes into account the &lt;strong&gt;shape of the histograms&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;Kullback-Leibler Divergence&lt;/strong&gt; measures the &lt;strong&gt;information lost&lt;/strong&gt; when approximating one histogram with the other.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By understanding these metrics and how to calculate them in Python, data scientists can choose the most appropriate metric for their analysis and gain deeper insights into the underlying distributions of their data.&lt;/p&gt;
&lt;p&gt;It is always recommended to try out different metrics and choose the best one that suits the problem and data.&lt;/p&gt;
&lt;p&gt;To learn more about these metrics and other techniques for comparing histograms, visit the following resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Chi-squared_test"&gt;Chi-Squared Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance"&gt;Earth Mover's Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;Kullback-Leibler Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/pyemd/"&gt;pyemd library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scipy.org/"&gt;SciPy library&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"/><category term="histogram"/><category term="statistics"/><category term="machine-learning"/><category term="statistical-tests"/><category term="metrics"/><category term="distance-metrics"/></entry><entry><title>Data Science Command-Line Tools</title><link href="http://127.0.0.1:8000/data-science-command-line-tools/" rel="alternate"/><published>2019-08-23T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-08-23:/data-science-command-line-tools/</id><summary type="html">&lt;p&gt;Description of GNU utils and other less standard tools that helps with processing data from CLI or with shell scripts.&lt;/p&gt;</summary><content type="html">&lt;p&gt;There are plenty of tools designed to ease the life of a Data Scientist. In this group, the special place has tools that are used from the command line. They are special because are available for most operating systems and are designed with Unix philosophy in mind: they do one thing extremely well, they can be chained creating convenient workflows.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of cli tools for data science" src="/images/cli_tools_1/cli_tools.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="textutils-from-gnu-coreutils"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="textutils-from-gnu-coreutils"&gt;Textutils from GNU Coreutils&lt;/h2&gt;
&lt;p&gt;From my experience, I have benefited most from mastering the GNU Coreutils. This is the collection of shellutils, fileutils, and textutils - and in this post, I will be discussing the latter.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Table 1. Textutils - text processing tools, part of GNU Coreutils&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;command&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;merge, print files to standard output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;a href="#comm"&gt;&lt;strong&gt;comm&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;compare sorted files line by line; can find differences, unique lines for each compared files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;csplit&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;divide files into parts using context&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;cut&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;remove sections from each line in the file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;expand/unexpand&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;convert tab into spaces / convert spaces to tabs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;fmt&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;simple text formatter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;fold&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;fold each input line to fit the given line length&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;&lt;a href="#head"&gt;head&lt;/a&gt;/&lt;a href="#tail"&gt;tail&lt;/a&gt;/&lt;a href="#shuf"&gt;shuf&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;display starting lines of file / display file ending lines/get random lines from file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;join&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;join lines from two files on common fields&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;nl&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;line numbering&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;paste&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;merge lines&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;a href="#sort"&gt;&lt;strong&gt;sort&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;sort lines of the text file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;&lt;a href="split"&gt;split&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;divide file into parts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;tac&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;merge, print in reverse order lines of files to standard output&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;tr&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;translate or remove characters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;&lt;a href="#uniq"&gt;uniq&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;remove duplicated lines from the file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;&lt;a href="#wc"&gt;wc&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;display the number of characters, words, and lines of the given file&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I will give a few examples of commands I use most often or ones that are exceptionally useful.&lt;/p&gt;
&lt;p&gt;&lt;a id="comm"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="comm"&gt;comm&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;comm&lt;/code&gt; command is very handy when comes to comparing lists stored in separate files. For example, you have a list of samples used for experiment #1 in one file and a list of samples used in experiment #2 in another file.
The &lt;code&gt;comm&lt;/code&gt; command takes sorted files as input and calculates unique lines for FILE1 and for FILE2 and the result is represented respectively as &lt;code&gt;column 1&lt;/code&gt; and &lt;code&gt;column 2&lt;/code&gt;. Having unique lines in column 1 and 2, the set of lines that appear in both files is calculated and the result is represented by column 3.&lt;/p&gt;
&lt;p&gt;Columns description for &lt;code&gt;comm&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;lines unique to FILE1&lt;/td&gt;
&lt;td&gt;lines unique to FILE1&lt;/td&gt;
&lt;td&gt;lines that appear in both files&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Having three columns calculated we can now suppress lines that appear in the given column in order to get lines that fulfill the desired condition.&lt;/p&gt;
&lt;p&gt;Meaning of the numbers used as &lt;code&gt;comm&lt;/code&gt; parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-1&lt;/code&gt;     suppress column 1 (lines unique to FILE1)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-2&lt;/code&gt;     suppress column 2 (lines unique to FILE2)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-3&lt;/code&gt;     suppress column 3 (lines that appear in both files)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See below three typical usages of &lt;code&gt;comm&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# find lines only in file1&lt;/span&gt;
comm&lt;span class="w"&gt; &lt;/span&gt;-23&lt;span class="w"&gt; &lt;/span&gt;file1&lt;span class="w"&gt; &lt;/span&gt;file2

&lt;span class="c1"&gt;# find lines only in file2&lt;/span&gt;
comm&lt;span class="w"&gt; &lt;/span&gt;-13&lt;span class="w"&gt; &lt;/span&gt;file1&lt;span class="w"&gt; &lt;/span&gt;file2

&lt;span class="c1"&gt;# find lines common to both files&lt;/span&gt;
comm&lt;span class="w"&gt; &lt;/span&gt;-12&lt;span class="w"&gt; &lt;/span&gt;file1&lt;span class="w"&gt; &lt;/span&gt;file2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="head"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="head"&gt;head&lt;/h3&gt;
&lt;p&gt;By default, &lt;code&gt;head&lt;/code&gt; prints the first 10 lines of each file provided as an argument to standard output.  With more than one file, precede each with a header giving the file name. You can control how many lines are displayed with option &lt;code&gt;-n&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head&lt;span class="w"&gt; &lt;/span&gt;-n5&lt;span class="w"&gt; &lt;/span&gt;file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can also print all but the last N lines by adding &lt;code&gt;-&lt;/code&gt; sign. For example in order to have all lines but the last one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;-1&lt;span class="w"&gt; &lt;/span&gt;file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="shuf"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="shuf"&gt;shuf&lt;/h3&gt;
&lt;p&gt;To quickly inspect the content of the dataset in a text file you can use &lt;code&gt;head&lt;/code&gt; which shows some first lines of the file. If you would like to have a more representative example of the content of the file, you can take a random sample of lines from the file with &lt;code&gt;shuf&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;shuf&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="sort"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="sort"&gt;sort&lt;/h3&gt;
&lt;p&gt;Here are options I often use for sort and unique operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;-b, --ignore-leading-blanks
              ignore leading blanks

-u, --unique
              with -c, check for strict ordering; without -c, output only the first  of  an equal run

-f, --ignore-case
              fold lower case to upper case characters

-i, --ignore-nonprinting
              consider only printable characters
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sort&lt;span class="w"&gt; &lt;/span&gt;-bufi&lt;span class="w"&gt; &lt;/span&gt;data.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="split"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="split"&gt;split&lt;/h3&gt;
&lt;p&gt;Sometimes there is a need to split datasets into smaller parts. For E.g. when processing large files you can be hit by memory limitations, or you want to speed up processing using parallel computing. To split the file into N parts with the equal number of lines, use:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;split&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;l/N&lt;span class="w"&gt; &lt;/span&gt;data.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;split&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;l/10&lt;span class="w"&gt; &lt;/span&gt;data.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will create series of files named: &lt;code&gt;xaa, xab, xac,...&lt;/code&gt;. The default pattern of  &lt;code&gt;split&lt;/code&gt; for file naming is PREFIXaa, PREFIXab,...; default PREFIX is &lt;code&gt;x&lt;/code&gt;. You can provide your own prefix e.g.
&lt;a id="tail"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="tail"&gt;tail&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;split&lt;span class="w"&gt; &lt;/span&gt;l/10&lt;span class="w"&gt; &lt;/span&gt;data.csv&lt;span class="w"&gt; &lt;/span&gt;part_
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and this will result in having files &lt;code&gt;part_aa, part_ab,...&lt;/code&gt;.  Suffixes don't have to be alphabetical: can be numeric (use switch &lt;code&gt;-d&lt;/code&gt;) or even hex (use switch &lt;code&gt;-x&lt;/code&gt;). What is particularly useful - prefixes can be used to save resulting files in a given location. Let's take an example &lt;code&gt;dataset.csv&lt;/code&gt; file located in the data directory, we want to split this file into parts and save results in &lt;code&gt;parts&lt;/code&gt; directory as shown below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;data
├── dataset.csv
└── parts
    ├── xaa
    ├── xab
    └── xac
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To achieve this create a parts directory and modify the prefix to &lt;code&gt;parts/x&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;split&lt;span class="w"&gt; &lt;/span&gt;l/3&lt;span class="w"&gt; &lt;/span&gt;data.csv&lt;span class="w"&gt; &lt;/span&gt;parts/x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Skipping the header row can be done with the tail&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tail&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;+2&lt;span class="w"&gt; &lt;/span&gt;file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This syntax with the usage of &lt;code&gt;+&lt;/code&gt; sign might require explanation. Here is an excerpt from the man page:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;-n N means output the last N lines, instead of the last 10; or use +N to output lines starting with the Nth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The usage of &lt;code&gt;+&lt;/code&gt; sign can be considered as inverting the argument and telling the tail to print everything but the first x-1 lines. Note that &lt;code&gt;tail -n +1&lt;/code&gt; would print the whole file, &lt;code&gt;tail -n +2&lt;/code&gt; everything but the first line, etc.&lt;/p&gt;
&lt;p&gt;the alternative solution involves &lt;code&gt;sed&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sed&lt;span class="w"&gt; &lt;/span&gt;1d&lt;span class="w"&gt; &lt;/span&gt;file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="uniq"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="uniq"&gt;uniq&lt;/h3&gt;
&lt;p&gt;often used with &lt;code&gt;sort&lt;/code&gt; in a way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cat&lt;span class="w"&gt; &lt;/span&gt;file.txt&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;sort&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;uniq
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If there are no special circumstances &lt;code&gt;cat&lt;/code&gt; should be avoided:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sort&lt;span class="w"&gt; &lt;/span&gt;file.txt&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;uniq
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Moreover, &lt;code&gt;sort&lt;/code&gt; has the option &lt;code&gt;-u&lt;/code&gt; which stands for &lt;em&gt;unique&lt;/em&gt;. Therefore, to have unique lines it is sufficient to use:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sort&lt;span class="w"&gt; &lt;/span&gt;-u&lt;span class="w"&gt; &lt;/span&gt;file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="wc"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="wc"&gt;wc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;count lines&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wc&lt;span class="w"&gt; &lt;/span&gt;-l&lt;span class="w"&gt; &lt;/span&gt;data.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;count lines in multiple files at once&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wc&lt;span class="w"&gt; &lt;/span&gt;-l&lt;span class="w"&gt; &lt;/span&gt;*.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For special tasks - counting lines in very large files one can consider using replacement: &lt;a href="https://github.com/crioux/turbo-linecount"&gt;Super-Fast Multi-Threaded Line Counter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Linux"/><category term="machine-learning"/><category term="Linux"/></entry><entry><title>Top Popular ZSH Plugins on GitHub (2019)</title><link href="http://127.0.0.1:8000/top-popular-zsh-plugins-on-github-2019/" rel="alternate"/><published>2019-07-14T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-07-14:/top-popular-zsh-plugins-on-github-2019/</id><summary type="html">&lt;p&gt;On the GitHub project &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt; you can find 1700+ links to plugins, themes, and Zsh plugin managers/frameworks. The number of tools listed on that page is high and it is difficult to get orientation on which plugins gained already a good reputation from the Zsh users community. This post aims at identifying the most popular tools where popularity is measured by the number of stars that Github users added to a given plugin or tool.&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This article was written in 2019, for a more recent version of the ranking see: &lt;a href="../top-popular-zsh-plugins-on-github-2023/"&gt;2023&lt;/a&gt;.
The older articles have description of the selected, interesting tools that are not in this article - so you might also to visit the older editions of the survey.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is a list of command line tools and Zsh plugins improving the experience when working in text mode. The list was created using resources from &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt;. The GitHub collection is called "Zsh plugins" but in fact, it is not strictly limited to Zsh plugins but contains a lot of CLI tools that are not Zsh plugins. I made already a similar list 1.5 years ago - you can find results here: &lt;a href="http://safjan.com/articles/posts/top-popular-zsh-plugins-on-github"&gt;top zsh plugins&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On the GitHub project &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt; you can find currently 1700+ links to plugins, themes, and Zsh plugin managers/frameworks. The collection grows in size and gets more and more intimidating for the reader. I decided to filter out tools that are most popular. The popularity is measured with the number of stars that Github users added to given plugin.&lt;/p&gt;
&lt;h1 id="what-are-github-stars"&gt;What are Github stars?&lt;/h1&gt;
&lt;p&gt;Stars is the way how users can 'bookmark' projects - this can serve as indication for others that project successfully grabbed someone's attention. The &lt;code&gt;stargazers&lt;/code&gt; statistics are available via &lt;a href="https://developer.github.com/v4/"&gt;GithubAPI&lt;/a&gt;. The metrics is not capturing directly how popular given tool is but I expect, it is a good indication of how well is grabbing attention of Github users. For sake of clarity, I have excluded frameworks, themes, prompts and fonts  that are also listed on "Awesome Zsh plugins" website. The most popular choices excluded from the tools list are posted in the end of this article.&lt;/p&gt;
&lt;h1 id="top-20-most-popular-tools-as-of-july-2019"&gt;Top 20 most popular tools as of July 2019&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th style="text-align: right;"&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/nvbn/thefuck"&gt;thefuck&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Magnificent app which corrects your previous console command&lt;/td&gt;
&lt;td style="text-align: right;"&gt;44.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/junegunn/fzf"&gt;fzf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A command-line fuzzy finder&lt;/td&gt;
&lt;td style="text-align: right;"&gt;22.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Simple Python version management&lt;/td&gt;
&lt;td style="text-align: right;"&gt;15.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/BurntSushi/ripgrep"&gt;ripgrep&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ripgrep recursively searches directories for a regex pattern&lt;/td&gt;
&lt;td style="text-align: right;"&gt;15.3k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sharkdp/bat"&gt;bat&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cat(1) clone with wings&lt;/td&gt;
&lt;td style="text-align: right;"&gt;14.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/powerline/powerline"&gt;powerline&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Powerline is a statusline plugin for vim, and provides statuslines and prompts for several other applications, including zsh, bash, tmux, IPython, Awesome and Qtile&lt;/td&gt;
&lt;td style="text-align: right;"&gt;10.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/rupa/z"&gt;z&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;z - jump around&lt;/td&gt;
&lt;td style="text-align: right;"&gt;10.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-autosuggestions"&gt;zsh-autosuggestions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish-like autosuggestions for zsh&lt;/td&gt;
&lt;td style="text-align: right;"&gt;10.0k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/wting/autojump"&gt;autojump&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cd command that learns - easily navigate directories from the command line&lt;/td&gt;
&lt;td style="text-align: right;"&gt;9.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/tmuxinator/tmuxinator"&gt;tmuxinator&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Manage complex tmux sessions easily&lt;/td&gt;
&lt;td style="text-align: right;"&gt;9.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/bcicen/ctop"&gt;ctop&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Top-like interface for container metrics&lt;/td&gt;
&lt;td style="text-align: right;"&gt;8.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-syntax-highlighting"&gt;zsh-syntax-highlighting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish shell like syntax highlighting for Zsh&lt;/td&gt;
&lt;td style="text-align: right;"&gt;7.5k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/peco/peco"&gt;peco&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Simplistic interactive filtering tool&lt;/td&gt;
&lt;td style="text-align: right;"&gt;5.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/asdf-vm/asdf"&gt;asdf&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Extendable version manager with support for Ruby, Node.js, Elixir, Erlang &amp;amp; more&lt;/td&gt;
&lt;td style="text-align: right;"&gt;5.4k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/StackExchange/blackbox"&gt;blackbox&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Safely store secrets in Git/Mercurial/Subversion&lt;/td&gt;
&lt;td style="text-align: right;"&gt;4.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/postmanlabs/newman"&gt;newman&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Newman is a command-line collection runner for Postman&lt;/td&gt;
&lt;td style="text-align: right;"&gt;3.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-completions"&gt;zsh-completions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Additional completion definitions for Zsh&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/mooz/percol"&gt;percol&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Adds flavor of interactive filtering to the traditional pipe concept of UNIX shell&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2.8k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/gruntwork-io/terragrunt"&gt;terragrunt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Terragrunt is a thin wrapper for Terraform that provides extra tools for working with multiple Terraform modules&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/karan/joe"&gt;joe&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A .gitignore magician in your command line&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2.7k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Linux"/><category term="zsh"/><category term="scrapping"/><category term="python"/><category term="Linux"/></entry><entry><title>Learn Bayesian Methods in 4 Steps - By Reading and by Doing</title><link href="http://127.0.0.1:8000/learn-bayesian-methods-in-4-steps-by-reading-and-by-doing/" rel="alternate"/><published>2019-07-09T00:00:00+02:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-07-09:/learn-bayesian-methods-in-4-steps-by-reading-and-by-doing/</id><summary type="html">&lt;p&gt;This post proposes a 4-step path for learning Bayesian methods. The first step is going through the book "Bayesian methods for hackers", second, using complementary books for probability and statistics, the third, reading How to become a Bayesian in eight easy steps, and last, going through the book full of exercises - "Think Bayes".&lt;/p&gt;</summary><content type="html">&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;When reading this article you probably have some experience with machine learning models. You might have tried RandomForest, XGBoost, etc. They are easy to use but it is difficult to understand how final predictions were done. This is sometimes referred to as predictions out of the black box. Of course, there are techniques that might help a bit e.g. extracting feature importance from the trained models.&lt;/p&gt;
&lt;p&gt;Bayesian methods are a great helper in understanding how actual reasoning was done. These methods provide probabilistic models that can describe the process that produced the data we want to process. Besides accurate predictions, we often need an understanding of what is important in the process. Through understanding gain confidence in used models. We need convenience when moving with machine learning from toy projects to real business applications. Bayesian can offer such understanding and convenience and that is why these methods are gaining attention.&lt;/p&gt;
&lt;p&gt;In this blog post I will present 4 steps for Bayesian methods mastery. The rough estimate is that you will need to dedicate around 100 hours to complete this 4-steps path.&lt;/p&gt;
&lt;h2 id="1-bayesian-methods-for-hackers-free-book-in-form-of-jupyter-notebooks-with-interactive-content"&gt;1. "Bayesian methods for hackers"  - free book in form of Jupyter notebooks with interactive content&lt;/h2&gt;
&lt;p&gt;&lt;img style="float: lefts;" src="/images/learn_bayes/bmh.jpg" width="25%" height="25%"&gt;&lt;/p&gt;
&lt;p&gt;The first chapter of "Bayesian methods for hackers" (BMH) will introduce you to the Bayesian way of thinking. Understand reducing uncertainty using observations. You will go through the first example that is showing statistical modeling of the texting rate. The following chapters explain new techniques in detail. New techniques are immediately applied to solving exemplary problems.&lt;/p&gt;
&lt;p&gt;For myself, when progressing through the book, I felt that I need to refresh my statistical knowledge and started looking for the proper book. The math required to use these methods is already provided in the book. Yet, I needed a better understanding of different random variable distributions.  This is something that I already learned years ago in university courses but I needed a refresher.&lt;/p&gt;
&lt;h2 id="2-probability-and-statistics-books-that-will-help-you-learnrefresh-math-to-build-a-solid-foundation"&gt;2. Probability and Statistics books that will help you learn/refresh math to build a solid foundation&lt;/h2&gt;
&lt;p&gt;My choice for complementary probability and statistics books was twofold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For a light introduction, on the college level: &lt;a href="https://leanpub.com/openintro-statistics"&gt;Open Intro to statistics 4th edition&lt;/a&gt;  by D. Diez, M. Cetinkaya-Rundel, and Ch. Barr. According to my needs, Chapter 4 "Distributions of random variables" was pleasant to read.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For deep dive: &lt;a href="http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf"&gt;Probability Theory: The Logic of Science: Principles and Elementary Applications"&lt;/a&gt; by E. T. Jaynes. This book itself could be a subject of learning for hundreds of hours, but reading separate chapters or sections still should be fine.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="3-how-to-become-a-bayesian-in-eight-easy-steps-an-annotated-reading-list"&gt;3. "How to become a Bayesian in eight easy steps: An annotated reading list"&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://psyarxiv.com/ph6sw"&gt;"How to become a Bayesian in eight easy steps: An annotated reading list"&lt;/a&gt; by Etz, Alexander, et al., is a paper, not a target at Computer Scientists. Actually, it originates from the field of psychology but is written in a domain-agnostic style, so readers from any discipline can enjoy reading this. The paper has a survey style and uses the classification of the covered papers in two dimensions: difficulty (from easy to hard), and focus (from theoretical to practical). See the Figure below, borrowed from the paper.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/learn_bayes/readinglist.png" width="45%" height="45%"&gt;&lt;/p&gt;
&lt;p&gt;The main paper and references are rather light reading and I found it useful in building context for diving into  Bayesian analysis.&lt;/p&gt;
&lt;h2 id="4-exercises-to-develop-bayesian-thinking-think-bayes-by-allen-downey"&gt;4. Exercises to develop Bayesian thinking: "Think Bayes" by Allen Downey&lt;/h2&gt;
&lt;p&gt;&lt;img style="float: left;" src="/images/learn_bayes/think_bayes_1.jpg" width="25%" height="25%"&gt;&lt;/p&gt;
&lt;p&gt;Another great book to learn Bayesian thinking. It is divided into smaller units than BMH which makes it easier to digest for readers that are quickly losing attention when reading scientific stuff. When compared to BMH, it has much more examples. Crashing a large number of cases is to me very good approach for training Bayesian intuition and learning methods.&lt;/p&gt;
&lt;p&gt;Will you give it a try to Bayesian methods? If you have a proposal for an alternative learning path - please email me.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="machine-learning"/><category term="statistics"/><category term="probability"/><category term="howto"/></entry><entry><title>Kaggle Evaluation Metrics Used for Regression Problems</title><link href="http://127.0.0.1:8000/kaggle-evaluation-metrics-used-for-regression-problems/" rel="alternate"/><published>2019-02-16T00:00:00+01:00</published><updated>2019-03-01T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-02-16:/kaggle-evaluation-metrics-used-for-regression-problems/</id><summary type="html">&lt;p&gt;This post describe evaluation metrics used in Kaggle competitions where problem to solve is has regression nature. Eight different metrics are described, namely - Absolute Error (AE), Mean Absolute Error (MAE), Weighted Mean Absolute Error (WMAE), Pearson Correlation Coefficient, Spearman’s Rank Correlation, Root Mean Squared Error (RMSE), Root Mean Squared Logarithmic Error (RMSLE), Mean Columnwise Root Mean Squared Error (MCRMSE).&lt;/p&gt;</summary><content type="html">&lt;p&gt;X:[[2019-02-16-kaggle-metrics_regression]]
X:[[2023-02-13-metrics_interpretation_for_measuring_regression_model]]
X:[[metrics_for_evaluation_in_regression_problems]]&lt;/p&gt;
&lt;p&gt;While crafting machine learning model there is always need to asses its performance. When trying multiple models or hyper parameter tuning it is useful to compare different approaches and choose the best one. The &lt;a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"&gt;sklearn.metrics&lt;/a&gt; provides plethora of metrics for suitable for distinct purposes.&lt;/p&gt;
&lt;p&gt;In this series of posts I will discuss four groups of common machine learning tasks each requires specific metrics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Regression&lt;/em&gt; - predict value of one or more variables that are continuous, e.g. predict stock price of given asset or predict temperature for next day.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Binary classification&lt;/em&gt; - assign sample to one of two classes - example: classify image as one containing "cat" or "dog"&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multiple class classification&lt;/em&gt; - assign sample to one of many classes example: classify new article to category "sport", "politics", "economy", "pop-culture",...&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Other&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href="https://www.kaggle.com/competitions"&gt;Kaggle competitions&lt;/a&gt; give insight into approach taken by Kaggle team to select best evaluation metrics for given task. There use to be Kaggle wiki under containing short definitions of metrics used in Kaggle competitions but it is not available anymore. In this post we will look closer at the first group and explain few model evaluation metrics used in regression problems. Here metrics that are discussed in this post.&lt;/p&gt;
&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#absolute-error---ae"&gt;Absolute Error - AE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mean-absolute-error---mae"&gt;Mean Absolute Error - MAE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#weighted-mean-absolute-error---wmae"&gt;Weighted Mean Absolute Error - WMAE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pearson-correlation-coefficient"&gt;Pearson Correlation Coefficient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spearman%E2%80%99s-rank-correlation"&gt;Spearman’s Rank Correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#root-mean-squared-error---rmse"&gt;Root Mean Squared Error - RMSE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#root-mean-squared-logarithmic-error---rmsle"&gt;Root Mean Squared Logarithmic Error - RMSLE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mean-columnwise-root-mean-squared-error---mcrmse"&gt;Mean Columnwise Root Mean Squared Error - MCRMSE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="absolute-error---ae"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="absolute-error-ae"&gt;Absolute Error - AE&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The sum of the absolute value of each individual error.&lt;/strong&gt;
$$
\mathrm{AE} = \sum_{i=1}^n | y_i - \hat{y}_i |
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;p&gt;$\mathrm{AE} = |e_i| = |y_i-\hat{y_i}|$,&lt;/p&gt;
&lt;p&gt;$n$ - number test of samples,&lt;/p&gt;
&lt;p&gt;$y_i$ - actual variable value,&lt;/p&gt;
&lt;p&gt;$\hat{y}_i$ - predicted variable value.&lt;/p&gt;
&lt;p&gt;MAE can cause notable difference between public and private leaderboard calculations. One drawback of the Absolute Error metrics is that direct comparison of the metrics for model used to predict variables on different scales is not possible. E.g. when using model to financial predictions of S&amp;amp;P 500 index and using the same model to predict value of Microsoft stock price we cannot compare their performance using this metrics since units and ranges are different. The S&amp;amp;P 500 is expressed in points and stock price of asset is expressed in dollars. In this situation one can use (percentage error) to get evaluation metrics in common scale.&lt;/p&gt;
&lt;p&gt;Exemplary competition using Mean Absolute Error for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/Eurovision2010#Evaluation"&gt;Forecast Eurovision Voting&lt;/a&gt; - This competition requires contestants to forecast the voting for this year's Eurovision Song Contest in Norway on May 25th, 27th and 29th.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="mean-absolute-error---mae"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mean-absolute-error-mae"&gt;Mean Absolute Error - MAE&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Mean of the absolute value of each individual error.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The mean absolute error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes. The mean absolute error is given by formula:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{MAE} = \frac{1}{n}\sum_{i=1}^n \left| y_i - \hat{y_i}\right| =\frac{1}{n}\sum_{i=1}^n \left| e_i \right|.
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;p&gt;$n$ - number test of samples,&lt;/p&gt;
&lt;p&gt;$y_i$ - actual variable value,&lt;/p&gt;
&lt;p&gt;$\hat{y}_i$ - predicted variable value.&lt;/p&gt;
&lt;p&gt;see also &lt;a href="http://climate.geog.udel.edu/~climate/publication_html/Pdf/WM_CR_05.pdf"&gt;paper&lt;/a&gt;: Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance&lt;/p&gt;
&lt;p&gt;Five exemplary competitions using Mean Absolute Error for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/LANL-Earthquake-Prediction#evaluation"&gt;LANL Earthquake Prediction&lt;/a&gt; - Can you predict upcoming laboratory earthquakes?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/pubg-finish-placement-prediction#evaluation"&gt;PUBG Finish Placement Prediction&lt;/a&gt; - Can you predict the battle royale finish of PUBG Players?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/allstate-claims-severity#evaluation"&gt;Allstate Claims Severity&lt;/a&gt; - How severe is an insurance claim?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/loan-default-prediction#evaluation"&gt;Loan Default Prediction - Imperial College London&lt;/a&gt; - Constructing an optimal portfolio of loans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/finding-elo#evaluation"&gt;Finding Elo&lt;/a&gt; - Predict a chess player's FIDE Elo rating from one game.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="weighted-mean-absolute-error---wmae"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="weighted-mean-absolute-error-wmae"&gt;Weighted Mean Absolute Error - WMAE&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Weighted average of absolute errors.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;WMAE can be used as evaluation tool for better assessing the model performance with respect to the goals of the application. For example, in the case of recommending books or movies it could be possible that the accuracy of the predictions varies when focusing on past or recent products. In this situation, it is not reasonable that every error were treated equally, so more stress should be put in recent items.&lt;/p&gt;
&lt;p&gt;WMAE can be also useful as a diagnosis tool that, using a "magnifying lens", can help to identify those cases where an algorithm is having trouble with. The formula for calculating WMAE is:&lt;/p&gt;
&lt;p&gt;$$
\textrm{WMAE} = \frac{1}{n} \sum_{i=1}^n w_i | y_i - \hat{y}_i |,
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;p&gt;$n$ - number test of samples,&lt;/p&gt;
&lt;p&gt;$w_i$ - weights for sample $i$,&lt;/p&gt;
&lt;p&gt;$y_i$ - actual variable value,&lt;/p&gt;
&lt;p&gt;$\hat{y}_i$ - predicted variable value.&lt;/p&gt;
&lt;p&gt;Two exemplary competitions using Weighted Mean Absolute Error for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/the-winton-stock-market-challenge#evaluation"&gt;The Winton Stock Market Challenge&lt;/a&gt; - Join a multi-disciplinary team of research scientists.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting#evaluation"&gt;Walmart Recruiting - Store Sales Forecasting&lt;/a&gt; - Use historical markdown data to predict store sales.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="pearson-correlation-coefficient"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="pearson-correlation-coefficient"&gt;Pearson Correlation Coefficient&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Covariance of the two variables divided by the product of the standard deviation of each data sample.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is the normalization of the covariance between the two variables to give an interpretable score. The Pearson correlation coefficient can be used to summarize the strength of the linear relationship between two data samples. The formula for calculating Pearson correlation coefficient is:&lt;/p&gt;
&lt;p&gt;$$
p = \frac{cov(y_i, \hat{y}_i)}{std(y_i)  std(\hat{y}_i)}
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;p&gt;$cov()$ - is covariation function,&lt;/p&gt;
&lt;p&gt;$std()$ - is standard deviation&lt;/p&gt;
&lt;p&gt;$y_i$ - actual variable value,&lt;/p&gt;
&lt;p&gt;$\hat{y}_i$ - predicted variable value&lt;/p&gt;
&lt;p&gt;$p$ - Pearson correlation coefficient.&lt;/p&gt;
&lt;p&gt;The use of mean and standard deviation in the calculation requires data samples to have a Gaussian or Gaussian-like distribution.&lt;/p&gt;
&lt;p&gt;Exemplary competition using Pearson Correlation Coefficient for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/MerckActivity#evaluation"&gt;Merck Molecular Activity Challenge&lt;/a&gt; - Help develop safe and effective medicines by predicting molecular activity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="spearman%E2%80%99s-rank-correlation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="spearmans-rank-correlation"&gt;Spearman’s Rank Correlation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Covariance of the two variables converted to ranks divided by the product of the standard deviation of ranks for each variable.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Two variables may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables. The two variables being considered may have a non-Gaussian distribution.&lt;/p&gt;
&lt;p&gt;The Spearman’s correlation coefficient can be used to summarize the nonlinear relation between the two data samples. Raw scores $y_i$ and $\hat{y}_i$ are converted to ranks respectively: $ry_i$ and $\hat{ry}_i$. The formula for calculating Spearman's rank correlation coefficient is:&lt;/p&gt;
&lt;p&gt;$$
r=\frac{cov(ry_i, \hat{ry}_i)}{std(ry_i)std(\hat{ry}_i)}
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;p&gt;$cov()$ - is covariation function,&lt;/p&gt;
&lt;p&gt;$std()$ - is standard deviation,&lt;/p&gt;
&lt;p&gt;$ry_i$ - rank of variable value,&lt;/p&gt;
&lt;p&gt;$\hat{ry}_i$ - rank of predicted variable value,&lt;/p&gt;
&lt;p&gt;$r$ - Spearman's correlation coefficient.&lt;/p&gt;
&lt;p&gt;Exemplary competition using Spearman’s Rank Correlation for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draper Satellite Image Chronology](&lt;a href="https://www.kaggle.com/c/draper-satellite-image-chronology#evaluation"&gt;https://www.kaggle.com/c/draper-satellite-image-chronology#evaluation&lt;/a&gt;) - Can you put order to space and time?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="root-mean-squared-error---rmse"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="root-mean-squared-error-rmse"&gt;Root Mean Squared Error - RMSE&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The square root of the mean/average of the square of all of the error.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The use of RMSE is very common and it makes an excellent general purpose error metric for numerical predictions. Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors. The formula for calculating RMSE is:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;p&gt;$n$ - number test of samples,&lt;/p&gt;
&lt;p&gt;$y_i$ - actual variable value,&lt;/p&gt;
&lt;p&gt;$\hat{y}_i$ - predicted variable value.&lt;/p&gt;
&lt;p&gt;Five exemplary competition using Root Mean Squared Error for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/elo-merchant-category-recommendation#evaluation"&gt;Elo Merchant Category Recommendation&lt;/a&gt; - Help understand customer loyalty.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/ga-customer-revenue-prediction#evaluation"&gt;Google Analytics Customer Revenue Prediction&lt;/a&gt; - Predict how much GStore customers will spend.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques#evaluation"&gt;House Prices: Advanced Regression Techniques&lt;/a&gt; - Predict sales prices and practice feature engineering, RFs, and gradient boosting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales#evaluation"&gt;Predict Future Sales&lt;/a&gt; - Final project for "How to win a data science competition" Coursera course.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/new-york-city-taxi-fare-prediction#evaluation"&gt;New York City Taxi Fare Prediction&lt;/a&gt; - Can you predict a rider's taxi fare?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="root-mean-squared-logarithmic-error---rmsle"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="root-mean-squared-logarithmic-error-rmsle"&gt;Root Mean Squared Logarithmic Error - RMSLE&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Root mean squared error of variables transformed to logarithmic scale&lt;/strong&gt;.
$$
\mathrm{RMSLE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(log(\hat{y}_i + 1) - log(y_i + 1))^2}
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;p&gt;$n$ - number of test samples,&lt;/p&gt;
&lt;p&gt;$\hat{y}_i$ is the predicted variable,&lt;/p&gt;
&lt;p&gt;$y_i$ is the actual variable,&lt;/p&gt;
&lt;p&gt;$log(x)$ is the natural logarithm of $x$.&lt;/p&gt;
&lt;p&gt;The RMSLE is higher when the discrepancies between predicted and actual values are larger. Compared to Root Mean Squared Error (RMSE), RMSLE does not heavily penalize huge discrepancies between the predicted and actual values when both values are huge. In this cases only the percentage differences matter (difference of variable logarithms is equivalent to ratio of variables).&lt;/p&gt;
&lt;p&gt;Exemplary competition using Root Mean Squared Logarithmic Error for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/santander-value-prediction-challenge#evaluation"&gt;Santander Value Prediction Challenge&lt;/a&gt; - Predict the value of transactions for potential customers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge#evaluation"&gt;Mercari Price Suggestion Challenge&lt;/a&gt; - Can you automatically suggest product prices to online sellers?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting#evaluation"&gt;Recruit Restaurant Visitor Forecasting&lt;/a&gt; - Predict how many future visitors a restaurant will receive&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/nyc-taxi-trip-duration#evaluation"&gt;New York City Taxi Trip Duration&lt;/a&gt; - Share code and data to improve ride time predictions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/c/sberbank-russian-housing-market#evaluation"&gt;Sberbank Russian Housing Market&lt;/a&gt; - Can you predict realty price fluctuations in Russia’s volatile economy?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="mean-columnwise-root-mean-squared-error---mcrmse"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="mean-columnwise-root-mean-squared-error-mcrmse"&gt;Mean Columnwise Root Mean Squared Error - MCRMSE&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Errors of each k-fold CV trials were averaged over n test samples across m target variables.&lt;/strong&gt;
$$
MCRMSE = \frac{1}{m}\sum_{j=1}^{m}\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_
{ij}-\hat{y}&lt;em j="1"&gt;{ij})^2}
$$
Note that expression under square root is RMSE, thus we can write:
$$
MCRMSE = \frac{1}{m}\sum&lt;/em&gt;RMSE_j
$$}^{m&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;p&gt;$m$ - number of predicted variables,&lt;/p&gt;
&lt;p&gt;$n$ - number of test samples,&lt;/p&gt;
&lt;p&gt;$y_{ij}$ - $i$-th actual value of $j$-th variable,&lt;/p&gt;
&lt;p&gt;$\hat{y}_{ij}$ - $i$-th predicted value of $j$-th variable.&lt;/p&gt;
&lt;p&gt;Exemplary competition using Mean Columnwise Root Mean Squared Error for model evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/afsis-soil-properties#evaluation"&gt;Africa Soil Property Prediction Challenge&lt;/a&gt; - Predict physical and chemical properties of soil using spectral measurements&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="references"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="archive.org"&gt;Kaggle wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ke.tu-darmstadt.de/lehre/arbeiten/studien/2015/Dong_Ying.pdf"&gt;Beating Kaggle the easy way, page 43&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/"&gt;How to Use Correlation to Understand the Relationship Between Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://books.google.pl/books?id=bsEDCwAAQBAJ&amp;amp;pg=PA23&amp;amp;lpg=PA23&amp;amp;dq=%22Mean+Column+Wise+Root+Mean+Squared+Error%22&amp;amp;source=bl&amp;amp;ots=CXTjTNgehR&amp;amp;sig=ACfU3U0Jsn7QkzFecjR-EQC5mtD9p_lBCA&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=2ahUKEwjAqYPixa_gAhXF8qYKHeGrDbYQ6AEwBXoECAAQAQ#v=onepage&amp;amp;q=%22Mean%20Column%20Wise%20Root%20Mean%20Squared%20Error%22&amp;amp;f=false"&gt;Mean Columnwise Root Mean Squared Error - google books&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/metrics-to-understand-regression-models-in-plain-english-part-1-c902b2f4156f"&gt;Metrics to Understand Regression Models in Plain English: Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Machine Learning"/><category term="machine-learning"/><category term="evaluation"/><category term="metrics"/><category term="model-performance"/><category term="evaluation"/><category term="Kaggle"/></entry><entry><title>How to Install TensorFlow and Keras on Windows 10</title><link href="http://127.0.0.1:8000/how-to-install-tensorflow-and-keras-on-windows-10/" rel="alternate"/><published>2019-01-17T00:00:00+01:00</published><updated>2019-02-08T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-01-17:/how-to-install-tensorflow-and-keras-on-windows-10/</id><summary type="html">&lt;p&gt;Guide on how to install TensorFlow cpu-only version - the case for machines without GPU supporting CUDA. Step-by-step procedure starting from creating conda environment till testing if TensorFlow and Keras Works.&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;EDIT 2021&lt;/strong&gt;: This post is partially depreciated by now since for TensorFlow 2.x CPU and GPU versions are integrated - there is no separate install and Keras is integrated with TensorFlow - no need to install separately unless you have good reasons for separate install.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Quick guide on how to install TensorFlow cpu-only version - the case for machines without GPU supporting CUDA.&lt;/p&gt;
&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#creating-conda-environment-for-working-with-tensorflow-and-keras"&gt;Creating Conda environment for working with TensorFlow and Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installing-tensorflow"&gt;Installing TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installing-keras"&gt;Installing Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="creating-conda-environment-for-working-with-tensorflow-and-keras"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="creating-conda-environment-for-working-with-tensorflow-and-keras"&gt;Creating Conda environment for working with TensorFlow and Keras&lt;/h1&gt;
&lt;p&gt;Open &lt;code&gt;anaconda prompt&lt;/code&gt; (hit &lt;code&gt;Win+Q&lt;/code&gt;, type anaconda) and create conda virtualenv:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda&lt;span class="w"&gt; &lt;/span&gt;create&lt;span class="w"&gt; &lt;/span&gt;-n&lt;span class="w"&gt; &lt;/span&gt;tf_windows&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;this will create minimal environement&lt;/p&gt;
&lt;p&gt;When the environment is created, activate it. After that the environment’s name will be added before the prompt.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;activate&lt;span class="w"&gt; &lt;/span&gt;tf_windows
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="installing-tensorflow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="installing-tensorflow"&gt;Installing TensorFlow&lt;/h1&gt;
&lt;p&gt;Then install TensorFlow for CPU-only machines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(tf_windows)&amp;gt; pip install tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There can be few variants of the &lt;code&gt;tensorflow&lt;/code&gt; package installation. If you need to run &lt;code&gt;pip&lt;/code&gt; behind corporate proxy, add proxy information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(tf_windows)&amp;gt; pip --proxy=&amp;quot;proxy_url:port&amp;quot; install tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you need GPU-enabled version (and your machine supports it)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(tf_windows)&amp;gt; pip install tensorflow-gpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To test if installation was successful, you might want to do check:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf_windows&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;python&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Hello world!&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;if everything was installed correctly, you should see:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="sa"&gt;b&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Hello world!&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On my machine I got warning when starting a new session:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;2019-01-17 07:09:01.477724: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Don't get scared by that, TensorFlow works, the information displayed means that it isn't as fast as it could be.
In order to suppress this you will need to build TensorFlow from sources using appropriate flags (see &lt;a href="https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions?rq=1"&gt;StackOverflow answer&lt;/a&gt;) for compilation otherwise you can ignore it.&lt;/p&gt;
&lt;p&gt;&lt;a id="installing-keras"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="installing-keras"&gt;Installing Keras&lt;/h1&gt;
&lt;p&gt;The way that worked for me was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;(tf_windows)&amp;gt;conda install mingw libpython
(tf_windows)&amp;gt;pip install --upgrade keras
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Using the &lt;code&gt;--upgrade&lt;/code&gt; flag ensures that the latest version of Keras will be installed.&lt;/p&gt;
&lt;p&gt;Perform the test if Keras was installed correctly&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;backend&lt;/span&gt;
&lt;span class="n"&gt;Using&lt;/span&gt; &lt;span class="n"&gt;TensorFlow&lt;/span&gt; &lt;span class="n"&gt;backend&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;backend&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_BACKEND&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tensorflow&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="machine-learning"/><category term="tensorflow"/><category term="howto"/></entry><entry><title>Darwin Approach to Traveling Salesman</title><link href="http://127.0.0.1:8000/darwin-approach-to-traveling-salesman/" rel="alternate"/><published>2019-01-12T00:00:00+01:00</published><updated>2023-07-12T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-01-12:/darwin-approach-to-traveling-salesman/</id><summary type="html">&lt;p&gt;Can the evolutionary approach crash the problem that brute-forcing will last far more than the age of the universe? This post shows how to attack the Traveling Salesman Problem using Darwin's approach. I'm describing the evolution model and design decisions. See the animation of how the population was evolving through the epochs.&lt;/p&gt;</summary><content type="html">&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-story-behind-this-post"&gt;The story behind this post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#traveling-salesman-problem-tsp"&gt;Traveling Salesman Problem (TSP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#initial-note"&gt;Initial note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#solution-outline"&gt;Solution outline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adaptation"&gt;Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-selection"&gt;Natural selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#matching-for-the-reproduction"&gt;Matching for the reproduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#create-children"&gt;Create children&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#crossover"&gt;Crossover&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mutation"&gt;Mutation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolution-in-action"&gt;Evolution in action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#alternative-algorithms"&gt;Alternative algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1--brute-force-algorithm"&gt;1.  Brute Force algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2--dynamic-programming-dp"&gt;2.  Dynamic Programming (DP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3--greedy-algorithm"&gt;3.  Greedy algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4--branch-and-bound"&gt;4.  Branch and Bound&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#5--ant-colony-optimization-aco"&gt;5.  Ant Colony Optimization (ACO)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#6--simulated-annealing-sa"&gt;6.  Simulated Annealing (SA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#7--artificial-neural-network-ann"&gt;7.  Artificial Neural Network (ANN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ideas-for-future-work"&gt;Ideas for future work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#credits"&gt;Credits&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="the-story-behind-this-post"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="the-story-behind-this-post"&gt;The story behind this post&lt;/h2&gt;
&lt;p&gt;During my Christmas break, I found a blog post from Peter Brookes-Smith &lt;a href="https://blog.objectivity.co.uk/darwin-and-the-travelling-salesperson/"&gt;Darwin and The Traveling Salesperson&lt;/a&gt;. He wrote, that he was looking for some project for his holidays. He found the blog post that inspired him to start the project and solve the problem by himself without finishing the article. It was similar to my case. I had genetic algorithms in the back of my head for a long, long time. Now while having some spare time between Christmas and New Year - I decided to get my feet wet on that topic.&lt;/p&gt;
&lt;p&gt;&lt;a id="introduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Are you familiar with the Traveling Salesman Problem (TSP)? It's a classic problem in computer science where a salesman is tasked with visiting a set of cities once and returning to the starting place, with the goal of finding the shortest possible route. Tackling the TSP problem can be very challenging, particularly as the number of cities increases. In this blog post, we will explore how to use genetic algorithms to solve TSP problem. We'll take an in-depth look at the different steps involved in the algorithm, along with the results obtained. We'll also provide an overview of the other algorithms that are used to solve TSP, and the comparison with genetic algorithm. Our goal is to provide an easy-to-follow guide that will help you understand the basics of using genetic algorithms to solve TSP, and to inspire you to try your hand at this fascinating problem.&lt;/p&gt;
&lt;p&gt;&lt;a id="traveling-salesman-problem-tsp"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="traveling-salesman-problem-tsp"&gt;Traveling Salesman Problem (TSP)&lt;/h2&gt;
&lt;p&gt;The problem can be defined like this: having a list of cities and finding the shortest route for a salesman to visit each of them once and come back to the starting place.&lt;/p&gt;
&lt;!-- ![TODO](tsp_animated_gif.gif) --&gt;

&lt;p&gt;For non-trivial cases of a few cities, the computational complexity for brute force search is high. The total number of unique routes ($TNUR$) is:&lt;/p&gt;
&lt;p&gt;$$
\mathrm{TNUR} = \frac{1}{2}(n-1)!
$$&lt;/p&gt;
&lt;p&gt;To help to grasp the order of complexity - when the number of cities grows beyond ~25, the time required to brute force search for an optimal solution would exceed the age of the known universe.&lt;/p&gt;
&lt;!-- ![TODO](tps_time_complexity_vs_age_of_universe.gif) --&gt;

&lt;p&gt;The traveling salesman is a beautiful problem to test various optimization algorithms against it. A big portion of metaheuristic solutions came from the evolution algorithm family.&lt;/p&gt;
&lt;p&gt;&lt;a id="initial-note"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="initial-note"&gt;Initial note&lt;/h2&gt;
&lt;p&gt;This post is the recording of an almost naive attempt to apply the evolution approach to solve the problem. Before writing this post, on purpose, I haven't done solid education on genetic algorithms so the terminology and methods might not correspond to ones established in the field. The positive side of this approach is that fun was not spoiled with existing solutions. This I meant as a fun project, not a conference paper. When the disclaimer was done, let's go to the problem-solving.&lt;/p&gt;
&lt;p&gt;&lt;a id="solution-outline"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="solution-outline"&gt;Solution outline&lt;/h2&gt;
&lt;p&gt;The idea for finding the best route using mechanisms that are available in nature in the evolution process described by Karol Darwin is as follows:
Algorithm initialization:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Draw &lt;strong&gt;initial population&lt;/strong&gt; of solutions - set of random routes (e.g. 500 routes). Then, in the loop run epochs of evolution steps;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adaptation&lt;/strong&gt; - corresponds to minor improvements that an organism is able to implement in order to better fit into the environment;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural selection&lt;/strong&gt; - corresponds to challenges, threads posted by nature that strong organisms are able to handle and survive and weak units are lost;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matching for reproduction&lt;/strong&gt; - match members of the population in pairs to prepare for the reproduction&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create children&lt;/strong&gt; - new organisms are born and their genome (route) consists of parts of routes coming from both parents (crossover mechanism). Add mutation to introduce further variation in the population;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeat the cycle&lt;/strong&gt; - go to step 1, and start a new epoch.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="steps" src="/images/tsp/tsp_steps.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. Steps of evolution algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="adaptation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="adaptation"&gt;Adaptation&lt;/h3&gt;
&lt;p&gt;I decided to include mechanisms for minor, local improvements that in nature, organisms might be able to perform to better fit the environment. The implementation of the adaptation is twofold:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Swap two consecutive cities on the list that represents the route and check if the change brought an improvement - if so, keep the change and check another pair of cities until the end of the list.&lt;/li&gt;
&lt;li&gt;Reverse three elements route segments and similarly, as in the previous method - keep the change if the new, candidate route with the reversed segment is shorter than the version with the non-reversed segment.
Both are implemented "swapper" or "reverser" as moving along the list of cities in the route.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ![TODO](tsp_reverse_two.jpg) --&gt;
&lt;!-- ![TODO](tsp_reverse_three.jpg) --&gt;

&lt;p&gt;&lt;a id="natural-selection"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="natural-selection"&gt;Natural selection&lt;/h3&gt;
&lt;p&gt;Natural selection allows to remove of poor solutions and makes room for new experimental variants of routes that inherit features from the best solutions found so far. The implementation was to sort the solutions in the ascending order and keep only &lt;code&gt;10%&lt;/code&gt; of the best solutions.&lt;/p&gt;
&lt;p&gt;&lt;a id="matching-for-the-reproduction"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="matching-for-the-reproduction"&gt;Matching for the reproduction&lt;/h3&gt;
&lt;p&gt;"Elite" selected in the natural selection step is then used to create pairs of parents to be used in the single act of reproduction. The pairs were drawn randomly with returning, but the random distribution was not uniform. The shortest route of the solution, the higher chance that the solution will be taken as a parent. The probability of selecting the solution was proportional to the inverse of the solution route length i.e. the shorter route - a higher probability to be drawn as a parent. The given organism could be selected as a paired element for multiple reproductions - which should be the case, especially for best-fitted solutions (ones with the shortest route).&lt;/p&gt;
&lt;p&gt;&lt;a id="create-children"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="create-children"&gt;Create children&lt;/h3&gt;
&lt;p&gt;In the process of reproduction always two children were born: one that was the effect of crossover routes from mother and father and the second, based on the same crossover but with mutation added.
When drawing pairs, the organism is assigned to the role of mother or father which is important only for the crossover process.&lt;/p&gt;
&lt;p&gt;&lt;a id="crossover"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="crossover"&gt;Crossover&lt;/h3&gt;
&lt;p&gt;In crossover selected segment of a given length (e.g. 5 cities) of the route from the father is being injected into the mother's route. The starting element of the segment in being found in the mother's route and the mother's route is divided into three parts: before the "anchor" element, "anchor" element, and after the "anchor" element. The "anchor" element is replaced by injection. In the next steps mother's route is cleaned up in order to remove duplicate cities (those that appeared in the injection from the father). The cities from the injection were removed from the before anchor and from the after anchor part.&lt;/p&gt;
&lt;!-- ![TODO](tsp_crossover.jpg) --&gt;

&lt;p&gt;&lt;a id="mutation"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="mutation"&gt;Mutation&lt;/h3&gt;
&lt;p&gt;The children obtained by applying crossover are then duplicated to have a second child that will be additionally mutated. The mutation is implemented as the random swap of two cities in the list.&lt;/p&gt;
&lt;!-- ![TODO](tsp_mutation.jpg) --&gt;

&lt;p&gt;&lt;a id="evolution-in-action"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="evolution-in-action"&gt;Evolution in action&lt;/h2&gt;
&lt;p&gt;The whole setup of the experiment is highly parameterised and gives endless opportunities to experiment with various setups. The results shown in this blog were produced for this parametrization:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# number of cities&lt;/span&gt;
&lt;span class="n"&gt;num_cities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;

&lt;span class="c1"&gt;# size of initial population (number of initial random routes)&lt;/span&gt;
&lt;span class="n"&gt;num_routes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;

&lt;span class="c1"&gt;# fraction of organisms that survives natural selection process&lt;/span&gt;
&lt;span class="n"&gt;survival&lt;/span&gt; &lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the first step of implementation, I was able to measure the effect of local adaptation and the gains achieved with this mechanism. While running a single swap of two elements and single reverse or three elements segment gains on random routes were up to 1%. Graphical representation of the route changes confirms that improvement was small - routes still have a high degree of randomness and are far from optimal.&lt;/p&gt;
&lt;p&gt;In the next step, I brought this adaptation process to an extreme number to check its potential - what maximum improvements are possible here. Of course, achieving stabilization of the results here would be difficult but I verified what will happen when we change a number of two- and three elements reversing cycles from 1x to 100x times. The improvements achieved in this configuration were XX on average.&lt;/p&gt;
&lt;p&gt;The next step involved the full process and proved crossover and mutation to be efficient for improving solution quality i.e. finding as short route as possible.&lt;/p&gt;
&lt;p&gt;&lt;img alt="best tour" src="/images/tsp/tsp_r-tour_animation.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2. Visualization of best route found in each epoch&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="length" src="/images/tsp/tsp_r-len.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3. Length of shortest route found in the given epoch of evolution process&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="conclusion"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The experiment showed me the beauty of the evolutionary approach - directed but random evolution leads to the emergence of the order and optimized solutions (however not optimal solutions).&lt;/li&gt;
&lt;li&gt;The fitness of the solution is stabilizing after ~100 epochs. The population doesn't have sufficient new genomes and variation decreases. With this, the ability to overcome imperfections in the best route found so far drops significantly.&lt;/li&gt;
&lt;li&gt;Being under the impression of the robustness of the evolutionary system I'm eager to try the evolutional approach as an alternative to Reinforcement learning such as described on &lt;a href="https://blog.openai.com/evolution-strategies/"&gt;openai blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Technical one: even such a side project playing with numerical problems benefits from accompanying tests. Many annoying bugs that appeared during the implementation (mainly from a global range of variables), and covering (at least partially) code with tests helped out to progress.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a id="alternative-algorithms"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="alternative-algorithms"&gt;Alternative algorithms&lt;/h2&gt;
&lt;p&gt;There are several other algorithms that can be used to solve the Traveling Salesman Problem (TSP):&lt;/p&gt;
&lt;p&gt;&lt;a id="1-brute-force-algorithm"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="1-brute-force-algorithm"&gt;1.  Brute Force algorithm&lt;/h3&gt;
&lt;p&gt;It is the most simple algorithm for solving TSP problem, where it checks all possible routes one by one and select the one with the minimum distance. However, the time complexity is high, and it is not efficient for a large number of cities.&lt;/p&gt;
&lt;p&gt;&lt;a id="2-dynamic-programming-dp"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="2-dynamic-programming-dp"&gt;2.  Dynamic Programming (DP)&lt;/h3&gt;
&lt;p&gt;It is a method of solving problems by breaking them down into smaller overlapping sub-problems and solving each of them independently. DP can be used to solve TSP by building a table of all possible sub-tours, and the optimal solution is constructed using the entries in the table.&lt;/p&gt;
&lt;p&gt;&lt;a id="3-greedy-algorithm"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="3-greedy-algorithm"&gt;3.  Greedy algorithm&lt;/h3&gt;
&lt;p&gt;It is a simple, intuitive and efficient algorithm. Greedy algorithm solves TSP problem by building a solution incrementally, choosing the next city to add to the tour based on the nearest neighbor criterion.&lt;/p&gt;
&lt;p&gt;&lt;a id="4-branch-and-bound"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="4-branch-and-bound"&gt;4.  Branch and Bound&lt;/h3&gt;
&lt;p&gt;It is a method that allows us to explore all possible solutions while avoiding the ones that are obviously suboptimal. Branch and Bound uses the concept of lower bound and upper bound to prune the branches that cannot give a better solution than the best solution found so far.&lt;/p&gt;
&lt;p&gt;&lt;a id="5-ant-colony-optimization-aco"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="5-ant-colony-optimization-aco"&gt;5.  Ant Colony Optimization (ACO)&lt;/h3&gt;
&lt;p&gt;It is a swarm intelligence-based algorithm. The ACO algorithm simulates the behavior of ants searching for the shortest path from their colony to a food source. The ants leave pheromones on the path they travel, and the path with the highest concentration of pheromones is likely to be the shortest path.&lt;/p&gt;
&lt;p&gt;&lt;a id="6-simulated-annealing-sa"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="6-simulated-annealing-sa"&gt;6.  Simulated Annealing (SA)&lt;/h3&gt;
&lt;p&gt;It is a probabilistic technique for approximating the global optimum of a given function. SA algorithm starts with a random solution and then repeatedly makes small changes to the solution in the hope of finding a better solution.&lt;/p&gt;
&lt;p&gt;&lt;a id="7-artificial-neural-network-ann"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="7-artificial-neural-network-ann"&gt;7.  Artificial Neural Network (ANN)&lt;/h3&gt;
&lt;p&gt;it is a machine learning algorithm that can be used to model the TSP problem and find an approximate solution. ANN algorithm learns from historical data, and&lt;/p&gt;
&lt;p&gt;There is website with multiple algorithms and animated solutions of TSP problem: &lt;a href="https://stemlounge.com/animated-algorithms-for-the-traveling-salesman-problem/"&gt;11 Animated Algorithms for the Traveling Salesman Problem&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="ideas-for-future-work"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="ideas-for-future-work"&gt;Ideas for future work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Deploy it in the server-less mode of operation. Route calculations and transformations are great candidates for "lambda" functions.&lt;/li&gt;
&lt;li&gt;Use GPU to compute things in parallel&lt;/li&gt;
&lt;li&gt;Overcome stagnation of evolution by evolving several populations independently and allowing for contact between populations from time to time - transfer some solutions from one to another population.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="credits"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="credits"&gt;Credits&lt;/h2&gt;
&lt;p&gt;&lt;a id="cover-image-from-httpscommonswikimediaorgwikifilehigh_school_biology_1-13pdf-by-ck-12-foundation-on-cc-license"&gt;&lt;/a&gt;
Cover image from &lt;a href="https://commons.wikimedia.org/wiki/File:High_School_Biology_1-13.pdf"&gt;https://commons.wikimedia.org/wiki/File:High_School_Biology_1-13.pdf&lt;/a&gt; by CK-12 Foundation on CC license&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Exploring new ideas"/><category term="machine-learning"/><category term="evolutionary"/><category term="tsp"/><category term="genetic-algorithms"/><category term="optimization"/><category term="optimization-techniques"/></entry><entry><title>How to Organize Data Science Project Based on Jupyter Notebook</title><link href="http://127.0.0.1:8000/how-to-organize-data-science-project-based-on-jupyter-notebook/" rel="alternate"/><published>2019-01-05T00:00:00+01:00</published><updated>2023-01-19T00:00:00+01:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2019-01-05:/how-to-organize-data-science-project-based-on-jupyter-notebook/</id><summary type="html">&lt;p&gt;Having several notebook-based projects behind you might result in a mess in the projects directory. Organize your Data Science project based on Jupyter notebooks in a way that one can navigate through it. Especially that "the one" will be most probably you in a few months time. To achieve that, keep your projects directory clean, name the project in a descriptive way and take care of the internal structure of the project.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this blog post, I will share the way to organize your Data Science project based on Jupyter notebooks in a way that one can navigate through it. Especially that "the one" will be most probably you in a few months' time. So be nice to yourselves and organize your project in a rational structure rather than spontaneous chaos.&lt;/p&gt;
&lt;!-- MarkdownTOC autolink="true" autoanchor="true" --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#problem-with-finding-valuable-work-you-did-in-the-jupyter-notebook-in-the-past"&gt;Problem with finding valuable work you did in the Jupyter notebook in the past&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rules-i-try-to-follow"&gt;Rules I try to follow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-version-control"&gt;Using version control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#specific-aspects-for-different-use-cases"&gt;Specific aspects for different use cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#additional-tips"&gt;Additional Tips&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;

&lt;p&gt;&lt;a id="problem-with-finding-valuable-work-you-did-in-the-jupyter-notebook-in-the-past"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="problem-with-finding-valuable-work-you-did-in-the-jupyter-notebook-in-the-past"&gt;Problem with finding valuable work you did in the Jupyter notebook in the past&lt;/h2&gt;
&lt;p&gt;I had a few problems like that. In my projects, I used to find multiple untitled notebooks such as &lt;code&gt;Untitled1.ipynb&lt;/code&gt;, &lt;code&gt;Untitled2.ipynb&lt;/code&gt;, &lt;code&gt;Untitled3.ipynb&lt;/code&gt; - that requires extra effort to check out what is inside. Similarly, with data: shall this be in &lt;code&gt;~/datasets&lt;/code&gt; or &lt;code&gt;~/projects/my_project/datasets&lt;/code&gt;. The need for standardization emerged after creating like dozen of notebook-based projects. Here are lessons learned and solutions worked out for my use cases.&lt;/p&gt;
&lt;p&gt;&lt;a id="rules-i-try-to-follow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="rules-i-try-to-follow"&gt;Rules I try to follow&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Rule #1: Keep your projects' directory clean&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This means not to put notebooks directly in your &lt;code&gt;projects&lt;/code&gt;, &lt;code&gt;src&lt;/code&gt; directory (or as you call it) but rather keep each project in a separate directory in order to have a uniform structure and, at the projects view level - only one item (directory) per project. This also helps when using version control on the project level.
Having this primitive in place, you should be able to get back to valuable pieces of code you have done in the past and adopt or reuse them whenever needed. Just don't mix raw data, processed data, images, and notebooks in one top-level directory, and you are 80% done.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rule #2: Name the project in a descriptive way&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use the names in a way that you will be sure which project it is. In the case of my side projects, I use different names for different incarnations of the project - when I start attacking the same problem from a different angle and starting basically from the scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rule #3: Take care of the internal structure of the project&lt;/strong&gt;
There might be a simple project that fits into one notebook and does not require any external files, but if you are doing something more complex e.g. using files with data and preprocessing the data - having well-thought structure of the project might pay off. Please note that the internal structure of the project depends on the use case. For my practice, I distinguish three use cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;End-to-end individual work&lt;/strong&gt;
This is a typical case of my side projects - if it is interesting enough it might evolve into one of the subsequent categories.
Example: A data scientist working on a personal project to analyze customer data from a retail store. This project would likely be short-term, have low complexity, and could be completed using a single Jupyter notebook.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collaborative project&lt;/strong&gt;
This can be work done for the client and deliverable could be further developed collaboratively to finally land as a product feature in production.
Example: A team of data scientists working on a project for a client to predict customer churn. This project would likely be of medium complexity, involve multiple Jupyter notebooks, and require collaboration among team members. The project may also require extracting some mature code into a python module that is imported into a notebook.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Individual work but final notebook shares as result&lt;/strong&gt;
This can be the analysis on-demand where a notebook report is a deliverable or think of a side-project/tutorial that you would like to publish on the blog.
Example: A data scientist working on an analysis on demand for a company's management team. The analysis would be done in a Jupyter notebook, and the final report would be shared with the management team.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Production-ready projects&lt;/strong&gt;
Example: A data scientist working on a project to build a recommendation engine for a streaming service. This project would likely be of high complexity, involve multiple Jupyter notebooks, and require extracting code into a python module that is imported into a notebook. Additionally, the final model would need to be deployed in a production environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Research projects&lt;/strong&gt;
Example: A data scientist working on a project to develop a new machine learning algorithm. This project would likely involve a lot of experimentation and iteration, and the final results would be shared in a research paper or conference presentation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on the use case project directory structure, data directory structure, and notebook directory structure might differ but from my experience, for small to medium Data Science projects a structure similar to this should work well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;my_project_name/
  ├── 1-preprocessing.ipynb
  ├── 2-analysis.ipynb
  ├── data
  │   ├── interim
  │   ├── processed
  │   └── raw
  └── reports
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;a id="using-version-control"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="using-version-control"&gt;Using version control&lt;/h2&gt;
&lt;p&gt;The use of version control tools like Git to manage and track changes in the project over time is a standard in software projects. However, data science project has its own specificity related to the fact that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data is a part of the project&lt;/li&gt;
&lt;li&gt;Jupyter notebooks are typically stored as in JSON format and can contain output that can cause problems in reviewing the changes in the notebook.
Using version control in the Data Science project is a topic for a separate article.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a id="specific-aspects-for-different-use-cases"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="specific-aspects-for-different-use-cases"&gt;Specific aspects for different use cases&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;End-to-end individual work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;short-time activity&lt;/li&gt;
&lt;li&gt;low complexity&lt;/li&gt;
&lt;li&gt;consist of a small number of notebooks (fits into a single notebook)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For longer/bigger projects consider using guidelines from Collaborative Project&lt;/p&gt;
&lt;p&gt;If not using version control or not doing small (atomic) commits consider adding a changelog cell to track the evolution of the notebook.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Collaborative project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider extracting some mature code into a python module that is imported into a notebook and take some effort to clean up and test the code. Take advantage of using modern IDEs such as PyCharm.&lt;/p&gt;
&lt;p&gt;Using version control is also highly recommended even if tracing differences in JSON files (format of Jupyter notebooks) is less convenient when compared to plain source code you will have a record of previous versions of the notebook.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Individual work but final notebook shares as a result&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Take care of reproducibility - either for yourself if the updates to the outcome document will be needed or for someone else that would like to reproduce your results e.g. when following the tutorial you created.&lt;/p&gt;
&lt;p&gt;For a more complex project setup take look at &lt;a href="https://drivendata.github.io/cookiecutter-data-science"&gt;Cookiecutter Data Science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="additional-tips"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="additional-tips"&gt;Additional Tips&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Notebook metadata&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is useful to include some metadata in the notebook. Here is an example borrowed from &lt;a href="http://pbpython.com/notebook-process.html"&gt;pbpython&lt;/a&gt;:
For that we will need to import &lt;code&gt;Path&lt;/code&gt; from &lt;code&gt;pathlib&lt;/code&gt; and &lt;code&gt;datetime&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Exemplary meta information on time (might indicate last modification) and files used might look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;today&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;sales_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;raw&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Sales-History.csv&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;pipeline_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;raw&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;pipeline_data.xlsx&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;summary_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;summary_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;%b-%d-%Y&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;.pkl&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Use cookiecutter to standardize your projects&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To bootstrap the software projects from a template one can use cookiecutter. You can create your template by following short instructions &lt;a href=""&gt;&lt;/a&gt;. Here is an exception from the documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Cookiecutter takes a source directory tree and copies it into your new project. It replaces all the names that it finds surrounded by templating tags &lt;code&gt;{{&lt;/code&gt; and &lt;code&gt;}}&lt;/code&gt; with names that it finds in the file &lt;code&gt;cookiecutter.json&lt;/code&gt;. That’s basically it.&lt;/p&gt;
&lt;p&gt;The replaced names can be file names, directory names, and strings inside files.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;X::[[2023-02-13-how_to_organize_datascience_project]]
X::[[competitive_data_science_project]]
X::[[MOC_Python_Project]]&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[[2023-01-19-common_types_of_data_science_projects|Common Types of Data Science Projects]]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Howto"/><category term="jupyter"/><category term="python"/><category term="notebook"/><category term="howto"/><category term="machine-learning"/><category term="best-practices"/><category term="project-management"/><category term="data-science"/></entry><entry><title>Top Popular ZSH Plugins on GitHub (2018)</title><link href="http://127.0.0.1:8000/top-popular-zsh-plugins-on-github/" rel="alternate"/><published>2018-03-22T00:00:00+01:00</published><updated>2022-09-01T00:00:00+02:00</updated><author><name>Krystian Safjan</name></author><id>tag:127.0.0.1,2018-03-22:/top-popular-zsh-plugins-on-github/</id><summary type="html">&lt;p&gt;There is an exhaustive but curated list of Zsh plugins posted on GitHub project Awesome Zsh plugins. You can find there 800+ links to plugins, themes and Zsh plugin managers/frameworks. Even though it is a collection of awesome stuff the number is a bit high get orientation which plugins gained already good reputation from Zsh users community. In this post I will identify most popular plugins - those which have the highest number of stars.&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This article was written in 2018, for a more recent version of the ranking see: &lt;a href="../top-popular-zsh-plugins-on-github-2023/"&gt;2023&lt;/a&gt;.
The older articles have description of the selected, interesting tools that are not in this article - so you might also to visit the older editions of the survey.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On the GitHub project &lt;a href="https://github.com/unixorn/awesome-zsh-plugins"&gt;Awesome Zsh plugins&lt;/a&gt; you can find 800+ links to plugins, themes, and Zsh plugin managers/frameworks. Even though it is a collection of awesome stuff the number is a bit high get orientation which plugins gained already good reputation from the Zsh users community. This post aims at identifying the most popular plugins where popularity is measured by the number of stars that GitHub users added to the given plugin. For clarity, I will focus here only on plugins excluding frameworks and themes that are also listed on "Awesome Zsh plugins" website.&lt;/p&gt;
&lt;h1 id="what-are-github-stars"&gt;What are Github stars?&lt;/h1&gt;
&lt;p&gt;Stars is the way how users can 'bookmark' projects - this can serve as an indication for others that a project successfully grabbed someone's attention. The &lt;code&gt;stargazers&lt;/code&gt; statistics are available via &lt;a href="https://developer.github.com/v4/"&gt;GithubAPI&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="top-20-most-popular-plugins-as-of-march-2018"&gt;Top 20 most popular plugins as of March 2018&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;link&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;stars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/wting/autojump"&gt;autojump&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A cd command that learns - easily navigate directories from the command line. Install autojump-zsh for best results.&lt;/td&gt;
&lt;td&gt;6524&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-syntax-highlighting"&gt;syntax-highlighting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Add syntax highlighting to your Zsh. Make sure you load this &lt;em&gt;before&lt;/em&gt; zsh-users/zsh-history-substring-search or they will both break.&lt;/td&gt;
&lt;td&gt;4851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-autosuggestions"&gt;autosuggestions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Fish-like fast/unobtrusive autosuggestions for Zsh.&lt;/td&gt;
&lt;td&gt;4836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/StackExchange/blackbox"&gt;blackbox&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stack Exchange's toolkit for storing keys/credentials securely in a git repository.&lt;/td&gt;
&lt;td&gt;4098&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/bobthecow/git-flow-completion"&gt;git-flow-completion&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zsh completion support for git-flow.&lt;/td&gt;
&lt;td&gt;2031&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-completions"&gt;zsh-completions&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A collection of extra completions for Zsh.&lt;/td&gt;
&lt;td&gt;2003&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/fcambus/ansiweather"&gt;ansiweather&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Weather in your terminal, with ANSI colors and Unicode symbols.&lt;/td&gt;
&lt;td&gt;1342&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/rimraf/k"&gt;k&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Directory listings for Zsh with git features.&lt;/td&gt;
&lt;td&gt;898&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/b4b4r07/enhancd"&gt;enhancd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A simple tool that provides enhanced &lt;code&gt;cd&lt;/code&gt; command.&lt;/td&gt;
&lt;td&gt;826&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/sobolevn/git-secret"&gt;git-secret&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A bash tool to store your private data inside a git repository.&lt;/td&gt;
&lt;td&gt;823&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zsh-history-substring-search"&gt;history-substring-search&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Needs to be loaded after zsh-syntax-highlighting, or they'll both break. You'll also need to bind keys to its functions, details are in the README.md.&lt;/td&gt;
&lt;td&gt;704&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/skx/sysadmin-util"&gt;sysadmin-util&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Steve Kemp's collection of tool scripts for sysadmins.&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/larkery/zsh-histdb"&gt;histdb&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stores your history in an SQLite database.&lt;/td&gt;
&lt;td&gt;426&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/lukechilds/zsh-nvm"&gt;nvm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zsh plugin for installing, updating, and loading nvm.&lt;/td&gt;
&lt;td&gt;421&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/zsh-users/zaw"&gt;zaw&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Zsh anything.el-like widget.&lt;/td&gt;
&lt;td&gt;402&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/eriwen/gradle-completion"&gt;gradle-completion&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Bash and Zsh completion support for gradle.&lt;/td&gt;
&lt;td&gt;362&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Below you can find a short description of the 10 most popular plugins&lt;/p&gt;
&lt;h2 id="1-autojump"&gt;1. &lt;a href="https://github.com/wting/autojump"&gt;autojump&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;autojump is a faster way to navigate your filesystem. It works by maintaining a database of the directories you use the most from the command line. Directories must be visited first before they can be jumped to. It can be an alternative for &lt;a href="https://github.com/rupa/z"&gt;z&lt;/a&gt; or &lt;a href="https://github.com/clvv/fasd"&gt;fasd&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="2-syntax-highlighting"&gt;2. &lt;a href="https://github.com/zsh-users/zsh-syntax-highlighting"&gt;syntax-highlighting&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This package provides Fish shell-like syntax highlighting for the Zsh. It enables highlighting of commands whilst they are typed at a Zsh prompt into an interactive terminal. This helps in reviewing commands before running them, particularly in catching syntax errors.
Some examples:&lt;/p&gt;
&lt;p&gt;Before: &lt;img alt="Screenshot #1.1" src="https://raw.githubusercontent.com/zsh-users/zsh-syntax-highlighting/master/images/before1-smaller.png"&gt;
&lt;br&gt;
After: &lt;img alt="Screenshot #1.2" src="https://raw.githubusercontent.com/zsh-users/zsh-syntax-highlighting/master/images/after1-smaller.png"&gt;&lt;/p&gt;
&lt;p&gt;Before: &lt;img alt="Screenshot #3.1" src="https://raw.githubusercontent.com/zsh-users/zsh-syntax-highlighting/master/images/before3-smaller.png"&gt;
&lt;br&gt;
After: &lt;img alt="Screenshot #3.2" src="https://raw.githubusercontent.com/zsh-users/zsh-syntax-highlighting/master/images/after3-smaller.png"&gt;&lt;/p&gt;
&lt;h2 id="3-autosuggestions"&gt;3. &lt;a href="https://github.com/zsh-users/zsh-autosuggestions"&gt;autosuggestions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This package provides Fish-like fast/unobtrusive autosuggestions for Zsh. It suggests commands as you type, based on command history.
Example:
&lt;a href="https://asciinema.org/a/37390" target="_blank"&gt;&lt;img src="https://asciinema.org/a/37390.png" width="400" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="4-blackbox"&gt;4. &lt;a href="https://github.com/StackExchange/blackbox"&gt;blackbox&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This package allows to safely store secrets in a VCS repo (i.e. Git, Mercurial, Subversion, or Perforce). These commands make it easy for you to Gnu Privacy Guard (GPG) to encrypt specific files in a repo, so they are "encrypted at rest" in your repository. However, the scripts make it easy to decrypt them when you need to view or edit them and decrypt them for use in production. Originally written for Puppet, BlackBox now works with any Git or Mercurial repository.&lt;/p&gt;
&lt;h2 id="5-git-flow-completion"&gt;5. &lt;a href="https://github.com/bobthecow/git-flow-completion"&gt;git-flow-completion&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Bash, Zsh, and Fish completion support for &lt;a href="http://github.com/nvie/gitflow"&gt;git-flow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This package provides support for completing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git-flow init and version&lt;/li&gt;
&lt;li&gt;feature, hotfix, and release branches&lt;/li&gt;
&lt;li&gt;remote feature, hotfix, and release branch names&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="6-zsh-completions"&gt;6. &lt;a href="https://github.com/zsh-users/zsh-completions"&gt;zsh-completions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This project aims at gathering/develop new completion scripts that are not available in Zsh yet.
Demo:&lt;/p&gt;
&lt;p&gt;&lt;img alt="zsh-completions-demo" src="https://raw.githubusercontent.com/supercrabtree/k/gh-pages/inside-work-tree.jpg"&gt;&lt;/p&gt;
&lt;h2 id="7-ansiweather"&gt;7. &lt;a href="https://github.com/fcambus/ansiweather"&gt;ansiweather&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;AnsiWeather is a Shell script for displaying the current weather conditions
in your terminal, with support for ANSI colors and Unicode symbols.&lt;/p&gt;
&lt;p&gt;&lt;img alt="AnsiWeather Screenshot" src="https://camo.githubusercontent.com/3e34f3781bf3109aa757104c8fa39c8e3ee95675/68747470733a2f2f7777772e63616d6275732e6e65742f66696c65732f616e7369776561746865722f616e7369776561746865722e706e67"&gt;&lt;/p&gt;
&lt;p&gt;Weather data comes from the &lt;code&gt;OpenWeatherMap&lt;/code&gt; free weather API.&lt;/p&gt;
&lt;h2 id="8-k"&gt;8. &lt;a href="https://github.com/rimraf/k"&gt;k&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;k is a Zsh script / plugin to make directory listings more readable, adding a bit of color and some git status information on files and directories.
&lt;img alt="k" src="https://raw.githubusercontent.com/supercrabtree/k/gh-pages/inside-work-tree.jpg"&gt;&lt;/p&gt;
&lt;h2 id="9-enhancd"&gt;9. &lt;a href="https://github.com/b4b4r07/enhancd"&gt;enhancd&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The new &lt;code&gt;cd&lt;/code&gt; command called "enhancd" enhanced the flexibility and usability for a user. enhancd will memorize all directories visited by a user and use it for the pathname resolution. If the log of enhancd has more than one directory path with the same name, enhancd will pass the candidate directories list to the filter within the &lt;code&gt;ENHANCD_FILTER&lt;/code&gt; environment variable in order to narrow it down to one directory.&lt;/p&gt;
&lt;p&gt;Thanks to this mechanism, the users can intuitively and easily change the directory they want to go to.&lt;/p&gt;
&lt;p&gt;&lt;img alt="enhancd-demo" src="https://raw.githubusercontent.com/b4b4r07/screenshots/master/enhancd/demo.gif"&gt;&lt;/p&gt;
&lt;h2 id="10-git-secret"&gt;10. &lt;a href="https://github.com/sobolevn/git-secret"&gt;git-secret&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;git-secret&lt;/code&gt; is a bash tool to store your private data inside a git repo. It encrypts data, using &lt;code&gt;gpg&lt;/code&gt;, the tracked files with the public keys of all the users that you trust. So every one of them can decrypt these files using only their personal secret key. Usage of private-public makes it easier for everyone to manage access rights. There are no passwords that change. When someone is out - just delete their public key, re-encrypt the files, and they won’t be able to decrypt secrets anymore.
&lt;a href="https://asciinema.org/a/41811?autoplay=1"&gt;&lt;img alt="git-secret terminal preview" src="https://asciinema.org/a/41811.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On the list there are 300+ projects that are falling into the scope of this research, you can get the complete dataset by downloading this CSV file: &lt;a href="/downloads/180322_results_sorted.csv"&gt;CSV&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Any comments or suggestions? &lt;a href="mailto:ksafjan@gmail.com?subject=Blog+post"&gt;Let me know&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</content><category term="Linux"/><category term="zsh"/><category term="scrapping"/><category term="python"/><category term="Linux"/></entry></feed>