
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="http://127.0.0.1:8000/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="http://127.0.0.1:8000/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="http://127.0.0.1:8000/styles/custom.css">








    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. It involves transforming a dataset into a new coordinate system that consists of principal components, which are linear combinations of the original features. PCA is useful for reducing the number …"/>
    <meta name="keywords" content="data-preprocessing, pca, dimensionality-reduction, python, interpretability, categorical-variables, multicollinearity, imbalanced-data">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="Checks and Data Preprocessing Steps Before Applying PCA"/>
  <meta property="og:description" content="Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. It involves transforming a dataset into a new coordinate system that consists of principal components, which are linear combinations of the original features. PCA is useful for reducing the number …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="http://127.0.0.1:8000/before-pca/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2023-02-20 00:00:00+01:00"/>
  <meta property="article:modified_time" content="2023-07-12 00:00:00+02:00"/>
  <meta property="article:author" content="http://127.0.0.1:8000/author/krystian-safjan/"/>
  <meta property="article:section" content="note"/>
  <meta property="article:tag" content="data-preprocessing"/>
  <meta property="article:tag" content="pca"/>
  <meta property="article:tag" content="dimensionality-reduction"/>
  <meta property="article:tag" content="python"/>
  <meta property="article:tag" content="interpretability"/>
  <meta property="article:tag" content="categorical-variables"/>
  <meta property="article:tag" content="multicollinearity"/>
  <meta property="article:tag" content="imbalanced-data"/>
  <meta property="og:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:image:alt" content="Krystian Safjan's Blog"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="Checks and Data Preprocessing Steps Before Applying PCA"/>
    <meta name="twitter:description" content="Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. It involves transforming a dataset into a new coordinate system that consists of principal..."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://127.0.0.1:8000/before-pca/"
  },
  "headline": "Checks and Data Preprocessing Steps Before Applying PCA",
  "datePublished": "2023-02-20T00:00:00+01:00",
  "dateModified": "2023-07-12T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "http://127.0.0.1:8000/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "/images/profile_new.jpg",  "url": "http://127.0.0.1:8000/before-pca/",
  "description": "Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. It involves transforming a dataset into a new coordinate system..."
}
</script>

    <title>    Checks and Data Preprocessing Steps Before Applying PCA
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="http://127.0.0.1:8000/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="http://127.0.0.1:8000/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist | Researcher | Team Leader<br><br> working at Ernst &amp; Young and writing about <a href="/category/machine-learning.html">Data Science and Visualization</a>, on <a href="/category/machine-learning.html">Machine Learning, Deep Learning</a> and <a href="/tag/nlp/">NLP</a>. There are also some <a href="/category/howto.html">howto</a> posts on tools and workflows.</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A • PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav>
            <a href="http://127.0.0.1:8000/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>



            <div id="search" class="nav-search"></div>
            <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2023-02-20 


                    <span class="share-icons">
                        <a href="https://twitter.com/intent/tweet?text=Checks%20and%20Data%20Preprocessing%20Steps%20Before%20Applying%20PCA&url=http%3A//127.0.0.1%3A8000/before-pca/&via=izikeros&hashtags=data-preprocessing,pca,dimensionality-reduction,python,interpretability,categorical-variables,multicollinearity,imbalanced-data" target="_blank" title="Share on Twitter" class="share-icon"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A//127.0.0.1%3A8000/before-pca/" target="_blank" title="Share on Facebook" class="share-icon"><i class="fab fa-facebook"></i></a>
                        <a href="https://news.ycombinator.com/submitlink?t=Checks%20and%20Data%20Preprocessing%20Steps%20Before%20Applying%20PCA&u=http%3A//127.0.0.1%3A8000/before-pca/" target="_blank" title="Share on Hacker News" class="share-icon"><i class="fab fa-hacker-news"></i></a>
                        <a href="https://www.reddit.com/submit?url=http%3A//127.0.0.1%3A8000/before-pca/&title=Checks%20and%20Data%20Preprocessing%20Steps%20Before%20Applying%20PCA" target="_blank" title="Share on Reddit" class="share-icon"><i class="fab fa-reddit"></i></a>
                    </span>
                <br/>
            </p>
            <h1 id="before-pca">Checks and Data Preprocessing Steps Before Applying PCA</h1>
            <div class="header-underline"></div>



        </header>



        <details class="toc-details" id="toc-container">
            <summary>Table of Contents</summary>
            <nav class="toc" aria-label="Table of Contents">
                <ul class="toc-list"></ul>
            </nav>
        </details>

        <div class="article-content">
            <p>Principal Component Analysis (PCA) is a popular technique used for dimensionality reduction. It involves transforming a dataset into a new coordinate system that consists of principal components, which are linear combinations of the original features. PCA is useful for reducing the number of features in a dataset, while still retaining most of the information.</p>
<!-- MarkdownTOC levels="2,3" autolink="true" autoanchor="true" -->

<ul>
<li><a href="#preparations-to-pca">Preparations to PCA</a></li>
<li><a href="#data-cleaning">Data Cleaning</a></li>
<li><a href="#data-standardization">Data Standardization</a></li>
<li><a href="#feature-selection">Feature Selection</a></li>
<li><a href="#check-for-linearity">Check for Linearity</a></li>
<li><a href="#determine-the-number-of-components">Determine the number of components</a></li>
<li><a href="#interpret-the-components">Interpret the components</a></li>
<li><a href="#consider-the-trade-off-between-dimensionality-reduction-and-interpretability">Consider the trade-off between dimensionality reduction and interpretability</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#extra-steps">Extra steps</a></li>
<li><a href="#handle-categorical-variables">Handle categorical variables</a></li>
<li><a href="#address-multicollinearity">Address multicollinearity</a></li>
<li><a href="#handle-imbalanced-data">Handle imbalanced data</a></li>
</ul>
<!-- /MarkdownTOC -->

<p><a id="preparations-to-pca"></a></p>
<h2 id="preparations-to-pca">Preparations to PCA</h2>
<p>Before applying PCA to a dataset, it's important to consider the following steps:</p>
<p><a id="data-cleaning"></a></p>
<h3 id="data-cleaning">Data Cleaning</h3>
<p>Before any analysis, we need to check the dataset for missing values, outliers, and any other data quality issues. These issues can affect the accuracy of the PCA model. If we have missing values, we can either remove the corresponding records or impute the missing values. If we have outliers, we may want to remove them or transform them to reduce their impact on the analysis.</p>
<p><a id="data-standardization"></a></p>
<h3 id="data-standardization">Data Standardization</h3>
<p>PCA is sensitive to the scale of the data, so we need to standardize the data before applying PCA. Standardization involves scaling the data so that each feature has a mean of 0 and a standard deviation of 1. This step is important because PCA will give more importance to features with larger variances.</p>
<p><a id="feature-selection"></a></p>
<h3 id="feature-selection">Feature Selection</h3>
<p>Before applying PCA, it's a good idea to perform feature selection to remove any irrelevant or redundant features. This can help to improve the performance of the PCA model and reduce the computational complexity. Feature selection can be performed using techniques such as correlation analysis, feature importance, or model-based selection.</p>
<p><a id="check-for-linearity"></a></p>
<h3 id="check-for-linearity">Check for Linearity</h3>
<p>PCA assumes that the relationships between the features are linear. It's important to check for linearity before applying PCA, as non-linear relationships can lead to inaccurate results. We can check for linearity using scatter plots or other visualization techniques.</p>
<p>Once we have completed these steps, we can apply PCA to the pre-processed data. Here's an example of how to perform PCA using the scikit-learn library in Python:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Load the dataset</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>

<span class="c1"># Data standardization</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Perform PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_std</span><span class="p">)</span>

<span class="c1"># Visualize the results</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal Component 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Principal Component 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p>In this example, we load the iris dataset, standardize the data, perform PCA, and visualize the results. We can see that the first two principal components capture most of the variation in the data.</p>
<p>Before applying PCA to a dataset for dimensionality reduction, we need to perform data cleaning, data standardization, feature selection, and check for linearity. These steps can help to improve the accuracy of the PCA model and ensure that we are getting the most out of the data.</p>
<p><a id="determine-the-number-of-components"></a></p>
<h3 id="determine-the-number-of-components">Determine the number of components</h3>
<p>Before applying PCA, it's important to decide on the number of principal components to retain. This depends on the amount of variance we want to preserve in the data. A common approach is to choose the number of components that explain a certain percentage of the total variance, such as 95% or 99%. We can use the <code>explained_variance_ratio_</code> attribute of the PCA model to determine the proportion of variance explained by each component.</p>
<p><a id="interpret-the-components"></a></p>
<h3 id="interpret-the-components">Interpret the components</h3>
<p>After applying PCA, it's important to interpret the principal components to understand the relationships between the original features. Each principal component is a linear combination of the original features, so we can examine the loadings of each component to determine which features are most strongly associated with that component. We can use the <code>components_</code> attribute of the PCA model to access the loadings.</p>
<p><a id="consider-the-trade-off-between-dimensionality-reduction-and-interpretability"></a></p>
<h3 id="consider-the-trade-off-between-dimensionality-reduction-and-interpretability">Consider the trade-off between dimensionality reduction and interpretability</h3>
<p>While PCA can be a useful tool for reducing the dimensionality of a dataset, it's important to consider the trade-off between dimensionality reduction and interpretability. When we reduce the number of features, we may lose some of the original information and make it more difficult to interpret the results. We can use domain knowledge and visualization techniques to help us interpret the results.</p>
<p>Here's an example of how to interpret the principal components using the iris dataset:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Interpret the components</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">component</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PC</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="n">coef</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">feature</span><span class="p">,</span><span class="w"> </span><span class="n">coef</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">zip</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span><span class="w"> </span><span class="n">component</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This code prints out the loadings for each principal component. We can see that the first principal component is strongly associated with sepal length and petal length, while the second principal component is strongly associated with petal width.</p>
<p><a id="conclusion"></a></p>
<h2 id="conclusion">Conclusion</h2>
<p>Before applying PCA for dimensionality reduction, we need to determine the number of components to retain, interpret the principal components, and consider the trade-off between dimensionality reduction and interpretability. PCA can be a powerful tool for reducing the dimensionality of a dataset, but it's important to use it judiciously and interpret the results carefully.</p>
<p><a id="extra-steps"></a></p>
<h2 id="extra-steps">Extra steps</h2>
<p>Here are a few more pre-processing steps that can be useful before applying PCA.</p>
<p><a id="handle-categorical-variables"></a></p>
<h3 id="handle-categorical-variables">Handle categorical variables</h3>
<p>PCA is designed to work with continuous numerical data, so if our dataset contains categorical variables, we need to convert them to numerical values before applying PCA. This can be done using techniques such as one-hot encoding or label encoding. One-hot encoding creates a binary variable for each category, while label encoding assigns a numerical value to each category.</p>
<p><a id="address-multicollinearity"></a></p>
<h3 id="address-multicollinearity">Address multicollinearity</h3>
<p>If our dataset contains features that are highly correlated with each other, this can lead to multicollinearity, which can affect the accuracy of the PCA model. We can check for multicollinearity using techniques such as correlation analysis or variance inflation factors (VIF). If we find that our dataset has high multicollinearity, we may want to consider removing some of the correlated features.</p>
<p><a id="handle-imbalanced-data"></a></p>
<h3 id="handle-imbalanced-data">Handle imbalanced data</h3>
<p>If our dataset is imbalanced, with some classes having many more observations than others, this can affect the accuracy of the PCA model. We can use techniques such as oversampling or undersampling to balance the classes before applying PCA.</p>
<p>Here's an example of how to handle categorical variables using one-hot encoding:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Load the dataset</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">)</span>

<span class="c1"># One-hot encoding</span>
<span class="n">data_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;category&#39;</span><span class="p">])</span>
</code></pre></div>

<p>In this example, we use the <code>get_dummies()</code> function from pandas to convert the categorical variable 'category' into binary variables.</p>
        </div>


        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="http://127.0.0.1:8000/tag/data-preprocessing/" class="article-tag">data-preprocessing</a>
                <a href="http://127.0.0.1:8000/tag/pca/" class="article-tag">pca</a>
                <a href="http://127.0.0.1:8000/tag/dimensionality-reduction/" class="article-tag">dimensionality-reduction</a>
                <a href="http://127.0.0.1:8000/tag/python/" class="article-tag">python</a>
                <a href="http://127.0.0.1:8000/tag/interpretability/" class="article-tag">interpretability</a>
                <a href="http://127.0.0.1:8000/tag/categorical-variables/" class="article-tag">categorical-variables</a>
                <a href="http://127.0.0.1:8000/tag/multicollinearity/" class="article-tag">multicollinearity</a>
                <a href="http://127.0.0.1:8000/tag/imbalanced-data/" class="article-tag">imbalanced-data</a>
        </div>





            <div class="related-posts">
                <h4 class="related-posts-title">You might also like</h4>
                <div class="related-posts-grid">
                        <a href="http://127.0.0.1:8000/implementing-sentence-boundary-detection-in-python-for-improved-text-chunkin/" class="related-post-card">
                            <span class="related-post-title">Implementing Sentence Boundary Detection in Python for Improved Text Chunking</span>
                            <span class="related-post-date">Aug 30, 2024</span>
                        </a>
                        <a href="http://127.0.0.1:8000/what-is-the-key-difference-between-pca-and-svd/" class="related-post-card">
                            <span class="related-post-title">What Is the Key Difference Between PCA and SVD?</span>
                            <span class="related-post-date">Nov 06, 2023</span>
                        </a>
                        <a href="http://127.0.0.1:8000/demystifying-perplexity-assessing-dimensionality-reduction-with-pca/" class="related-post-card">
                            <span class="related-post-title">Demystifying Perplexity - Assessing Dimensionality Reduction With PCA</span>
                            <span class="related-post-date">Jun 30, 2023</span>
                        </a>
                        <a href="http://127.0.0.1:8000/tsne-tutorial/" class="related-post-card">
                            <span class="related-post-title">Unleashing the Power of T-Sne for Dimensionality Reduction in Python</span>
                            <span class="related-post-date">Mar 15, 2021</span>
                        </a>
                </div>
            </div>




    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "http://127.0.0.1:8000",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="http://127.0.0.1:8000/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false
        });
    });
</script>

<script src="http://127.0.0.1:8000/theme/js/theme-switcher.js"></script>
</body>
</html>