
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="color-scheme" content="light dark"/>
    <script>
      (function() {
        var theme = localStorage.getItem('theme-preference');
        if (theme === 'dark' || theme === 'light') {
          document.documentElement.setAttribute('data-theme', theme);
        }
      })();
    </script>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"/>
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap"
              rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/stylesheet/style.css">


    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/pygments/github.min.css">


    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/fontawesome.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/brands.css">
    <link rel="stylesheet" type="text/css" href="https://www.safjan.com/theme/font-awesome/css/solid.css">

    <link rel="stylesheet" href="https://www.safjan.com/pagefind/pagefind-ui.css">

        <link rel="stylesheet" type="text/css"
              href="https://www.safjan.com/styles/custom.css">

        <link href="https://www.safjan.com/feeds/all.atom.xml?utm_source=rss&utm_medium=feed&utm_campaign=rss-feed" type="application/atom+xml" rel="alternate"
              title="Krystian Safjan's Blog Atom">

        <link href="https://www.safjan.com/feeds/all.rss.xml?utm_source=rss&utm_medium=feed&utm_campaign=rss-feed" type="application/rss+xml" rel="alternate"
              title="Krystian Safjan's Blog RSS">


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RM2PKDCCYM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-RM2PKDCCYM');
</script>
<!-- End of Google tag (gtag.js) -->



    <meta name="author" content="Krystian Safjan"/>
    <meta name="description" content="QLoRA (Quantized Low-Rank Adaptation) enables efficient fine-tuning of large language models using a 4-bit quantized base model and trainable low-rank adapters, significantly reducing memory usage while maintaining performance. Key techniques include 4-bit NormalFloat quantization, paged optimizers, and double quantization."/>
    <meta name="keywords" content="qlora, lora, low-rank-adaptation, quantisation, weights, adapters, 4-bit-normal-float, double-quantisation, training, fine-tuning">


  <meta property="og:site_name" content="Krystian Safjan's Blog"/>
  <meta property="og:title" content="How does QLoRA works?"/>
  <meta property="og:description" content="QLoRA (Quantized Low-Rank Adaptation) enables efficient fine-tuning of large language models using a 4-bit quantized base model and trainable low-rank adapters, significantly reducing memory usage while maintaining performance. Key techniques include 4-bit NormalFloat quantization, paged optimizers, and double quantization."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://www.safjan.com/how-does-qlora-works/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-07-03 00:00:00+02:00"/>
  <meta property="article:modified_time" content="2024-07-03 00:00:00+02:00"/>
  <meta property="article:author" content="https://www.safjan.com/author/krystian-safjan/"/>
  <meta property="article:section" content="note"/>
  <meta property="article:tag" content="qlora"/>
  <meta property="article:tag" content="lora"/>
  <meta property="article:tag" content="low-rank-adaptation"/>
  <meta property="article:tag" content="quantisation"/>
  <meta property="article:tag" content="weights"/>
  <meta property="article:tag" content="adapters"/>
  <meta property="article:tag" content="4-bit-normal-float"/>
  <meta property="article:tag" content="double-quantisation"/>
  <meta property="article:tag" content="training"/>
  <meta property="article:tag" content="fine-tuning"/>
  <meta property="og:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:image" content="/images/profile_new.jpg"/>
    <meta name="twitter:image:alt" content="Krystian Safjan's Blog"/>


    <meta name="twitter:site" content="@izikeros"/>
    <meta name="twitter:creator" content="@izikeros"/>
    <meta name="twitter:title" content="How does QLoRA works?"/>
    <meta name="twitter:description" content="QLoRA (Quantized Low-Rank Adaptation) enables efficient fine-tuning of large language models using a 4-bit quantized base model and trainable low-rank adapters, significantly reducing memory..."/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.safjan.com/how-does-qlora-works/"
  },
  "headline": "How does QLoRA works?",
  "datePublished": "2024-07-03T00:00:00+02:00",
  "dateModified": "2024-07-03T00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Krystian Safjan",
    "url": "https://www.safjan.com/author/krystian-safjan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Krystian Safjan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/profile_new.jpg"
    }  },
"image": "/images/profile_new.jpg",  "url": "https://www.safjan.com/how-does-qlora-works/",
  "description": "QLoRA (Quantized Low-Rank Adaptation) enables efficient fine-tuning of large language models using a 4-bit quantized base model and trainable low-rank...",
  "articleSection": "note",
  "inLanguage": "en",
  "keywords": "qlora, lora, low-rank-adaptation, quantisation, weights, adapters, 4-bit-normal-float, double-quantisation, training, fine-tuning",
  "wordCount": 554,
  "speakable": {
    "@type": "SpeakableSpecification",
    "cssSelector": ["article h1", ".summary", ".tldr", "article \u003e p:first-of-type"]
  }}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://www.safjan.com/"
    },
    {
      "@type": "ListItem",
      "position": 2,
      "name": "note",
      "item": "https://www.safjan.com/category/note.html"
    },
    {
      "@type": "ListItem",
      "position": 3,
      "name": "How does QLoRA works?"
    }
  ]
}
</script>



<meta name="ai:summary" content="QLoRA (Quantized Low-Rank Adaptation) enables efficient fine-tuning of large language models using a 4-bit quantized base model and trainable low-rank adapters, significantly reducing memory usage while maintaining performance. Key techniques include 4-bit NormalFloat quantization, paged...">

<meta name="ai:topics" content="qlora, lora, low-rank-adaptation, quantisation, weights, adapters, 4-bit-normal-float, double-quantisation, training, fine-tuning">



<meta name="ai:word-count" content="554">
<meta name="ai:reading-time" content="2 min">

<meta name="ai:section" content="note">



<meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">

    <title>    How does QLoRA works?
</title>



<!-- Google Automatic Ads -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9767732578835199"
     crossorigin="anonymous"></script>
<!-- End of Google Automatic Ads -->


</head>
<body>
<div id="reading-progress" class="reading-progress"></div>
<aside>
    <div>
        <a href="https://www.safjan.com/">
                <img src="/images/profile_new.jpg" alt="Krystian Safjan's Blog" title="Krystian Safjan's Blog" width="140" height="140">
        </a>

        <h1>
            <a href="https://www.safjan.com/">Krystian Safjan's Blog</a>
        </h1>

<p>Data Scientist and Team Leader writing about Machine Learning, MLOps, and Python</p>


        <ul class="social">
                <li>
                    <a  class="sc-linkedin" href="https://pl.linkedin.com/in/krystiansafjan"
                       target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-github" href="https://github.com/izikeros"
                       target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-envelope" href="mailto:ksafjan@gmail.com"
                       target="_blank">
                        <i class="fas fa-envelope"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-graduation-cap" href="https://scholar.google.pl/citations?user=UlNJgMoAAAAJ"
                       target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li>
                    <a  class="sc-rss" href="/feeds/all.rss.xml"
                       target="_blank">
                        <i class="fas fa-rss"></i>
                    </a>
                </li>
        </ul>
    </div>

<div class="promo-box">
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-image">
        <img src="/images/mlop_interview_book_cover_3D_300px.jpg" alt="MLOps Interview Book Cover">
    </a>
    
    <p class="promo-box-headline">Ace Your MLOps Interview</p>
    
    <a href="https://ksafjanuser.gumroad.com/l/mlops" class="promo-box-cta">
        Get for $2.99
    </a>
    
    <p class="promo-box-features">50 Q&A • PDF/ePUB/mobi</p>
</div>
</aside>
<main>

        <nav>
            <a href="https://www.safjan.com/">Home</a>

                <a href="/archives.html">Articles</a>
                <a href="/til.html">Notes</a>
                <a href="/categories.html">Categories</a>
                <a href="/pdfs/Krystian_Safjan_resume_priv.pdf">Resume</a>

                <a href="https://www.safjan.com/feeds/all.atom.xml">Atom</a>

                <a href="https://www.safjan.com/feeds/all.rss.xml">RSS</a>

            <div id="search" class="nav-search"></div>
            <button type="button" id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <i class="fas fa-sun"></i>
                <i class="fas fa-moon"></i>
            </button>
        </nav>

    <article class="single">
        <header>
                
            <p>
                <!-- Posted on: -->
                2024-07-03 


                <br/>
            </p>
            <h1 id="how-does-qlora-works">How does QLoRA works?</h1>
            <div class="header-underline"></div>



        </header>




        <div class="article-content">
            <h2 id="tldr">TL;DR</h2>
<blockquote>
<p><strong>QLoRA</strong> (Quantized Low-Rank Adaptation) is a memory-efficient fine-tuning method for large language models. It uses a frozen 4-bit quantized base model with trainable adapters. During fine-tuning, only the adapters are updated, with gradients backpropagated through the quantized weights. Key innovations include 4-bit NormalFloat quantization, paged optimizers, and double quantization, all of which significantly reduce memory usage. This allows fine-tuning of large models on consumer hardware without compromising performance.
</p>
</blockquote>
<p>This article outline in brief idea of QLoRA. For the deeper understanding of QLoRA, I highly recommend reading <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">blog post</a> by the QLoRA authors explaining the QLoRA idea in a clear way.</p>
<h2 id="understanding-qlora-efficient-fine-tuning-for-large-language-models">Understanding QLoRA: Efficient Fine-tuning for Large Language Models</h2>
<p>QLoRA (Quantized Low-Rank Adaptation) is an innovative technique that enables efficient fine-tuning of large language models. It combines several key components to reduce memory usage and computational costs without sacrificing performance. Let's break down how QLoRA works:</p>
<h3 id="core-components">Core Components</h3>
<ol>
<li><strong>4-bit Quantized Base Model</strong>: QLoRA starts with a pre-trained language model quantized to 4-bit precision. This dramatically reduces memory requirements compared to full-precision models.</li>
<li><strong>Low-Rank Adapters</strong>: Small, trainable modules are added on top of the frozen base model. These adapters capture task-specific information during fine-tuning.</li>
<li><strong>4-bit NormalFloat</strong>: A novel quantization data type that maintains a normal distribution of values, preserving model quality better than traditional integer quantization.</li>
<li><strong>Paged Optimizers</strong>: A memory management technique that efficiently swaps optimizer states between CPU and GPU memory.</li>
<li><strong>Double Quantization</strong>: Further compresses the quantization constants, reducing memory usage even more.</li>
</ol>
<h3 id="how-it-works">How It Works?</h3>
<ol>
<li>The base model is quantized to 4-bit precision and frozen.</li>
<li>Low-rank adapters are added to each layer of the model.</li>
<li>During fine-tuning, only the adapters are updated.</li>
<li>Backpropagation occurs through the 4-bit weights into the adapters.</li>
<li>Paged optimizers manage memory usage during training.</li>
<li>Double quantization further reduces memory requirements.</li>
</ol>
<p>This approach allows for fine-tuning of very large models on consumer-grade hardware, opening up new possibilities for researchers and developers working with state-of-the-art language models.</p>
<p><img alt="Full Fine Tuning, LoRA and QLoRA fine tuning compared" src="/images/qlora/qlora_fine_tuning.png">
<em>Figure from: <a href="https://arxiv.org/abs/2305.14314">QLoRA paper by Dettmers et al</a></em></p>
<h2 id="three-innovative-techniques-for-memory-efficiency">Three Innovative Techniques for Memory Efficiency</h2>
<h3 id="4-bit-normalfloat-nf4">4-bit NormalFloat (NF4)</h3>
<p>The QLoRA paper introduces the concept of 4-bit NormalFloat (NF4), a novel <strong>data type</strong> that is information theoretically <strong>optimal for normally distributed weights</strong>.
NF4 is used for quantization in QLoRA, which aims to make large language models more accessible by reducing memory usage during fine-tuning. Unlike traditional 4-bit integer or 4-bit floating-point representations, NF4 is specifically designed for normally distributed weights, making it more efficient for certain tasks.</p>
<h3 id="paged-optimizers">Paged Optimizers</h3>
<p>In the context of QLoRA, paged optimizers are introduced to manage memory spikes during fine-tuning. These optimizers help mitigate memory usage by <strong>efficiently handling memory transfers between the GPU and CPU</strong>. While the specifics of paged optimizers are not covered extensively in the QLoRA paper, they play a crucial role in achieving memory efficiency.</p>
<h3 id="double-quantization">Double Quantization</h3>
<p>Double quantization is a technique used to reduce the average memory footprint in QLoRA. It involves <strong>quantizing</strong> not only the <strong>model parameters</strong> but also the <strong>quantization constants themselves</strong>. By applying double quantization, QLoRA achieves memory savings without compromising performance.</p>
<p>These innovations collectively contribute to QLoRA’s ability to fine-tune large language models efficiently while maintaining performance.</p>
<h1 id="further-reading">Further Reading</h1>
<ul>
<li><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a> - blog post by the QLoRA authors explaining the QLoRA idea in a clear way.</li>
<li><a href="https://arxiv.org/abs/2305.14314">Original QLoRA paper by Dettmers et al</a> (2023)</li>
</ul>
        </div>


        <div class="article-tags">
            <span class="tags-label">Tags:</span>
                <a href="https://www.safjan.com/tag/qlora/" class="article-tag">qlora</a>
                <a href="https://www.safjan.com/tag/lora/" class="article-tag">lora</a>
                <a href="https://www.safjan.com/tag/low-rank-adaptation/" class="article-tag">low-rank-adaptation</a>
                <a href="https://www.safjan.com/tag/quantisation/" class="article-tag">quantisation</a>
                <a href="https://www.safjan.com/tag/weights/" class="article-tag">weights</a>
                <a href="https://www.safjan.com/tag/adapters/" class="article-tag">adapters</a>
                <a href="https://www.safjan.com/tag/4-bit-normal-float/" class="article-tag">4-bit-normal-float</a>
                <a href="https://www.safjan.com/tag/double-quantisation/" class="article-tag">double-quantisation</a>
                <a href="https://www.safjan.com/tag/training/" class="article-tag">training</a>
                <a href="https://www.safjan.com/tag/fine-tuning/" class="article-tag">fine-tuning</a>
        </div>











    </article>

    <footer>
<p>
  &copy; 2026 Krystian Safjan - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike</a>
</p>    </footer>
</main>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Blog",
  "name": "Krystian Safjan's Blog",
  "url": "https://www.safjan.com",
"image": "/images/profile_new.jpg",  "description": ""
}
</script>


<script src="https://www.safjan.com/pagefind/pagefind-ui.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', function() {
        new PagefindUI({
            element: "#search",
            showSubResults: false,
            showImages: false,
            basePath: "https://www.safjan.com/pagefind/"
        });
    });
</script>

<script src="https://www.safjan.com/theme/js/theme-switcher.js"></script>
</body>
</html>